# Visualization for Explain-ability

<!-- As AI goes more and more AutoML it becomes black box more and more. Deep Learning is inherently non transparent. This hurts debugging and impediments justifying predictions. It has become legally imperative to back-trace the predictions.

Explainable AI (XAI) has approaches to deal with this problem. Although there are mathematical techniques like SHAPly values and LIME for XAI, stress here would be to achieve explain-ability via Visualization and insights that come through pictures and animations. -->

Concepts or algorithms are hard to understand, many a times. Typically due unavailability of good explanation. Just reading may not ingest the understanding. Visualization helps. Using animation is better.

Code-bases in this folder attempt to show such examples.


- What it is not: UI, UX or HCI design
- What it is: explain-ability via visualization
- Application to: AI-ML, NLP, Research papers, Education, Dashboards, etc.

Copyright (C) 2021 Yogesh H Kulkarni

## Principles

Structuring Math explanations - by Grant Sanderson, https://www.youtube.com/watch?v=ojjzXyQCzso
- Concrete before Abstract
  - Give examples first, then theory
	- Be aware of abstractions:
	
		![Layers](./Manim/images/layers.png)
		
		![Layers Example](./Manim/images/layersexample.png)

- Topic choice > production quality.
	- Content is king, how you produce it is secondary.
	- Choose wisely, needs to be fresh
	- But no 0 production quality though. Good sound a must.
- Be niche
	- Choose something rare, esoteric
	- Cant compete with other million videos.
	- Build small, loyal audience
	- May appeal to more than what you think.
- Know your genre
	- Be upfront about your level, discovery mode
	- Helping others, tutoring mode
	- Slow/Fast? Don't follow others as is
- Definitions not at the beginning


	
## References
- Papers:
  - https://visxai.io/
  - https://distill.pub/
- Groups:
  - https://research.google/teams/brain/pair/
  - http://vis.cse.ust.hk/groups/xai-vis/
