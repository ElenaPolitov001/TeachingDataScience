%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \begin{center}
    {\Large Introduction to TensorFlow 2.0}
  \end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Website}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{dl_tf2_11}
\end{center}


\tiny{(Ref: Introduction to TensorFlow 2.0 - Brad Miro)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{TensorFlow is}
\begin{itemize}
\item Open source, Free library, with Python bindings, by Google Brain team
\item Other libraries are: Caffe (Berkeley), Torch (Facebook), Cntk (Microsoft),
\item Can deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API
\item Flexibility: from Raspberry Pi, Android, Windows, iOS, Linux to server farms
\item Till 2019 Keras was popular as a separate library  (with back-end as Tensorflow) but with Tensorflow 2.0, Keras as become its default front end API.
\item TensorFlow 2.0 merges keras as ``tf.keras''. It allows you to design, fit, evaluate deep learning models.
\end{itemize}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Why Tf.Keras?}

% \begin{itemize}
% \item  Simple, Highly modular 
% \item  Deep enough to build models
% \item  A focus on user experience.
% \item  Large adoption in the industry and research community.
% \item  Easy production of models using TensorFlow Serving
% \item Easy visualization using TensorBoard
% \end{itemize}
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Open Source Community}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{dl_tf2_1}
\end{center}

As of Oct 2019 \ldots

\tiny{(Ref: Introduction to TensorFlow 2.0 - Brad Miro)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Tensorflow 2.0}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{dl_tf2_2}
\end{center}


\tiny{(Ref: Introduction to TensorFlow 2.0 - Brad Miro)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Deploy Anywhere}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{dl_tf2_3}
\end{center}


\tiny{(Ref: Introduction to TensorFlow 2.0 - Brad Miro)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Training and Deployment}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{dl_tf2_4}
\end{center}


\tiny{(Ref: Introduction to TensorFlow 2.0 - Brad Miro)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Ecosystem/Verticals}

\begin{center}
\includegraphics[width=0.3\linewidth,keepaspectratio]{dl_tf2_5}
\end{center}


\tiny{(Ref: Introduction to TensorFlow 2.0 - Brad Miro)}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Compared to TF 1.0}

\begin{columns}
    \begin{column}[T]{0.6\linewidth}
	What’s Gone
      \begin{itemize}
		\item \lstinline|Session.run|
		\item \lstinline|tf.control_dependencies|
		\item \lstinline|tf.global_variables_initializer|
		\item \lstinline|tf.cond, tf.while_loop|
		\item \lstinline|tf.contrib|
	  \end{itemize}

    \end{column}
    \begin{column}[T]{0.4\linewidth}
	What’s New
      \begin{itemize}
		\item Eager execution by default
		\item \lstinline|tf.function|
		\item Keras as main high-level api
	  \end{itemize}
    \end{column}
  \end{columns}

\tiny{(Ref: Introduction to TensorFlow 2.0 - Brad Miro)}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{tf.keras}

% \begin{center}
% \includegraphics[width=\linewidth,keepaspectratio]{dl_tf2_6}
% \end{center}


% \tiny{(Ref: Introduction to TensorFlow 2.0 - Brad Miro)}
% \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Installation}

\begin{itemize}
\item Have Python installed, such as Python 3.6 or higher.
\item Easy way to install TensorFlow
\item Linux:
\begin{lstlisting}
sudo pip install tensorflow
\end{lstlisting}
\item Windows:
\begin{lstlisting}
pip install tensorflow
\end{lstlisting}
\end{itemize}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Installation Check}

\begin{itemize}
\item Confirm the installation by:
\begin{lstlisting}
# check version
import tensorflow
print(tensorflow.__version__)
\end{lstlisting}
\item It must be 2.0 onwards
\item If you get warning like below, Don't worry, just IGNORE.
\begin{lstlisting}
Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
XLA service 0x7fde3f2e6180 executing computations on platform Host. Devices:
StreamExecutor device (0): Host, Default Version
\end{lstlisting}

\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Hello World!!}

\begin{lstlisting}
import tensorflow as tf # Assuming TF 2.0 is installed
a = tf.constant([[1, 2],[3, 4]])
b = tf.matmul(a, a)
print(b) 
# tf.Tensor( [[ 7 10] [15 22]], shape=(2, 2), dtype=int32)
print(type(b.numpy()))
# <class 'numpy.ndarray'>
\end{lstlisting}


\tiny{(Ref: Introduction to TensorFlow 2.0 - Brad Miro)}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{tf.keras}

% \begin{center}
% \includegraphics[width=\linewidth,keepaspectratio]{dl_tf2_6}
% \end{center}


% \tiny{(Ref: Introduction to TensorFlow 2.0 - Brad Miro)}
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Keras and tf.keras}

\begin{itemize}
\item  Fast prototyping, advanced research, and production
\item keras.io = reference implementation \lstinline|import keras|
\item tf .keras  = TensorFlow’s implementation (a superset, built-in to TF, no need to install Keras separately) \lstinline|from tensorflow import keras|
\end{itemize}

\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{dl_tf2_6}
\end{center}

\tiny{(Ref: Introduction to TensorFlow 2.0 - Brad Miro)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Steps to use tf.Keras}

\begin{itemize}
\item  Define the model.
\item  Compile the model.
\item  Fit the model.
\item  Evaluate the model.
\item  Make predictions.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Define the Model}

\begin{itemize}
\item  First, select the type of the model.
\item Choose architecture or network topology.
\item Meaning, define layers, its parameters.
\item There are multiple API ways to define the model (will look at later)
\end{itemize}

\begin{lstlisting}
...
# define the model
model = ...
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Compile the Model}

\begin{itemize}
\item Select loss function that you want to optimize, eg Cross Entropy or Mean Squared Error
\item Select Optimization method, eg Adam, Stochastic Gradient Descent
\item Select performance metrics to be used during Training
\end{itemize}

\begin{lstlisting}
...
# compile the model
opt = SGD(learning_rate=0.01, momentum=0.9)
model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Fit the Model}

\begin{itemize}
\item Select Training configuration (epochs, batch size, etc)
\item *Epochs: number of full cycles (forward + backward) during training
\item *Batch Size: Number of samples used to estimate model error, or update the weights
\item Can take minutes to hours to days depending on complexity, hardware, training samples size.
\item Progress bar shows status of each epoch, performance, etc.
\end{itemize}

\begin{lstlisting}
...
# fit the model
model.fit(X, y, epochs=100, batch_size=32)
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Evaluate the Model}

\begin{itemize}
\item Select a holdout dataset (cross validation)
\item This is not used for training but just for evaluation as it has correct answers as well.
\end{itemize}

\begin{lstlisting}
...
# evaluate the model
loss = model.evaluate(X, y, verbose=0)
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Making Predictions}

\begin{itemize}
\item Get Test set for which answers have to be found out.
\item Better to save the model and later load it to make predictions. 
\item May choose to fit a model on all of the available data before you start using it.
\end{itemize}

\begin{lstlisting}
...
# make a prediction
yhat = model.predict(X)
\end{lstlisting}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Model Definition}
API styles
\begin{itemize}
\item   The Sequential Model
\begin{itemize}
\item   Dead simple
\item   Only for single-input, single-output, sequential layer stacks
\item   Good for 70+\% of use cases
\end{itemize}
\item   The functional API
\begin{itemize}
\item   Like playing with Lego bricks
\item    Multi-input, multi-output, arbitrary static graph topologies
\item   Good for 95\% of use cases
\end{itemize}
\item   Model subclassing
\begin{itemize}
\item    Maximum flexibility
\item    Larger potential error surface
\end{itemize}
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Sequential Model API (Simple)}

\begin{itemize}
\item Called ``Sequential'' because it involves using Sequential class and adding layers to it one-by-one, in a sequence.
\item E.g. 8 inputs, one hidden layer with 10 nodes, and one output layer with one node to predict numerical value would look:
\end{itemize}

\begin{lstlisting}
# example of a model defined with the sequential api
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
# define the model
model = Sequential()
model.add(Dense(10, input_shape=(8,)))
model.add(Dense(1))
\end{lstlisting}

Note:
\begin{itemize}
\item Input layer, per say, is NOT added. Its an argument for the first HIDDEN layer.
\item Here `input\_shape' of $(8,)$ means one sample/row is of 8 values. And such, many samples/rows can come, so left blank.
\end{itemize}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{The Sequential API}
% A deeper model would look:

% \begin{center}
% \includegraphics[width=0.8\linewidth,keepaspectratio]{kr20}
% \end{center}
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Functional Model API (Advanced)}

\begin{itemize}
\item Need to explicitly connections between layers.
\item Models may have multiple input/output paths (a word and a number)
\item Input layer needs to be defined explicitly, like:
\begin{lstlisting}
x_in = Input(shape=(8,))
\end{lstlisting}

\item Next, a fully connected layer can be connected to the input by calling the layer and passing the input layer. This will return a reference to the output connection in this new layer.
\begin{lstlisting}
x = Dense(10)(x_in)
\end{lstlisting}

\item Once connected, we define a Model object and specify the input and output layers.
\begin{lstlisting}
x_in = Input(shape=(8,))
x = Dense(10)(x_in)
x_out = Dense(1)(x)
# define the model
model = Model(inputs=x_in, outputs=x_out)
\end{lstlisting}
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Sub-classing Model API (Very Advanced)}


\begin{lstlisting}
class MyModel(tf.keras.Model):
  def __init__(self, num_classes=10):
    super(MyModel, self).__init__(name='my_model')
    self.dense_1 = layers.Dense(32, activation='relu')
    self.dense_2 = layers.Dense(num_classes, activation='sigmoid')
	
  def call(self, inputs):
    # Define your forward pass here,
    x = self.dense_1(inputs)
    return self.dense_2(x)
\end{lstlisting}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{The functional API}
% \begin{center}
% \includegraphics[width=0.8\linewidth,keepaspectratio]{kr21}
% \end{center}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Model subclassing}
% \begin{center}
% \includegraphics[width=0.8\linewidth,keepaspectratio]{kr22}
% \end{center}
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Understanding deferred (symbolic)
vs. eager (imperative)}

\begin{itemize}
\item  Deferred: Build a computation graph that gets compiled first and then once values are filled, executed later
\item  Eager: Model is a python exe,Execution is runtime (like Numpy)
\item Deferred: Symbolic tensors don’t have a value in your Python code (yet)
\item  Eager: tensors have a value in your Python code
\item Eager: can use value-dependent dynamic topologies 
(tree-RNNs)
\end{itemize}
\end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Sample eager execution code}

% \begin{lstlisting}
% lstm_cell = tf.keras.layers.LSTMCell(10)

% def fn(input, state):
  % return lstm_cell(input, state)
  
% input = tf.zeros([10, 10]); state = [tf.zeros([10, 10])] * 2
% lstm_cell(input, state); fn(input, state) # warm up
% # benchmark
% timeit.timeit(lambda: lstm_cell(input, state), number=10) # 0.03
% \end{lstlisting}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Let’s make this faster}

% \begin{lstlisting}
% lstm_cell = tf.keras.layers.LSTMCell(10)

% @tf.function
% def fn(input, state):
  % return lstm_cell(input, state)
  
% input = tf.zeros([10, 10]); state = [tf.zeros([10, 10])] * 2
% lstm_cell(input, state); fn(input, state) # warm up
% # benchmark
% timeit.timeit(lambda: lstm_cell(input, state), number=10) # 0.03
% \end{lstlisting}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{AutoGraph makes this possible}

% Say, for a sample function
% \begin{lstlisting}
% @tf.function
% def f(x):
  % while tf.reduce_sum(x) > 1:
    % x = tf.tanh(x)
  % return x
% # you never need to run this (unless curious)
% print(tf.autograph.to_code(f))
% \end{lstlisting}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Generated code}

% We need not undesrtand this, but still \ldots
% \begin{lstlisting}
% def tf__f(x):
  % def loop_test(x_1):
    % with ag__.function_scope('loop_test'):
      % return ag__.gt(tf.reduce_sum(x_1), 1)
  % def loop_body(x_1):
    % with ag__.function_scope('loop_body'):
      % with ag__.utils.control_dependency_on_returns(tf.print(x_1)):
        % tf_1, x = ag__.utils.alias_tensors(tf, x_1)
        % x = tf_1.tanh(x)
        % return x,
  % x = ag__.while_stmt(loop_test, loop_body, (x,), (tf,))
  % return x
% \end{lstlisting}
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Distribution Strategy}

For the sample code below \ldots
\begin{lstlisting}
 model = tf.keras.models.Sequential([
      tf.keras.layers.Dense(64, input_shape=[10]),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(10, activation='softmax')])
  model.compile(optimizer='adam',
                loss='categorical_crossentropy',
                metrics=['accuracy'])
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{ Multi-GPU}

One of the computations distribution strategy could be \ldots
\begin{lstlisting}
strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
  model = tf.keras.models.Sequential([
      tf.keras.layers.Dense(64, input_shape=[10]),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(10, activation='softmax')])
  model.compile(optimizer='adam',
                loss='categorical_crossentropy',
                metrics=['accuracy'])
\end{lstlisting}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{tensorf low_datasets}

% Abstraction to get data loaded\ldots
% \begin{lstlisting}
% import tensorflow_datasets as tfds
% dataset = tfds.load(‘cats_vs_dogs', as_supervised=True)
% mnist_train, mnist_test = dataset['train'], dataset['test']
% def scale(image, label):
  % image = tf.cast(image, tf.float32)
  % image /= 255
  % return image, label
% mnist_train = mnist_train.map(scale).batch(64)
% mnist_test = mnist_test.map(scale).batch(64)
% \end{lstlisting}
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{TensorFlow Datasets}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{dl_tf2_7}
\end{center}

More at tensorflow.org/datasets

\tiny{(Ref: Introduction to TensorFlow 2.0 - Brad Miro)}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Eager Execution}

% The Keras functional API and Sequential API work with eager execution


% \begin{center}
% \includegraphics[width=\linewidth,keepaspectratio]{kr23}
% \end{center}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Eager Execution}

% Eager execution allows you to write imperative custom layers


% \begin{center}
% \includegraphics[width=0.65\linewidth,keepaspectratio]{kr24}
% \end{center}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Eager Execution}

% Maximum flexibility: imperative Model subclassing

% \begin{center}
% \includegraphics[width=0.8\linewidth,keepaspectratio]{kr25}
% \end{center}
% \end{frame}



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}
  % \begin{center}
    % {\Large Sequential Workflow: Recap}
    
  % \end{center}
% \end{frame}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Steps}

% \begin{itemize}
% \item  Prepare your 
% input and 
% output tensors 
% \item  Create first 
% layer to handle 
% input layer
% \item  Create last 
% layer to handle 
% output targets
% \item Build any 
% model you like 
% in between
% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Steps}
% \begin{center}
% \includegraphics[width=0.65\linewidth,keepaspectratio]{kr2}
% \end{center}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Build Layers}
% \begin{lstlisting}
% from tf.keras.models import Sequential
% from tf.keras.layers import Dense, Activation

% model = Sequential()

% model.add(Dense(units=64, input_dim=100))
% model.add(Activation('relu'))
% model.add(Dense(units=10))
% model.add(Activation('softmax'))
% \end{lstlisting}

% \begin{itemize}
% \item Constructing a Sequential model, and
% adding three layers to it: a linear, or dense, layer;
% the activation layer; and the output layer.
% \begin{lstlisting}
% >>> model = Sequential()
% >>> model.add(Dense(512, input_shape=(28 * 28,)))
% >>> model.add(Activation("sigmoid"))
% >>> model.add(Dense(10))
% \end{lstlisting}
% \item The input shape parameter needs to be set for the
% first layer, but can be inferred for all of the other
% layers.
% \end{itemize}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Compile Model}
% \begin{lstlisting}
% model.compile(loss='categorical_crossentropy',optimizer='sgd',
% metrics=['accuracy'])

% or

% model.compile(loss=keras.losses.categorical_crossentropy,optimizer=
% keras.optimizers.SGD(lr=0.01, momentum=0.9))
% \end{lstlisting}

% \begin{itemize}
% \item Compile the model (this may take a minute
% or more) using stochastic gradient descent.
% \begin{lstlisting}
% >>> sgd = SGD(lr = 0.02, momentum = 0.01, nesterov = True)
% >>> model.compile(loss='mse', optimizer=sgd)
% \end{lstlisting}
% \item By setting various environment variables, we could do this
% over Tensorflow or Theano, and could make it run over
% various CPU or GPU architectures.
% \end{itemize}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Training}
% \begin{lstlisting}
% model.fit(x_train, y_train, epochs=5, batch_size=32)

% # You can feed data batches manually

% model.train_on_batch(x_batch, y_batch)
% \end{lstlisting}

% \begin{itemize}
% \item To train the actual model, we run the fit method on
% the model, giving the desired batch size and number
% of epochs:
% \begin{lstlisting}
% >>> model.fit(X_train, Y_train, batch_size=32, nb_epoch=25,
% ...           verbose=1, show_accuracy=True)
% \end{lstlisting}
% \item This will produce a running output of the model training
% process.

% % \item Run this is the Python interpreter
% % and see the various advanced features than can be easily added
% % to adapt the final model.
% \end{itemize}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Usage}
% \begin{lstlisting}
% # Evaluation

% loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)

% # Prediction

% classes = model.predict(x_test, batch_size=128)
% \end{lstlisting}

% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Minimal working example}

% \begin{lstlisting}
% >>> from tf.keras.models import Sequential
% >>> from tf.keras.layers.core import Dense, Activation
% >>> from tf.keras.optimizers import SGD
% \end{lstlisting}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Model Specification}

% \begin{itemize}
% \item Constructing a Sequential model, and
% adding three layers to it: a linear, or dense, layer;
% the activation layer; and the output layer.
% \begin{lstlisting}
% >>> model = Sequential()
% >>> model.add(Dense(512, input_shape=(28 * 28,)))
% >>> model.add(Activation("sigmoid"))
% >>> model.add(Dense(10))
% \end{lstlisting}
% \item The input shape parameter needs to be set for the
% first layer, but can be inferred for all of the other
% layers.
% \end{itemize}
% % \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Model Compilation}

% \begin{itemize}
% \item Compile the model (this may take a minute
% or more) using stochastic gradient descent.
% \begin{lstlisting}
% >>> sgd = SGD(lr = 0.02, momentum = 0.01, nesterov = True)
% >>> model.compile(loss='mse', optimizer=sgd)
% \end{lstlisting}
% \item By setting various environment variables, we could do this
% over Tensorflow or Theano, and could make it run over
% various CPU or GPU architectures.
% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Model Fitting}

% \begin{itemize}
% \item To train the actual model, we run the fit method on
% the model, giving the desired batch size and number
% of epochs:
% \begin{lstlisting}
% >>> model.fit(X_train, Y_train, batch_size=32, nb_epoch=25,
% ...           verbose=1, show_accuracy=True)
% \end{lstlisting}
% \item This will produce a running output of the model training
% process.

% \item Run this is the Python interpreter
% and see the various advanced features than can be easily added
% to adapt the final model.
% \end{itemize}
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Terminologies}

In the neural network terminology:

\begin{itemize}
\item one epoch = one forward pass and one backward pass of all the training examples
\item batch size = the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.
\item number of iterations = number of passes, each pass using [batch size] number of examples. To be clear, one pass = one forward pass + one backward pass (we do not count the forward pass and backward pass as two different passes).
\item Example: if you have 1000 training examples, and your batch size is 500, then it will take 2 iterations to complete 1 epoch.
\end{itemize}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Terminologies: Layers Parameters}

% Keras Layers Shape parameters

% \begin{itemize}
% \item Units: number of neurons. Output shape. Its a property of each layer.
% \item Input Shape: In Keras, there is no input layer. Its a tensor. It goes to the first hidden layer. This tensor must have the same shape as your training data. Example: if you have 30 images of 50x50 pixels in RGB (3 channels), the shape of your input data is (30,50,50,3). Then your input layer tensor, must have this shape. (See next slide for more info on input shapes)
% \item Now, the input shape is the only one you must define, because your model cannot know it. Only you know that, based on your training data.

% \item All the other shapes are calculated automatically based on the units and particularities of each layer.
% \item Since the input shape is the only one you need to define, Keras will demand it in the first layer.
% \end{itemize}

% \tiny{(Ref: Keras input explanation: input\_shape, units, batch\_size, dim, etc - Stack Overflow)}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Terminologies: Input Shapes}

% Each type of layer requires the input with a certain number of dimensions:

% \begin{itemize}
% \item Dense layers have output shape based on "units". Dense layers require inputs as (batch\_size, input\_size) or (batch\_size, optional,...,optional, input\_size)
% \item Convolutional layers have output shape based on "filters". But it's always based on some layer property. 
% \item 2D convolutional layers need inputs as:
% \begin{itemize}
% \item if using channels\_last: (batch\_size, imageside1, imageside2, channels)
% \item if using channels\_first: (batch\_size, channels, imageside1, imageside2)
% \end{itemize}
% \item 1D convolutions and recurrent layers use (batch\_size, sequence\_length, features)
% \end{itemize}

% \tiny{(Ref: Keras input explanation: input\_shape, units, batch\_size, dim, etc - Stack Overflow)}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Terminologies: Input Shapes}

% As such, the following three snippets are strictly equivalent:

% \begin{lstlisting}
% model = Sequential()
% model.add(Dense(32, input_shape=(784,)))
% \end{lstlisting}

% \begin{lstlisting}
% model = Sequential()
% model.add(Dense(32, batch_input_shape=(None, 784)))
% # note that batch dimension is "None" here,
% # so the model will be able to process batches of any size.
% \end{lstlisting}

% \begin{lstlisting}
% model = Sequential()
% model.add(Dense(32, input_dim=784))
% \end{lstlisting}

% \tiny{(Ref: Specifying the input shape - Keras Documentation)}
% \end{frame}



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Terminologies: Input Shapes}

% And so are the following three snippets:

% \begin{lstlisting}
% model = Sequential()
% model.add(LSTM(32, input_shape=(10, 64)))
% \end{lstlisting}

% \begin{lstlisting}
% model = Sequential()
% model.add(LSTM(32, batch_input_shape=(None, 10, 64)))
% \end{lstlisting}

% \begin{lstlisting}
% model = Sequential()
% model.add(LSTM(32, input_length=10, input_dim=64))
% \end{lstlisting}

% \tiny{(Ref: Specifying the input shape - Keras Documentation)}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Terminologies: Shapes}

% \begin{itemize}
% \item Example: 30 images, 50x50 pixels and 3 channels, having an input shape of (30,50,50,3).
% \item But in this definition, Keras ignores the first dimension, which is the batch size. Your model should be able to deal with any batch size, so you define only the other dimensions: \lstinline|input_shape = (50,50,3) #regardless of how many images I have, each image has this shape|
% \item But if you want to pass batch size (or required by certain models)  then \lstinline|batch_input_shape=(30,50,50,3) or batch_shape=(30,50,50,3)|
% \item Either way you choose, tensors in the model will have the batch dimension.
% \item So, even if you used input\_shape=(50,50,3), when keras sends you messages, or when you print the model summary, it will show (None,50,50,3).
% \item The first dimension is the batch size, it's None because it can vary depending on how many examples you give for training. (If you defined the batch size explicitly, then the number you defined will appear instead of None)
% \end{itemize}

% \tiny{(Ref: Keras input explanation: input\_shape, units, batch\_size, dim, etc - Stack Overflow)}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Terminologies: The output shape}

% Relation between shapes and units

% \begin{itemize}
% \item Given the input shape, all other shapes are results of layers calculations.
% \item The "units" of each layer will define the output shape
% \item A dense layer has an output shape of (batch\_size,units)
% \item Weights will be entirely automatically calculated based on the input and the output shapes.
% \item In a dense layer, weights multiply all inputs. It's a matrix with one column per input and one row per unit
% \end{itemize}

% \tiny{(Ref: Keras input explanation: input\_shape, units, batch\_size, dim, etc - Stack Overflow)}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Terminologies: Dim}

% What is dim?

% \begin{itemize}
% \item If your input shape has only one dimension, you don't need to give it as a tuple, you give input\_dim as a scalar number.
% \item So, in your model, where your input layer has 3 elements, you can use any of these two:
% \begin{itemize}
% \item input\_shape=(3,) -- The comma is necessary when you have only one dimension
% \item input\_dim = 3
% \end{itemize}
% \item But when dealing directly with the tensors, often dim will refer to how many dimensions a tensor has. For instance a tensor with shape (25,10909) has 2 dimensions.
% \end{itemize}

% \tiny{(Ref: Keras input explanation: input\_shape, units, batch\_size, dim, etc - Stack Overflow)}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Example: Images}

% With the Sequential model:

% \begin{lstlisting}
% from tf.keras.models import Sequential  
% from tf.keras.layers import *  

% model = Sequential()    

% #start from the first hidden layer, since the input is not actually a layer   
% #but inform the shape of the input, with 3 elements.    
% model.add(Dense(units=4,input_shape=(3,))) #hidden layer 1 with input

% #further layers:    
% model.add(Dense(units=4)) #hidden layer 2
% model.add(Dense(units=1)) #output layer   
% \end{lstlisting}

% \tiny{(Ref: Keras input explanation: input\_shape, units, batch\_size, dim, etc - Stack Overflow)}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Example: Images}

% With the functional API Model:

% \begin{lstlisting}
% from tf.keras.models import Model   
% from tf.keras.layers import * 

% #Start defining the input tensor:
% inpTensor = Input((3,))   

% #create the layers and pass them the input tensor to get the output tensor:    
% hidden1Out = Dense(units=4)(inpTensor)    
% hidden2Out = Dense(units=4)(hidden1Out)    
% finalOut = Dense(units=1)(hidden2Out)   

% #define the model's start and end points    
% model = Model(inpTensor,finalOut)  
% \end{lstlisting}

% Remember you ignore batch sizes when defining layers:

% \begin{itemize}
% \item  inpTensor: (None,3)
% \item  hidden1Out: (None,4)
% \item  hidden2Out: (None,4)
% \item  finalOut: (None,1)
% \end{itemize}


% \tiny{(Ref: Keras input explanation: input\_shape, units, batch\_size, dim, etc - Stack Overflow)}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}
  % \begin{center}
    % {\Large Keras Binary Classification: Step by Step}
    
    % \tiny{(Ref:  Develop Your First Neural Network in Python With Keras Step-By-Step, Jason Brownlee)}
  % \end{center}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Tutorial Overview}
% Steps:
% \begin{itemize}
% \item  Load Data.
% \item  Define Model.
% \item  Compile Model.
% \item  Fit Model.
% \item  Evaluate Model.
% \item  Tie It All Together.

% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Imports}
% Note: machine learning algorithms that use a stochastic process (e.g. random numbers), it is a good idea to set the random number seed.
% \begin{lstlisting}
% from tf.keras.models import Sequential
% from tf.keras.layers import Dense
% import numpy
% # fix random seed for reproducibility
% numpy.random.seed(7)
% \end{lstlisting}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Data set}
 % \begin{itemize}
 % \item  Download the Pima Indian dataset from the UCI Machine Learning repository http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data 
% \item  It describes patient medical record data for Pima Indians and whether they had an onset of diabetes within five years.
% \item  It is a binary classification problem (onset of diabetes as 1 or not as 0). 
% \item All of the input variables that describe each patient are numerical. 
% \item This makes it easy to use directly with neural networks that expect numerical input and output values
% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Load Data}
 % \begin{itemize}
% \item  There are eight input variables and one output variable (the last column).
% \item Once loaded we can split the dataset into input variables (X) and the output class variable (Y).
% \end{itemize}
% \begin{lstlisting}
% # load pima indians dataset
% dataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")
% # split into input (X) and output (Y) variables
% X = dataset[:,0:8]
% Y = dataset[:,8]
% \end{lstlisting}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Define Model: Input}
 % \begin{itemize}
% \item  Models in Keras are defined as a sequence of layers.
% \item We create a Sequential model and add layers one at a time.
% \item The first thing to get right is to ensure the input layer has the right number of inputs. 
% \item This can be specified when creating the first layer with the input\_dim argument and setting it to 8 for the 8 input variables.
% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Define Model: Layers}
 % \begin{itemize}
% \item  How do we know the number of layers and their types?
% \item Use Heuristics, trial and error, experience.
% \item Here, we will use a fully-connected network structure with three layers.

% \end{itemize}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Define Model: Dense Layers}
 % \begin{itemize}
% \item  Fully connected layers are defined using the Dense class. 
% \item We can specify the number of neurons in the layer as the first argument, the initialization method as the second argument as init and specify the activation function using the activation argument.
% \end{itemize}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Define Model: Dense Layers Weights}
 % \begin{itemize}
% \item  We initialize the network weights to a small random number generated from a uniform distribution (`uniform'), in this case between 0 and 0.05 because that is the default uniform weight initialization in Keras. 
% \item Another traditional alternative would be `normal' for small random numbers generated from a Gaussian distribution.
% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Define Model: Dense Layers Activations}
 % \begin{itemize}
% \item  We will use the rectifier (`relu') activation function on the first two layers and the sigmoid function in the output layer.
% \item It used to be the case that sigmoid and tanh activation functions were preferred for all layers. 
% \item These days, better performance is achieved using the rectifier activation function. 
% \item We use a sigmoid on the output layer to ensure our network output is between 0 and 1 and easy to map to either a probability of class 1 or snap to a hard classification of either class with a default threshold of 0.5.
% \end{itemize}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Define Model: Dense Layers}
 % \begin{itemize}
% \item  The first layer has 12 neurons and expects 8 input variables. 
% \item The second hidden layer has 8 neurons and finally,
% \item the output layer has 1 neuron to predict the class (onset of diabetes or not).
% \end{itemize}
% \begin{lstlisting}
% # create model
% model = Sequential()
% model.add(Dense(12, input_dim=8, activation='relu'))
% model.add(Dense(8, activation='relu'))
% model.add(Dense(1, activation='sigmoid'))
% \end{lstlisting}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Compile Model}
 % \begin{itemize}
% \item  When compiling, we must specify some additional properties required when training the network. 
% \item Remember training a network means finding the best set of weights to make predictions for this problem.
% \item We must specify the loss function to use to evaluate a set of weights, the optimizer used to search through different weights for the network and any optional metrics we would like to collect and report during training.
% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Compile Model}
 % \begin{itemize}
% \item We will use logarithmic loss, which for a binary classification problem is defined in Keras as ``binary\_crossentropy''. 
% \item We will also use the efficient gradient descent algorithm ``adam'' for no other reason that it is an efficient default. 
% \item Note: No data has yet been provided to FIT the model
% \item We compiled it just to be ready for efficient computation.
% \end{itemize}
% \begin{lstlisting}
% # Compile model
% model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
% \end{lstlisting}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Fit Model}
 % \begin{itemize}
% \item Will run for a fixed number of iterations through the dataset called epochs, nepochs
% \item Can also set the number of instances that are evaluated before a weight update in the network is performed, called the batch size and set using the batch\_size argument.
% \item Both are to be chosen by heiristics, trial and error or by experince
% \item Will run for a small number of iterations (150) and use a relatively small batch size of 10
% \end{itemize}
% \begin{lstlisting}
% # Fit the model
% model.fit(X, Y, epochs=150, batch_size=10)
% \end{lstlisting}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Evaluate Model}
 % \begin{itemize}
% \item We have trained our neural network on the entire dataset and we can evaluate the performance of the network on the same dataset (for simplicity)
% \item Ideally another labelled data should have been used for testing (or some Cross Validation within Training dataset)
% \end{itemize}
% \begin{lstlisting}
% # evaluate the model
% scores = model.evaluate(X, Y)
% print("\n{}: {}".format(model.metrics_names[1], scores[1]*100))
% \end{lstlisting}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Make Predictions}
 % \begin{itemize}
% \item Use the fitted model to generate predictions on the training dataset, pretending it is a new dataset we have not seen before.
% \item For now, using X itself
% \end{itemize}
% \begin{lstlisting}
% # calculate predictions
% predictions = model.predict(X)
% # round predictions
% rounded = [round(x[0]) for x in predictions]
% print(rounded)

% [1.0, 0.0, ... 1.0, 0.0]
% \end{lstlisting}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}
  % \begin{center}
    % {\Large Keras Mutliclass Classification: Step by Step}
    
    % \tiny{(Ref:  Multi-Class Classification Tutorial with the Keras Deep Learning Library - Jason Brownlee)}
  % \end{center}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Data set}
 % \begin{itemize}
 % \item  Iris flower dataset http://archive.ics.uci.edu/ml/datasets/Iris
 % \item  4 input variables are numeric and have the same scale in centimeters. 
 % \item Each instance describes the properties of an observed flower measurements and the output variable is specific iris species.
 % \item This is a multi-class classification problem, meaning that there are more than two classes to be predicted, in fact there are three flower species.
% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Imports}
% \begin{lstlisting}
% import numpy
% import pandas
% from tf.keras.models import Sequential
% from tf.keras.layers import Dense
% from tf.keras.wrappers.scikit_learn import KerasClassifier
% from tf.keras.utils import np_utils
% from sklearn.model_selection import cross_val_score
% from sklearn.model_selection import KFold
% from sklearn.preprocessing import LabelEncoder
% from sklearn.pipeline import Pipeline

% # fix random seed for reproducibility
% seed = 7
% numpy.random.seed(seed)
% \end{lstlisting}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Load Data}
 % \begin{itemize}
 % \item  The dataset can be loaded directly. 
 % \item Because the output variable contains strings, it is easiest to load the data using pandas. 
 % \item We can then split the attributes (columns) into input variables (X) and output variables (Y).
 % \end{itemize}
% \begin{lstlisting}
% # load dataset
% dataframe = pandas.read_csv("iris.csv", header=None)
% dataset = dataframe.values
% X = dataset[:,0:4].astype(float)
% Y = dataset[:,4]
% \end{lstlisting}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{ Encode Output}
 % \begin{itemize}
 % \item  The output variable contains three different string values.
 % \item Need One Hot Encoding or creating dummy variables from a categorical variable
 % \item For example, in this problem three class values are Iris-setosa, Iris-versicolor and Iris-virginica
 % \item We can turn this into a one-hot encoded binary matrix for each data instance that would look as follows:
 % \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{ Encode Output}
% \begin{lstlisting}
% # encode class values as integers
% encoder = LabelEncoder()
% encoder.fit(Y)
% encoded_Y = encoder.transform(Y)
% # convert integers to dummy variables (i.e. one hot encoded)
% dummy_y = np_utils.to_categorical(encoded_Y)

% Iris-setosa,	Iris-versicolor,	Iris-virginica
% 1,		0,			0
% 0,		1, 			0
% 0, 		0, 			1
% \end{lstlisting}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{ Define Model}
 % \begin{itemize}
 % \item  Create a simple fully connected network with one hidden layer that contains 8 neurons.
 % \item The hidden layer uses a rectifier activation function
 % \item Due to 3 valued One hot encoding, the output layer must create 3 output values, one for each class. 
 % \item The output value with the largest value will be taken as the class predicted by the model.
 % \item The network topology of this simple one-layer neural network can be summarized as:
 % \end{itemize}
 % \begin{lstlisting}
% 4 inputs -> [8 hidden nodes] -> 3 outputs
% \end{lstlisting}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{ Define Model}
 % \begin{itemize}
 % \item  Softmax at the end layer,  to ensure the output values are in the range of 0 and 1 and may be used as predicted probabilities.
 % \item Use the efficient Adam gradient descent optimization algorithm with a logarithmic loss function, which is called ``categorical\_crossentropy'' in Keras.
 % \end{itemize}
 % \begin{lstlisting}
% # define baseline model
% def baseline_model():
	% # create model
	% model = Sequential()
	% model.add(Dense(8, input_dim=4, activation='relu'))
	% model.add(Dense(3, activation='softmax'))
	% # Compile model
	% model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
	% return model
% \end{lstlisting}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{KerasClassifier}
 % \begin{itemize}
 % \item  This neural network model can be used in Scikit Learn to leverage ML facilities there.
 % \item Create our KerasClassifier for use in scikit-learn
 % \item KerasClassifier takes the name of a function as an argument. 
 % \item This function (called ``baseline\_model'' here) must return the constructed neural network model, ready for training.
 % \end{itemize}
 % \begin{lstlisting}
% estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)
% \end{lstlisting}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Evaluate with k-Fold Cross Validation}
 % \begin{itemize}
 % \item  The scikit-learn has excellent capability to evaluate models using a suite of techniques, like K-Fold cross validation
 % \item We can evaluate our model (estimator) on our dataset (X and dummy\_y) using a 10-fold cross-validation procedure (kfold).
 % \end{itemize}
 % \begin{lstlisting}
 % kfold = KFold(n_splits=10, shuffle=True, random_state=seed)
% results = cross_val_score(estimator, X, dummy_y, cv=kfold)
% print("Accuracy: {}\% ({}\%)".format(results.mean()*100, results.std()*100))
% \end{lstlisting}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Results}
 % \begin{itemize}
 % \item  The results are summarized as both the mean and standard deviation of the model accuracy on the dataset.
 % \item  This is a reasonable estimation of the performance of the model on unseen data.
 % \end{itemize}
 % \begin{lstlisting}
% Accuracy: 97.33% (4.42%)
% \end{lstlisting}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}
  % \begin{center}
    % {\Large Keras Regression: Step by Step}
    
    % \tiny{(Ref:  Regression Tutorial with the Keras Deep Learning Library in Python, Jason Brownlee)}
  % \end{center}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Data set}
 % \begin{itemize}
 % \item   Boston house price dataset : https://archive.ics.uci.edu/ml/datasets/Housing
 % \item  The dataset describes 13 numerical properties of houses in Boston suburbs and is concerned with modeling the price of houses in those suburbs in thousands of dollars. 
 % \item As such, this is a regression predictive modeling problem. 
 % \item Input attributes include things like crime rate, proportion of nonretail business acres, chemical concentrations and more.
% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Imports}
% \begin{lstlisting}
% import numpy
% import pandas
% from tf.keras.models import Sequential
% from tf.keras.layers import Dense
% from tf.keras.wrappers.scikit_learn import KerasRegressor
% from sklearn.model_selection import cross_val_score
% from sklearn.model_selection import KFold
% from sklearn.preprocessing import StandardScaler
% from sklearn.pipeline import Pipeline
% \end{lstlisting}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Load Data}
 % \begin{itemize}
 % \item The dataset is in fact not in CSV format, instead separated by whitespace. 
 % \item We can then split the input (X) and output (Y) attributes
 % \end{itemize}
% \begin{lstlisting}
% # load dataset
% dataframe = pandas.read_csv("housing.csv", delim_whitespace=True, header=None)
% dataset = dataframe.values
% # split into input (X) and output (Y) variables
% X = dataset[:,0:13]
% Y = dataset[:,13]
% \end{lstlisting}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras Regressor}
 % \begin{itemize}
 % \item Scikit-learn excels at evaluating models and will allow us to use powerful data preparation and model evaluation schemes with very few lines of code.
 % \item The Keras wrappers require a function as an argument. This function that we must define is responsible for creating the neural network model to be evaluated.
 % \item No activation function is used for the output layer because it is a regression problem and we are interested in predicting numerical values directly without transform.
 % \end{itemize}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras Regressor}
 % \begin{lstlisting}
% def baseline_model():
	% # create model
	% model = Sequential()
	% model.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))
	% model.add(Dense(1, kernel_initializer='normal'))
	% # Compile model
	% model.compile(loss='mean_squared_error', optimizer='adam')
	% return model
% \end{lstlisting}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras Regressor}
 % \begin{itemize}
 % \item The Keras wrapper object for use in scikit-learn as a regression estimator is called KerasRegressor. 
 % \item We create an instance and pass it both the name of the function to create the neural network model as well as some parameters to pass along to the fit() function of the model later, such as the number of epochs and batch size.
 % \end{itemize}
% \begin{lstlisting}
% # fix random seed for reproducibility
% seed = 7
% numpy.random.seed(seed)
% # evaluate model with standardized dataset
% estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=100, batch_size=5, verbose=0)
% \end{lstlisting}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras Regressor}
 % \begin{itemize}
 % \item The Keras wrapper object for use in scikit-learn as a regression estimator is called KerasRegressor. 
 % \item We create an instance and pass it both the name of the function to create the neural network model as well as some parameters to pass along to the fit() function of the model later, such as the number of epochs and batch size.
 % \end{itemize}
% \begin{lstlisting}
% # fix random seed for reproducibility
% seed = 7
% numpy.random.seed(seed)
% # evaluate model with standardized dataset
% estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=100, batch_size=5, verbose=0)
% \end{lstlisting}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Results}
% The final step is to evaluate this baseline model. We will use 10-fold cross validation to evaluate the model.
% \begin{lstlisting}
% kfold = KFold(n_splits=10, random_state=seed)
% results = cross_val_score(estimator, X, Y, cv=kfold)
% print("Results: {}({}) MSE".format(results.mean(), results.std()))

% Results: 31.64 (26.82) MSE
% \end{lstlisting}
% \end{frame}



% % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}
  % \begin{center}
    % {\Large Keras: In Depth}
    
    % {Ref: Deep Learning using Keras- alyosamah}
  % \end{center}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Sequential Models}
% \begin{center}
% \includegraphics[width=\linewidth,keepaspectratio]{kr3}
% \end{center}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras Models}
 % Model Class API
% \begin{itemize}
% \item Optimized over all outputs Graph model 
% \item Allows for two or more independent networks to diverge or 
% merge 
% \item Allows for multiple separate inputs or outputs 
% \item Different merging layers (sum or concatenate)
% \end{itemize}
% \begin{lstlisting}
% from tf.keras.models import Model 
% from tf.keras.layers import Input, Dense 

% a = Input(shape=(32,)) 
% b = Dense(32)(a) 
% model = Model(inputs=a, outputs=b)
% \end{lstlisting}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras Layers}
% Layers are used to define what your architecture. Examples of layers are:
% \begin{itemize}
% \item  Dense layers (this is the normal, fully-connected 
% layer)
% \item  Convolutional layers (applies convolution operations on the previous layer)
% \item  Pooling layers (used after convolutional layers)
% \item  Dropout layers (these are used for regularization, to avoid overfitting)
% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras Layers}
% Keras has a number of pre-built layers. Notable examples include: Regular dense, MLP type
% \begin{lstlisting}
% keras.layers.core.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', 
% bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, 
% kernel_constraint=None, bias_constraint=None)
% \end{lstlisting}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras Layers}
% Recurrent layers, LSTM, GRU, etc

% \begin{lstlisting}
% keras.layers.recurrent.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', 
% use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', 
% bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, 
% bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, 
% bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)
% \end{lstlisting}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras Layers}
% 2D Convolutional layers 

% \begin{lstlisting}
% keras.layers.convolutional.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', 
% data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, 
% kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, 
% bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)
% \end{lstlisting}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras Layers}
% Autoencoders can be built with any other type of layer

% \begin{lstlisting}
% from tf.keras.layers import Dense, Activation
% model.add(Dense(units=32, input_dim=512))
% model.add(Activation('relu'))
% model.add(Dense(units=512))
% model.add(Activation('sigmoid'))
% \end{lstlisting}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras Layers}
% Other types of layer include: 
% \begin{itemize}
% \item  Noise 
% \item  Pooling 
% \item  Normalization 
% \item  Embedding 
% \item  And many more \ldots
% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras Activations}
% All your favorite activations are available: 
% \begin{itemize}
% \item  Sigmoid, tanh, ReLu, softplus, hard sigmoid, linear 
% \item  Advanced activations implemented as a layer (after desired neural layer) 
% \item  Advanced activations: LeakyReLu, PReLu, ELU, Parametric Softplus, Thresholded linear and 
% Thresholded Relu
% \end{itemize}
% \begin{center}
% \includegraphics[width=0.8\linewidth,keepaspectratio]{kr4}
% \end{center}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras Losses}
% \begin{center}
% \includegraphics[width=\linewidth,keepaspectratio]{kr5}
% \end{center}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras Optimizers}
% An optimizer is one of the two arguments required for compiling a Keras model:

% \begin{lstlisting}
% from tf.keras import optimizers 
% model = Sequential() 
% model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,))) 
% model.add(Activation('tanh')) 
% sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) 
% model.compile(loss='mean_squared_error', optimizer=sgd)
% \end{lstlisting}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras Optimizers}

% \begin{itemize}
% \item  SGD :Stochastic gradient descent optimizer.
% \item RMSprop: RMSProp optimizer  is usually a good choice for recurrent neural networks.
% Adagrad
% \item Nadam : Much like Adam is essentially RMSprop with momentum, Nadam is Adam RMSprop with Nesterov
% momentum
% \item Adadelta
% \item Adam
% \item You can also use a wrapper class for native TensorFlow optimizers TFOptimizer
% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras Optimizers}
% \begin{lstlisting}
% keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)
% keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0) 
% keras.optimizers.Adagrad(lr=0.01, epsilon=1e-08, decay=0.0) 
% keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)
% keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) 
% keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, 
% schedule_decay=0.004) 

% keras.optimizers.TFOptimizer(optimizer) 
% \end{lstlisting}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{ Learning Rate Scheduler}
% In Keras you have two types of learning rate schedule:
% \begin{center}
% \includegraphics[width=\linewidth,keepaspectratio]{kr6}
% \end{center}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{ Time based Learning Rate Scheduler}
% \begin{center}
% \includegraphics[width=\linewidth,keepaspectratio]{kr7}
% \end{center}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{ Drop based Learning Rate Scheduler}
% \begin{center}
% \includegraphics[width=\linewidth,keepaspectratio]{kr8}
% \end{center}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{  Tips for Using Learning Rate}
% \begin{center}
% \includegraphics[width=\linewidth,keepaspectratio]{kr9}
% \end{center}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras Metrics}

% \begin{itemize}
% \item   Accuracy
% \begin{itemize}
% \item  binary\_accuracy
% \item  categorical\_accuracy
% \item  sparse\_categorical\_accuracy
% \item  top\_k\_categorical\_accuracy
% \item  sparse\_top\_k\_categorical\_accuracy
% \end{itemize}
% \item  Precision 
% \item  Recall
% \item  FScore
% \end{itemize}
% \begin{lstlisting}
% from tf.keras import metrics 
% model.compile(loss='categorical_crossentropy', 
% optimizer='adadelta', 
% metrics=['accuracy', 'f1score', 'precision', 'recall'])
% \end{lstlisting}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras Metrics}
% Custom metrics can be passed at the compilation step. The function would need to 
% take (y\_true, y\_pred) as arguments and return a single tensor value.
% \begin{lstlisting}
% import keras.backend as K 
% def mean_pred(y_true, y_pred): 
% return K.mean(y_pred)
% model.compile(optimizer='rmsprop', loss='binary_crossentropy', 
% metrics=['accuracy', mean_pred])
% \end{lstlisting}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras Performance evaluation strategies}
% The large amount of data and the complexity of the models require very long training times.
% Keras provides a three convenient ways of evaluating your deep learning algorithms :
% \begin{itemize}
% \item Use an automatic verification dataset.
% \item Use a manual verification dataset.
% \item Use a manual k-Fold Cross Validation.
% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras Performance evaluation strategies}
% \begin{center}
% \includegraphics[width=\linewidth,keepaspectratio]{kr10}
% \end{center}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Automatic Verification Dataset}
% Keras can separate a portion of your training data into a validation dataset and evaluate the 
% performance of your model on that validation dataset each epoch
% \begin{center}
% \includegraphics[width=\linewidth,keepaspectratio]{kr11}
% \end{center}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Manual Verification Dataset}
% Keras also allows you to manually specify the dataset to use for validation during training.
% \begin{lstlisting}
% # MLP with manual validation set
% from sklearn.model_selection import train_test_split
% import numpy
% # fix random seed for reproducibility
% seed = 7
% numpy.random.seed(seed)
% X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)
% # Fit the model
% model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=150, batch_size=10)
% \end{lstlisting}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{ Manual k-Fold Cross Validation}
% \begin{lstlisting}
% from sklearn.model_selection import StratifiedKFold
% kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)
% cvscores = []
% for train, test in kfold.split(X, Y):
	% # create model
	% model = Sequential()
	% model.add(Dense(12, input_dim=8, activation='relu'))
	% model.add(Dense(8, activation='relu'))
	% model.add(Dense(1, activation='sigmoid'))
	% model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	% # Fit the model
	% model.fit(X[train], Y[train], epochs=150, batch_size=10, verbose=0)
	% # evaluate the model
	% scores = model.evaluate(X[test], Y[test], verbose=0)
	% print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))
	% cvscores.append(scores[1] * 100)
% print("%.2f%% (+/- %.2f%%)" % (numpy.mean(cvscores), numpy.std(cvscores)))
% \end{lstlisting}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras Regularizers}
% Nearly everything in Keras can be regularized to avoid overfitting 
% In addition to the Dropout layer, there are all sorts of other regularizers available, such as:
% \begin{itemize}
% \item  Weight regularizers
% \item  Bias regularizers
% \item  Activity regularizers
% \end{itemize}
% \begin{lstlisting}
% from tf.keras import regularizers 
% model.add(Dense(64, input_dim=64, 
% kernel_regularizer=regularizers.l2(0.01), 
% activity_regularizer=regularizers.l1(0.01)))
% \end{lstlisting}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras Pickle}
% Saving/loading whole models (architecture + weights + optimizer state)
% \begin{lstlisting}
% from tf.keras.models import load_model
% model.save('my_model.h5') # creates a HDF5 file 'my_model.h5'
% del model # deletes the existing model
% # returns a compiled model
% # identical to the previous one
% model = load_model('my_model.h5')
% \end{lstlisting}
% You can also save/load only a model's architecture or weights only. 
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras Model Visualization}
% In case you want an image of your model :
% \begin{lstlisting}
% from tf.keras.utils import plot_model
% plot_model(model, to_file='model.png')
% \end{lstlisting}
% \begin{center}
% \includegraphics[width=\linewidth,keepaspectratio]{kr12}
% \end{center}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras Callbacks}
% Allow for function call during training:
% \begin{itemize}
% \item  Callbacks can be called at different points of training (batch or epoch) 
% \item  Existing callbacks: Early Stopping, weight saving after epoch 
% \item  Easy to build and implement, called in training function, fit()
% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras Callbacks Examples}
% \begin{itemize}
% \item  TerminateOnNaN : Callback that terminates training when a NaN loss is encountered.
% \item  EarlyStopping: Stop training when a monitored quantity has stopped improving.
% \item  ModelCheckpoint : Save the model after every epoch.
% \begin{lstlisting}
% keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, 
% save_best_only=False, save_weights_only=False, mode='auto', period=1)
% \end{lstlisting}
% \item ReduceLROnPlateau: Reduce learning rate when a metric has stopped improving.
% \begin{lstlisting}
% keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, 
% patience=10, verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0) 
% \end{lstlisting}
% \item Also, You can create a custom callback by extending the base class keras.callbacks.Callback
% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Keras + TensorBoard: Visualizing Learning}
% \begin{itemize}
% \item  TensorBoard is a visualization tool provided with TensorFlow.
% \item  This callback writes a log for TensorBoard, which allows you to visualize dynamic graphs of 
% your training and test metrics, as well as activation histograms for the different layers in your 
% model.
% \begin{lstlisting}
% tbCallBack= keras.callbacks.TensorBoard(log_dir='./logs', 
% histogram_freq=0, batch_size=32, write_graph=True, 
% write_grads=False, write_images=False, embeddings_freq=0, 
% embeddings_layer_names=None, embeddings_metadata=None) 

% # Use your terminal
% tensorboard --logdir path_to_current_dir/logs
% \end{lstlisting}
% \end{itemize}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}
  % \begin{center}
    % {\Large Classification in Keras}
    
    % {Ref: Deep Learning A-Z - Udemy}
  % \end{center}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Customer Churn}
% Bank customers churn (exits)

% Dataset:
% \begin{center}
% \includegraphics[width=0.7\linewidth,keepaspectratio]{kr13}
% \end{center}
% Target: ``Exited''.

% Inputs: Some of the other columns.
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Get Inputs}
% \begin{lstlisting}
% # Importing the libraries
% import numpy as np
% import matplotlib.pyplot as plt
% import pandas as pd

% # Importing the dataset
% dataset = pd.read_csv('Churn_Modelling.csv')
% X = dataset.iloc[:, 3:13].values
% y = dataset.iloc[:, 13].values
% \end{lstlisting}
% First 2 columns are not relevant for modeling, so skipped.
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Process Inputs}
% Convert categorical variables to Integers. 
% \begin{lstlisting}
% # Encoding categorical data
% from sklearn.preprocessing import LabelEncoder, OneHotEncoder
% labelencoder_X_1 = LabelEncoder()
% X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])
% labelencoder_X_2 = LabelEncoder()
% X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])
% # Country is not ordinal, so make it one hot
% onehotencoder = OneHotEncoder(categorical_features = [1])
% X = onehotencoder.fit_transform(X).toarray()
% X = X[:,1:]
% \end{lstlisting}
% Cities being non-ordinal (not ranked), need to be convrted to One-hot. So 3 new dummy columns got added.
% One of them is removed to avoid ``dummy variable trap'' ( multicollinear - a scenario in which two or more variables are highly correlated; in simple terms one variable can be predicted from the others.)
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Process Inputs}
% Its necessary to normalize feature values
% \begin{lstlisting}
% # Feature Scaling
% from sklearn.preprocessing import StandardScaler
% sc = StandardScaler()
% X_train = sc.fit_transform(X_train)
% X_test = sc.transform(X_test)
% \end{lstlisting}
% Input X looks like:
% \begin{center}
% \includegraphics[width=0.4\linewidth,keepaspectratio]{kr14}
% \end{center}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Build Classifier}
% Basic NN, defined by layers one by one.
% \begin{lstlisting}
% import keras
% from tf.keras.models import Sequential
% from tf.keras.layers import Dense


% classifier = Sequential()
% classifier.add(Dense(6,kernel_initializer='uniform', activation='relu',input_shape=(11,)))
% \end{lstlisting}
% Adding Hidden layer and specifying input dimensions there. Dense is a function between two layers. It randomly inittializes the weights to small numbers close to 0 (but not 0).

% Tip: Number of nodes in the hidden layer = average of input and output dimensions. So, Avrg(11,1) = 6.
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Build Classifier}
% Adding more layers
% \begin{lstlisting}
% classifier.add(Dense(6,kernel_initializer='uniform', activation='relu'))
% classifier.add(Dense(1,kernel_initializer='uniform', activation='sigmoid'))

% classifier.compile(optimzer='adam',loss="binary_crossentropy")
% \end{lstlisting}
% The last activation has to be sigmoid (or softamax for more than 2 categories) to get the probabilities.

% Finally compile by spcifying adam (a Stochastic Gradient Descent) algorithm. For more than 2 categories loss is ``categorical\_crossentropy''.
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Fit with X and y}
% \begin{lstlisting}
% classifier.fit(X_train,y_train,batch_size=10, nb_epoch=100)

% Epoch 98/100
% 8000/8000 [==============================] - 1s 144us/step - loss: 0.4002 - acc: 0.8361
% Epoch 99/100
% 8000/8000 [==============================] - 1s 143us/step - loss: 0.4001 - acc: 0.8344
% Epoch 100/100
% 8000/8000 [==============================] - 1s 131us/step - loss: 0.4004 - acc: 0.8339
% \end{lstlisting}
% Batch is the number of records after which the weights are updated.
% Accuracy goes on increasing and reaches to 83\%
% \end{frame}



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Using model}
% \begin{lstlisting}
% # Predicting the Test set results
% y_pred = classifier.predict(X_test)
% y_pred = (y_pred > 0.5)

% # Making the Confusion Matrix
% from sklearn.metrics import confusion_matrix
% cm = confusion_matrix(y_test, y_pred)

% array([[1542,   53],
       % [ 261,  144]], dtype=int64)
% \end{lstlisting}
% Need to decide THRESHOLD to decide boolean output. 0.50 is ok here.
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Predict}
% Unseen Customer \ldots
% \begin{itemize}
% \item Geography: France
% \item Credit Score: 600
% \item Gender: Male
% \item Age: 40 years old
% \item Tenure: 3 years
% \item Balance: \$60000
% \item Number of Products: 2
% \item Does this customer have a credit card ? Yes
% \item Is this customer an Active Member: Yes
% \item Estimated Salary: \$50000
% \end{itemize}
% Will the customer leave?
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Predict}
% \begin{lstlisting}
% new_test_x = np.array([[0,0,600,1,40,3,60000, 2,1,1,50000]])
% new_test_x = sc.transform(new_test_x)
% new_prediction = classifier.predict(new_test_x)
% new_prediction = (new_prediction > 0.5)
% \end{lstlisting}
% FALSE!!!
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Evaluating}
% \begin{itemize}
% \item Using only one x\_test set may not give a representative result and accuracy. 
% \item Need to use K-fold which uses different test sets for evaluation. 
% \item This solves High Variance problems (diff test sets given divergent results).
% \item For k=10 folds, we will get 10 accuracies. 
% \item We can find mean and std deviation of those 10 values. 
% \item Based on these, we can be one of following 4 cases
% \end{itemize}
% \begin{center}
% \includegraphics[width=0.35\linewidth,keepaspectratio]{kr15}
% \end{center}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{K-Fold with Keras}
% \begin{itemize}
% \item K-fold functionality is in Scikit Learn.
% \item Keras can use it by wrapping it.
% \item KerasClassifier needs a function that builds and returns classifier NN
% \end{itemize}
% \begin{lstlisting}
% from tf.keras.wrappers.scikit_learn import KerasClassifier
% from sklearn.model_selection import cross_val_score

% def build_classifier():
    % classifier = Sequential()
    % classifier.add(Dense(6,kernel_initializer='uniform', activation='relu',input_shape=(11,)))
    % classifier.add(Dense(6,kernel_initializer='uniform', activation='relu'))
    % classifier.add(Dense(1,kernel_initializer='uniform', activation='sigmoid'))
    % classifier.compile(optimizer='adam',loss="binary_crossentropy", metrics = ['accuracy'])
    % return classifier
% \end{lstlisting}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{K-Fold with Keras}
% \begin{itemize}
% \item Individual classifiers within the function will be spawed on different test-train sets, during K-folds
% \item A common classfier representing total classification process is defined with the build\_classifier function.
% \item cross\_val\_score from sklearn will return set of all accuracies.
% \item Find mean and std dev to see where we are.
% \end{itemize}
% \begin{lstlisting}
% classifier_common = KerasClassifier(build_fn=build_classifier,batch_size = 10, nb_epoch=100)
% accuracies = cross_val_score(estimator=classifier_common,X=X_train,y=y_train,cv=10,n_jobs=-1)
% mean = accuracies.mean()
% variance = accuracies.std()
% \end{lstlisting}
% Put -1 for n\_jobs to use all CPUs.
% \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Transfer Learning}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{dl_tf2_8}
\end{center}


\tiny{(Ref: Introduction to TensorFlow 2.0 - Brad Miro)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Transfer Learning}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{dl_tf2_9}
\end{center}


\tiny{(Ref: Introduction to TensorFlow 2.0 - Brad Miro)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Transfer Learning}

\begin{lstlisting}
import tensorflow as tf 
base_model = tf.keras.applications.SequentialMobileNetV2(
                 input_shape=(160, 160, 3),
                 include_top=False,
                 weights="imagenet")
								 
base_model.trainable = False
model = tf.keras.models.Sequential([
  base_model,
  tf.keras.layers.GlobalAveragePooling2D(),
  tf.keras.layers.Dense(1)
])
# Compile and fit
\end{lstlisting}


\tiny{(Ref: Introduction to TensorFlow 2.0 - Brad Miro)}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Transfer Learning with TF Hub}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{dl_tf2_10}
\end{center}




\tiny{(Ref: Introduction to TensorFlow 2.0 - Brad Miro)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Learning More \ldots}

\begin{itemize}
\item  Latest tutorials and guides at http://tensorflow.org/beta
\item Book: Hands-on ML with Scikit-Learn, Keras and TensorFlow (2nd edition) 
\end{itemize}
\tiny{(Ref: Intro to TensorFlow 2.0 - Josh Gordon)}
\end{frame}