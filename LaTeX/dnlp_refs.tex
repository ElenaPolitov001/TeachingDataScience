\begin{frame}\frametitle{References}
\begin{itemize}
\item Natural Language Processing - Information Extractoin, Christopher Manning
\item Word2Vec - (Girish K.,Sivan Biham \& Adam Yaari, Adrian Colyer)
\item Deep Learning for Natural Language Processing - Sihem Romdhani
\item Notebooks and Material @ https://github.com/rouseguy/DeepLearningNLP\_Py
% \item A key idea from the highly cited paper:
% \begin{quote}
% Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S. Corrado, and
% Jeff Dean. ``Distributed representations of words and phrases
% and their compositionality." In Advances in neural information
% processing systems, pp. 3111-3119. (2013).
% \end{quote}
% \item And the closely related follow-up:
% \begin{quote}
% Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean.
% ``Efficient estimation of word representations in vector space."
% arXiv preprint arXiv:1301.3781 (2013).
% \end{quote}
% \item Want to use the context of a word to predict other similar
% words that tend to co-occur with it.
% \item  Two approaches used are called {skip grams} and {continuous bag of words}.
\end{itemize}
\end{frame}

