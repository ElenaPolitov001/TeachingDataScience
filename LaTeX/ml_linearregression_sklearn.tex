%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Linear Regression with Scikit-Learn}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Template code for Linear Regression}
\begin{lstlisting}
from sklearn import linear_model 

x_train=input_variables_values_training_datasets 
y_train=target_variables_values_training_datasets 
x_test=input_variables_values_test_datasets 

linear = linear_model.LinearRegression() 

linear.fit(x_train, y_train) 
linear.score(x_train, y_train) 

print('Coefficient: \n', linear.coef_) 
print('Intercept: \n', linear.intercept_) 

predicted= linear.predict(x_test)
\end{lstlisting}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Linear Regression}
{\tiny (Ref: Metrics To Evaluate Machine Learning Algorithms in Python - Jason Brownlee)}

\begin{lstlisting}
import pandas
from sklearn import model_selection
from sklearn.linear_model import LinearRegression
url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.data"
names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']
dataframe = pandas.read_csv(url, delim_whitespace=True, names=names)
array = dataframe.values
X = array[:,0:13]
Y = array[:,13]
seed = 7
kfold = model_selection.KFold(n_splits=10, random_state=seed)
model = LinearRegression()
scoring = 'neg_mean_absolute_error'
results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
print("MAE: %.3f (%.3f)") % (results.mean(), results.std())

MAE: -4.005 (2.084)
\end{lstlisting}



\end{frame}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Artificial Data Creation}
% Fitting a line to sample $ (x,y)$ data

\begin{center}
\includegraphics[width=0.5\linewidth]{regrex1}
\end{center}

\begin{lstlisting}
import matplotlib.pyplot as plt
import numpy as np

rng = np.random.RandomState(42)
x = 10 * rng.rand(50)
y = 2 * x - 1 + rng.randn(50)
plt.scatter(x, y);
\end{lstlisting}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Data Arrangement}
\begin{itemize}
\item Arrange data into a features matrix and target vector.
\item Requires a two-dimensional features matrix and a one-dimensional target array.
\item Simple reshaping: \lstinline|X = x[:, np.newaxis]|
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Model Selection}
\begin{itemize}
\item For a simple linear regression model use: \lstinline|from sklearn.linear_model import LinearRegression|
\item Choose model hyperparameters \lstinline|model = LinearRegression(fit_intercept=True)|
\item Documentation:

\end{itemize}

\begin{lstlisting}
LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Training}
\begin{itemize}
\item Fit the model to your data \lstinline|model.fit(X, y)|
\item This fit() causes many internal computations to take place, and the results are stored in \lstinline|model.coef_| and \lstinline|model.intercept_|
\item These two parameters represent the slope and intercept of the simple linear fit to the data. 
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Model Parameters}
\begin{itemize}
\item During the fitting process, the state of the estimator is stored in instance attributes that have a trailing underscore ('\_'). 
\item For example, the coefficients of a LinearRegression estimator are stored in the attribute \texttt{coef\_}:
\end{itemize}

\begin{lstlisting}
est.coef_   # access coefficients
# Output : array([ 0.33176871,  0.34910639])
\end{lstlisting}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Testing}
\begin{itemize}
\item Predict labels for unknown data \lstinline|xtest = np.linspace(-1, 11)|
\item Similar to training data this unseen data also needs to be reshaped into required matrix form
\end{itemize}

\begin{lstlisting}
Xtst = xtest[:, np.newaxis]
ytst = model.predict(Xtst)
# Visualize

plt.plot(Xtst, ytst);
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Simple linear regression}
\begin{center}
\includegraphics[width=0.9\linewidth]{regrex2}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Extended Exercise}
\begin{itemize}
\item Make current x list of values as x1
\item Create another list as x2 (same dimensions!!)
\item Incorporate both x1 and x2 in X
\item Build the model
\item Make appropriate changes while testing.
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile]\frametitle{Another Example}
%
%\begin{lstlisting}
%from sklearn.linear_model import LinearRegression
%import numpy as np
%
%# random training data
%X = np.random.rand(10, 2)
%y = np.random.randint(2, size=10)
%
%est = LinearRegression(fit_intercept=False)
%est.fit(X, y)
%\end{lstlisting}
%\end{frame}
%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Test Case: Glass Identification }
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Glass identification Dataset}

\begin{center}
\includegraphics[width=0.9\linewidth]{glassdt}
\end{center}


\begin{itemize}
\item Toy Dataset:  http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data
\item  Pretend that we want to predict ri, and our only feature is al. How could we do it using machine learning?
\end{itemize}
%\begin{lstlisting}
%import pandas as pd
%advertising = pd.read_csv('../data/Advertising.csv')
%advertising.head(5)
%\end{lstlisting}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Example: Glass identification Dataset}
\begin{lstlisting}
import pandas as pd
col_names = ['id','ri','na','mg','al','si','k','ca','ba','fe','glass_type']
glass = pd.read_csv('glass.data', names=col_names, index_col='id')
glass.sort('al', inplace=True)
print(glass.head())
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Example: Glass identification Dataset}
\begin{center}
\includegraphics[width=0.9\linewidth]{glassdt}
\end{center}
%\tiny
\begin{itemize}
\item Question: Pretend that we want to predict ri, and our only feature is al. How could we do it using machine learning?
\item Answer: We could frame it as a regression problem, and use a linear regression model with al as the only feature and ri as the response.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Glass identification Dataset}
\begin{itemize}
\item Question: How would we visualize this model?
\item Answer: Create a scatter plot with al on the x-axis and ri on the y-axis, and draw the line of best fit.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Glass identification Dataset}
\begin{center}
\includegraphics[width=0.45\linewidth]{glasssns}
\end{center}

\begin{lstlisting}
import seaborn as sns
import matplotlib.pyplot as plt

sns.set(font_scale=1.5)
sns.lmplot(x='al', y='ri', data=glass, ci=None)
\end{lstlisting}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Glass identification Dataset}
Without using Seaborn? Use Plot from Pandas

\begin{center}
\includegraphics[width=0.6\linewidth]{glasspd}
\end{center}

\begin{lstlisting}
# scatter plot using Pandas
glass.plot(kind='scatter', x='al', y='ri')
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Glass identification Dataset}
In Matplotlib?

\begin{center}
\includegraphics[width=0.6\linewidth]{glassplt}
\end{center}

\begin{lstlisting}
# equivalent scatter plot using Matplotlib
plt.scatter(glass.al, glass.ri)
plt.xlabel('al')
plt.ylabel('ri')
\end{lstlisting}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Linear Regression Model}
\begin{lstlisting}
from sklearn.linear_model import LinearRegression

linreg = LinearRegression()
feature_cols = ['al']
X = glass[feature_cols]
y = glass.ri

linreg.fit(X, y)

glass['ri_pred'] = linreg.predict(X)

print(glass.head())
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Linear Regression Model}
\begin{center}
\includegraphics[width=0.9\linewidth]{glasspred}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Plot}
\begin{center}
\includegraphics[width=0.6\linewidth]{glasspredplot}
\end{center}


\begin{lstlisting}
# plot those predictions connected by a line
plt.plot(glass.al, glass.ri_pred, color='red')
plt.xlabel('al')
plt.ylabel('Predicted ri')
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Plot}

\begin{center}
\includegraphics[width=0.6\linewidth]{glasspredplottog}
\end{center}

\begin{lstlisting}
# put the plots together
plt.scatter(glass.al, glass.ri)
plt.plot(glass.al, glass.ri_pred, color='red')
plt.xlabel('al')
plt.ylabel('ri')
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Refresher: interpreting linear regression coefficients}

Linear regression equation: $ y = \beta_0 + \beta_1 x$

Interpretation: 1 unit increase in 'al' is associated with a 0.0025 unit decrease in 'ri'.
\begin{lstlisting}
# compute prediction for al=2 using the equation
linreg.intercept_ + linreg.coef_ * 2 
# array([ 1.51699012])

# compute prediction for al=2 using the predict method
linreg.predict(2) 
# array([ 1.51699012])

# examine coefficient for al
zip(feature_cols, linreg.coef_) 
# [('al', -0.002477606387469623)]
\end{lstlisting}

\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{}
% \begin{center}
% {\Large Test Case: Advertising Budgets }
% \end{center}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Advertising Dataset}
% \begin{itemize}
% \item Toy Dataset: Advertising, Advertising.csv
% \item Sales totals for a product in 200 different markets. Advertising budget in each market, broken down into TV, radio, newspaper
% \end{itemize}
% %\begin{lstlisting}
% %import pandas as pd
% %advertising = pd.read_csv('../data/Advertising.csv')
% %advertising.head(5)
% %\end{lstlisting}
% \begin{center}
% \includegraphics[width=0.8\linewidth]{advtdata}
% \end{center}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Problem}
% \begin{itemize}
% \item Goal: What marketing plan for next year will result in high product sales?
% \item Questions (Hypothesis):
	% \begin{itemize}
	% \item Is there a relationship between advertising budget and sales?
	% \item How strong is the relationship between advertising budget and sales?
	% \item Which media contribute to sales?
	% \item How accurately can we estimate the effect of each medium on sales?
	% \item Is the relationship linear?
	% \item Is there any interaction effect? (called ``synergy'' in business). Example: spending 50k on TV ads + 50k on radio ads results in more sales than spending 100k on only TV
	% \end{itemize}
% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Advertising Dataset}
% \begin{itemize}
% \item What are some ways we can regress sales onto adverting using Simple Linear Regression?
% \item One model:
% \begin{align*}
% sales & \approx \beta_0 +  \beta_1 \times TV \\
% y & \approx \beta_0 + \beta_1 x
% \end{align*}
% \item Scatter plot visualization for TV and Sales.
% \begin{lstlisting}
% %matplotlib inline
% advertising.plot.scatter(x='TV', y='Sales');
% \end{lstlisting}
% \begin{center}
% \includegraphics[width=0.4\linewidth]{advtscatter}
% \end{center}
% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Advertising Dataset}
% \begin{itemize}
% \item Simple Linear Model in Python (using pandas and scikit): Predictor: $x$, Response: $y$
% \begin{lstlisting}
% reg = linear_model.LinearRegression()
% reg.fit(advertising['TV'].reshape(-1,1), advertising['Sales'].reshape(-1,1))
% print('Coefficients: \n', reg.coef_)
% print('Intercept: \n', reg.intercept_)

% Coefficients: [[ 0.04753664]] 
% Intercept: [ 7.03259355]
% \end{lstlisting}
% \item $Sales = 7.03259 + 0.04754  \times  TV$
% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Advertising Dataset}
% Scatter plot visualization for TV and Sales with Linear Model.
% \begin{lstlisting}
% plt.scatter(advertising['TV'], advertising['Sales'],  color='black')
% plt.plot(advertising['TV'], reg.predict(advertising['TV'].reshape(-1,1)), 
        % color='blue', linewidth=3)

% \end{lstlisting}
% \begin{center}
% \includegraphics[width=0.6\linewidth]{advtscatterline}
% \end{center}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{}
% \begin{center}
% {\Large Test Case: Loan Disbursement}
% \end{center}
% \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile]\frametitle{Assignment: Line fitting on real data}
%
%\begin{itemize}
%\item The Lending Club is a peer-to-peer lending site where members make loans to each other. 
%\item The site makes anonymized data on loans and borrowers publicly available.
%\item Just look at how borrower FICO score affects interest rate charged.
%\item {\bf Implement:} Load the csv in pandas and plot FICO score and Interest Rate.
%\end{itemize}
%
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile]\frametitle{Line fitting on real data}
%
%\begin{lstlisting}
%import pandas as pd
%df = pd.read_csv('../data/loanf.csv')
%
%intrate = df['Interest.Rate']
%fico = df['FICO.Score']
%p = plot(fico,intrate,'o')
%ax = gca()
%xt = ax.set_xlabel('FICO Score')
%yt = ax.set_ylabel('Interest Rate %')
%\end{lstlisting}
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile]\frametitle{Line fitting on real data}
%\begin{center}
%\includegraphics[width=0.9\linewidth]{loan1}
%\end{center}
%\end{frame}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile]\frametitle{Line fitting on real data}
%\begin{center}
%\includegraphics[width=0.5\linewidth]{loan1}
%\end{center}
%\begin{itemize}
%\item Here we see a distinct downward linear trend where Interest Rate goes down with increasing FICO score. 
%\item But we also see that for the same FICO score there is a range of Interest rates.
%\item This suggests that FICO by itself might not be enough to predict Interest Rate.
%\end{itemize}
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile]\frametitle{How can I predict?}
%How can I predict interest rates based on borrower and loan attributes? 
%
%Steps:
%\begin{itemize}
%\item Browse the data
%\item Data cleanup
%\item Visual exploration
%\item Model derivation
%\end{itemize}
%%\begin{center}
%%\includegraphics[width=0.5\linewidth]{loan1}
%%\end{center}
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile]\frametitle{Browse Data}
%{\bf ToDo:} Find all the variables, their data types, their ranges, meanings, etc.
%\end{frame}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile]\frametitle{Browse Data}
%\begin{itemize}
%\item Amount.Requested - numeric. The amount (in dollars) requested in the loan application.
%\item Amount.Funded.By.Investors - numeric. The amount (in dollars) loaned to the individual.
%\item Interest.rate - character. The lending interest rate charged to the borrower.
%\item Loan.length - character. The length of time (in months) of the loan.
%\item Loan.Purpose - categorical variable. The purpose of the loan as stated by the applicant.
%\item FICO.range - categorical (expressed as a string label e.g. ``650-655''). A range indicating the applicants FICO score.
%\end{itemize}
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile]\frametitle{Data Cleanup}
%We find the data are ''messy'' i.e aren't cleanly prepared for import - for instance numeric columns might have some strings in them. 
%\begin{lstlisting}
%import pandas as pd
%loansData = pd.read_csv('https://spark-public.s3.amazonaws.com/dataanalysis/loansData.csv')
%print(loansData['Interest.Rate'][0:5]) # first five rows of Interest.Rate
%
%81174     8.90%
%99592    12.12%
%80059    21.98%
%15825     9.99%
%33182    11.71%
%\end{lstlisting}
%\end{frame}
%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile]\frametitle{Credit Card Fraud}
%\begin{itemize}
%\item We can also think of classification as a function estimation problem where the function that we want to estimate separates the two classes. 
%\item This is illustrated in the example below where our goal is to predict whether or not a credit card transaction is fraudulent
%\item he dataset is provided by James et al., \textbf{Introduction to Statistical Learning}.
%\end{itemize}
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile]\frametitle{Credit Card Fraud}
%\begin{center}
%\includegraphics[width=\linewidth]{sklclass1}
%\end{center}
%\begin{center}
%\includegraphics[width=0.5\linewidth]{sklclass2}
%\end{center}
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile]\frametitle{Credit Card Fraud}
%\begin{itemize}
%\item 	On the left you can see a scatter plot where fraudulent cases are red dots and non-fraudulent cases are blue dots. 
%\item A good separation seems to be a vertical line at around a balance of 1400 as indicated by the boxplots on the next slide.
%\end{itemize}
%\begin{center}
%\includegraphics[width=0.6\linewidth]{sklclass3}
%\end{center}
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile]\frametitle{}
%\begin{center}
%\includegraphics[width=0.8\linewidth]{sklclass4}
%\end{center}
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile]\frametitle{Simple Approach - Linear Regression}
%\begin{itemize}
%\item A simple approach to binary classification is to simply encode default as a numeric variable with 'Yes' == 1 and 'No' == -1; fit an Ordinary Least Squares regression model and use this model to predict the response as 'Yes' if the regressed value is higher than 0.0 and 'No' otherwise. 
%\item The points for which the regression model predicts 0.0 lie on the so-called decision surface - since we are using a linear regression model, the decision surface is linear as well.
%\end{itemize}
%\end{frame}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile]\frametitle{}
%\begin{center}
%\includegraphics[width=0.6\linewidth]{sklclass6}
%\end{center}
%\begin{center}
%\includegraphics[width=0.4\linewidth]{sklclass7}
%\end{center}
%\begin{itemize}
%\item Points that lie on the left side of the decision boundary will be classified as negative; 
%\item Points that lie on the right side, positive. 
%\end{itemize}
%%The implementation of plot_surface can be found in the Appendix. 
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile]\frametitle{Confusion Matrix}
%\begin{itemize}
%\item We can assess the performance of the model by looking at the confusion matrix - a cross tabulation of the actual and the predicted class labels. 
%\item The correct classifications are shown in the diagonal of the confusion matrix. The off-diagonal terms show you the \textbf{classification errors}. 
%\item A condensed summary of the model performance is given by the \textbf{misclassification rate} determined simply by dividing the number of errors by the total number of cases.
%\end{itemize}
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile]\frametitle{Confusion Matrix}
%\begin{center}
%\includegraphics[width=0.8\linewidth]{sklclass8}
%\end{center}
%\begin{center}
%\includegraphics[width=0.6\linewidth]{sklclass9}
%\end{center}
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile]\frametitle{Cross Validation}
%\begin{itemize}
%\item In this example we are assessing the model performance on the same data that we used to fit the model. 
%\item This might be a biased estimate of the models performance, for a classifier that simply memorizes the training data has zero training error but would be totally useless to make predictions.
%\item  It is much better to assess the model performance on a separate dataset called the test data.
%\item  Scikit-learn provides a number of ways to compute such held-out estimates of the model performance.
%\item One way is to simply split the data into a \textbf{training set} and \textbf{testing set}.
%\end{itemize}
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile]
%\begin{center}
%\includegraphics[width=0.8\linewidth]{sklclass10}
%\end{center}
%\begin{center}
%\includegraphics[width=0.6\linewidth]{sklclass11}
%\end{center}
%\end{frame}
