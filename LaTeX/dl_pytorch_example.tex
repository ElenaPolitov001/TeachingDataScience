
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \begin{center}
    {\Large Simple Linear Regression Example}
    
\tiny{(Ref: PyTorchZeroToAll  - Sung Kim)}
  \end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Background: Simple Example}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pyhun1}
\end{center}

How solve?


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Simple Example}
 Start with some $w$, calculate the loss.
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pyhun2}
\end{center}

Plot loss for different $w$ values


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Simple Example}
 Start with some $w$, calculate the loss.
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pyhun3}
\end{center}

Plot loss for different $w$ values, and find minimum.

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Simple Example}
Simple python code looks like:
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pyhun4}
\end{center}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Simple Example}
For data:
\begin{lstlisting}
x_data = [1.0, 2.0, 3.0]
y_data = [2.0, 4.0, 6.0]
\end{lstlisting}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pyhun5}
\end{center}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Simple Example}
Entire program:
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pyhun6}
\end{center}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Simple Example}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pyhun7}
\end{center}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Simple Example}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pyhun8}
\end{center}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Whats the derivative?}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pyhun9}
\end{center}



\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Update}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pyhun10}
\end{center}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Update}
\begin{itemize}
\item Function for loss seen was simple, we could calculate and implement it easily, but for complicated network, with non-linearity, its not easy.
\item From $x$ to $loss$ there could be many sub-variables in between.
\item Here, we use chain rule. 
\item We calculate gradient at each stage, then the total gradient is just the multiplication of all.
\end{itemize}

\begin{center}
\includegraphics[width=0.5\linewidth,keepaspectratio]{pyhun11}
\end{center}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Update}
Example: if $f(x,y) = x.y$ and somehow final gradient is given as 5.
$dz/dx = d(fx,y)/dx = d(x.y)/dx = y$ 

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pyhun12}
\end{center}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Update}
Computational Graph of the loss function looks like:

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pyhun13}
\end{center}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Update}
Forward Pass, with some input and w values:

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pyhun13}
\end{center}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Update}
For backward pass, we need to calculate local gradients. Meaning, around each node.
Say, $s$ is input and $s^2$ is output on the last node. so its derivative is $ds^2/ds$.

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pyhun14}
\end{center}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Update}
Total gradient is just multiplication of all the node-wise gradients. Note that gradient is not calculated for x and y as they are input variables.

\begin{center}
\includegraphics[width=0.7\linewidth,keepaspectratio]{pyhun15}
\end{center}

Final gradient $d(loss)/dw$ is - 2. Update w with it.


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Update}
No need to compute gradient in pytorch. If you make $w$ as $Variable$ then its calculated automatically, looking at the computation path using it.

\begin{center}
\includegraphics[width=0.7\linewidth,keepaspectratio]{pyhun16}
\end{center}




\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Update}
Pytorch's loss.backward() does back-propagation and the gradient values get stored in w.

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pyhun17}
\end{center}


(Ref: PyTorchZeroToAll  - Sung Kim)
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Update}
Summary: It calculates all the sub gradients but we are interested in the whole gradient. $w.data$ is w and $w.grad.data$ is the gradient.

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pyhun18}
\end{center}


(Ref: PyTorchZeroToAll  - Sung Kim)
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Steps in PyTorch}
\begin{itemize}
\item Design Neural Network model inside a class with variables
\item Construct loss and set optimizer
\item Write forward, call backward and put the whole thing in iterations.

\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Steps in PyTorch}


\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pyhun19}
\end{center}





\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Steps in PyTorch}
\begin{itemize}
\item Inputs and outputs are in the form of tensors (nd-arrays)
\item Your class is derived from nn.Module.
\item Init creates a linear block with 1 input and 1 output.
\item In forward we use the linear block. No external x but its internal to the block.
\item backward() calculates all the gradients in the Variables.
\item step() updates the parameters like, w, which will then be used in the next epoch
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Steps in PyTorch}
In optimizer we do not explicitly ask to minimize w, but it optimizes all (w , b) parameters.

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pyhun20}
\end{center}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Steps in PyTorch}
Once model is read/stable, we can use to predict.

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pyhun21}
\end{center}


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Steps in PyTorch}
To convert linear regression to logistic, just add sigmoid in the forward()

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pyhun22}
\end{center}



\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Steps in PyTorch}
Single variable (x) may not have good predictive power. Lets have one more feature, predicting whether the student will get admitted or not.

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pyhun23}
\end{center}

Use matrix multiplications. Note: Linear model will decide the weights.


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Steps in PyTorch}
x is matrix. y is also matrix with 1 column. W will thus take such matrix giving $x.w = y$

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pyhun24}
\end{center}

Use matrix multiplications. Note: Linear model will decide the weights.


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Steps in PyTorch}
We can have multiple layers. Deep!!

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pyhun25}
\end{center}

Make sure you have inputs and ouputs correctly assigned, along with their dimensions.
Inputs and outut dimensions are fixed. Rest you can put anything.
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Steps in PyTorch}
\begin{itemize}
\item In deep network, Sigmoid can be a problem. 
\item It squashes number to small values (in case of 0/False). 
\item Multiplying such small number, makes gradient vanish in just a few layers.
\item Better to use other activations for internal layers and sigmoid/softmax for the last.
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Example in PyTorch}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pyhun26}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Example in PyTorch}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pyhun27}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{ NN Modules}
\begin{itemize}
\item Modules built on Variable
\item Gradient handled by PyTorch
\item Common Modules
\begin{itemize}
\item  Convolution layers
\item  Linear layers
\item  Pooling layers
\item  Dropout layers
\item  Etc \ldots
\end{itemize}
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Neural Networks}
\begin{itemize}
\item Constructed using the torch.nn package.
\item Contains layers, and a method forward(input)that returns the output.
\end{itemize}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pyt3}
\end{center}
Takes the input, feeds it through several layers one after the other, and then finally gives the output.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Convolution Layer}
\begin{itemize}
\item N-th Batch (N), Channel (C)
\item torch.nn.Conv1d: input [N, C, W] \# moving kernel in 1D
\item torch.nn.Conv2d: input [N, C, H, W] \# moving kernel in 2D
\item torch.nn.Conv3d: input [N, C, D, H, W] \# moving kernel in 3D
\item Example: \lstinline|torch.nn.conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)|
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Define the network}
\begin{lstlisting}
import torch
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
	pass
	
net = Net()
print(net)
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Define the network}
\begin{lstlisting}
class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        # 1 input image channel, 6 output channels, 5x5 square convolution
        # kernel
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        # an affine operation: y = Wx + b
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)
\end{lstlisting}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Define the network}
\begin{lstlisting}
class Net(nn.Module):

    def forward(self, x):
        # Max pooling over a (2, 2) window
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        # If the size is a square you can only specify a single number
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{ Training Procedure}
\begin{itemize}
\item You just have to define the forward function, and the backward function (where gradients are computed) is automatically defined for you using autograd. 
\item You can use any of the Tensor operations in the forward function.
\item The learnable parameters of a model are returned by net.parameters()
\end{itemize}
\begin{lstlisting}
params = list(net.parameters())
print(len(params))
print(params[0].size())  # conv1's .weight

10
torch.Size([6, 1, 5, 5])
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{ Training Procedure}
\begin{itemize}
\item The input to the forward is an autograd.Variable, and so is the output. 
\item nn.Conv2d will take in a 4D Tensor of nSamples x nChannels x Height x Width 
\item If you have a single sample, just use input.unsqueeze(0) to add a fake batch dimension.
\item Note: Expected input size to this net(LeNet) is 32x32. To use this net on MNIST dataset,please resize the images from the dataset to 32x32
\item Zero the gradient buffers of all parameters and backprops with random gradients:
\end{itemize}
\begin{lstlisting}
net.zero_grad()
out.backward(torch.randn(1, 10))
\end{lstlisting}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Define the network}
\begin{lstlisting}
class Net(nn.Module):

    def num_flat_features(self, x):
        size = x.size()[1:]  # all dimensions except the batch dimension
        num_features = 1
        for s in size:
            num_features *= s
        return num_features
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Loss Function}
\begin{itemize}
\item A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.
\item There are several different loss functions under the nn package . 
\item A simple loss is: nn.MSELoss which computes the mean-squared error between the input and the target.
\end{itemize}
\begin{lstlisting}
output = net(input)
target = Variable(torch.arange(1, 11))  # a dummy target, for example
criterion = nn.MSELoss()

loss = criterion(output, target)
print(loss)

Variable containing:
 38.9008
[torch.FloatTensor of size 1]
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Loss Function}
Now, if you follow loss in the backward direction, using it's .grad\_fn attribute, you will see a graph of computations that looks like this:
\begin{lstlisting}
input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d
      -> view -> linear -> relu -> linear -> relu -> linear
      -> MSELoss
      -> loss
\end{lstlisting}
So, when we call loss.backward(), the whole graph is differentiated w.r.t. the loss, and all Variables in the graph will have their .grad Variable accumulated with the gradient.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Loss Function}
For illustration, let us follow a few steps backward:
\begin{lstlisting}
print(loss.grad_fn)  # MSELoss
print(loss.grad_fn.next_functions[0][0])  # Linear
print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU

<MseLossBackward object at 0x7ff91efb6ba8>
<AddmmBackward object at 0x7ff91efb6860>
<ExpandBackward object at 0x7ff91efb6860>
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Backprop}
\begin{itemize}
\item To backpropagate the error all we have to do is to loss.backward(). 
\item You need to clear the existing gradients though, else gradients will be accumulated to existing gradients
\item Now we shall call loss.backward(), and have a look at conv1's bias gradients before and after the backward.
\end{itemize}
\begin{lstlisting}
net.zero_grad()     # zeroes the gradient buffers of all parameters

print('conv1.bias.grad before backward')
print(net.conv1.bias.grad)

loss.backward()

print('conv1.bias.grad after backward')
print(net.conv1.bias.grad)
\end{lstlisting}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Update the weights}
\begin{itemize}
\item $weight = weight - learning_rate * gradient$
\item We can implement this using simple python code:
\end{itemize}
\begin{lstlisting}
learning_rate = 0.01
for f in net.parameters():
    f.data.sub_(f.grad.data * learning_rate)
\end{lstlisting}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Update rules}
\begin{itemize}
\item To use various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. 
\item To enable this, we built a small package: torch.optim that implements all these methods. 
\end{itemize}
\begin{lstlisting}
import torch.optim as optim

# create your optimizer
optimizer = optim.SGD(net.parameters(), lr=0.01)

# in your training loop:
optimizer.zero_grad()   # zero the gradient buffers
output = net(input)
loss = criterion(output, target)
loss.backward()
optimizer.step()    # Does the update
\end{lstlisting}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Neural Networks Example}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pyt33}
\end{center}
\tiny{(Reference:PyTorch Tutorial-NTU Machine Learning Course-Lyman Lin )}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Neural Networks Example}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pyt34}
\end{center}
\tiny{(Reference:PyTorch Tutorial-NTU Machine Learning Course-Lyman Lin )}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Neural Networks Example}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pyt35}
\end{center}
\tiny{(Reference:PyTorch Tutorial-NTU Machine Learning Course-Lyman Lin )}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Neural Networks Example}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pyt36}
\end{center}
\tiny{(Reference:PyTorch Tutorial-NTU Machine Learning Course-Lyman Lin )}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Saving Models}
\begin{itemize}
\item  First Approach (Recommend by PyTorch)
\begin{lstlisting}
# save only the model parameters
torch.save(the_model.state_dict(), PATH)
# load only the model parameters
the_model = TheModelClass(*args, **kwargs)
the_model.load_state_dict(torch.load(PATH))
\end{lstlisting}

\item Second Approach
\begin{lstlisting}
 torch.save(the_model, PATH) # save the entire model
the_model = torch.load(PATH) # load the entire model
\end{lstlisting}
\end{itemize}
\tiny{(Reference:http://pytorch.org/docs/master/notes/serialization.html\#recommended-approach-for-saving-a-model)}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Classification Example: MNIST}
Inputs: Images

Output: 10 labels

\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{pyhun32}
\end{center}

Logistic regression will give binary output, so sigmoid was ok.

\tiny{(Ref: PyTorchZeroToAll  - Sung Kim)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Classification Example: MNIST}
W matrix size will be mx10. Need softmax activation at the end for multi label classification (probabilities).

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pyhun33}
\end{center}


\tiny{(Ref: PyTorchZeroToAll  - Sung Kim)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Classification Example: MNIST}
Softmax has float input (called logit) and outputs 10 probablilites.

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pyhun34}
\end{center}


\tiny{(Ref: PyTorchZeroToAll  - Sung Kim)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Classification Example: MNIST}
Loss is based on cross entropy. Meaning it compares probabilities with one-hot format.

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pyhun35}
\end{center}

$D(\hat{Y},Y) = \sum -Y \log \hat{Y}$, where $Y$ and $\hat{Y}$ are two different distributions.

\tiny{(Ref: PyTorchZeroToAll  - Sung Kim)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Classification Example: MNIST}
Pytorch gives a ready loss function. Multiple lables and multiple predictions can also be compared.

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pyhun36}
\end{center}

\tiny{(Ref: PyTorchZeroToAll  - Sung Kim)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Classification Example: MNIST}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pyhun37}
\end{center}

We can have many hidden layers in between.

\tiny{(Ref: PyTorchZeroToAll  - Sung Kim)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Classification Example: MNIST}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pyhun38}
\end{center}

Weight matrices are set with approprioate dimensions.

\tiny{(Ref: PyTorchZeroToAll  - Sung Kim)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Classification Example: MNIST}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pyhun39}
\end{center}

\tiny{(Ref: PyTorchZeroToAll  - Sung Kim)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Classification Example: MNIST}

Connect the layers in forward() function. We need to reshape it to single vector.

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pyhun40}
\end{center}

\tiny{(Ref: PyTorchZeroToAll  - Sung Kim)}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Classification Example: MNIST}

Entire program:

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pyhun41}
\end{center}


\tiny{(Ref: PyTorchZeroToAll  - Sung Kim)}
\end{frame}




