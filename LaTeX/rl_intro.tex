%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Introduction}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{What is Reinforcement Learning?}
Reinforcement Learning(RL) is a type of machine learning technique that enables an agent to learn in an interactive environment by trial and error using feedback from its own actions and experiences, to maximize cumulative rewards.


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{The big picture}
\begin{itemize}
\item An agent (an AI) will learn from the environment by interacting with it (through trial and error) and receiving rewards (negative or positive) as feedback for performing actions.
\item Learning from interaction with the environment comes from our natural experiences.
\item Like, without any supervision, the child will get better and better at playing the game.
\end{itemize}

{\tiny (Ref: Chapter 1 of the Deep Reinforcement Learning Class with Hugging Face)}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Real World Example}

\begin{columns}
\begin{column}{0.5\textwidth}

\begin{itemize}
\item A shop sells ice cream
\item Customers coming to shop can be random (stochastic), so shop owner finds it difficult to know how much stock to keep.
\item If stock is kept less, then all customers can not be served, so loss of business/income.
\item If stock is kept more, then wastage, so loss of business/income.
\item What's the optimum stock to maximize profit? Say, in a quarter.
\end{itemize}

\end{column}
\begin{column}{0.5\textwidth}  %%<--- here


\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rl56}
\end{center}
\end{column}
\end{columns}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Reinforcement Learning way}

\begin{itemize}
\item Agent: Shop owner, as he Acts
\item Environment: shop, shopkeeper, customers, stock, etc. even things like, 'demand'.
\item Action: how much to order from supplier to keep as stock for that day?
\item Reward: cumulative profits over the quarter. Its 'expected' as customer demand is stochastic.
\end{itemize}
Goal: how much to stock daily so as to maximize expected quarterly profits
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Reinforcement Learning way}

\begin{itemize}
\item An \textit{agent} interacts with an \textit{environment}. 
\item The goal of the agent is to maximize cumulative reward (called \textit{return}). 
\item Reinforcement learning (RL) is a field of study for algorithms that do that.
\end{itemize}

\begin{columns}
\begin{column}{0.5\textwidth}
    \begin{center}
     \includegraphics[width=\textwidth]{rl1}
     \end{center}
\end{column}
\begin{column}{0.5\textwidth}  %%<--- here
\begin{lstlisting}[language=python]
  obs = env.reset()
  done = False
  while not(done):
  	act = agent.get_action(obs)
  	next_obs, reward, done, info = env.step(act)
  	obs = next_obs
\end{lstlisting}
\end{column}
\end{columns}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Reinforcement Learning Visual way}

\begin{columns}
\begin{column}{0.5\textwidth}

\begin{itemize}
\item Circle is State of the Environment, has attributes like stock, weather, etc
\item Action emanates from it become next State. Action can be of ordering 2/5/10 crates of ice creams. The next state has that attribute updated.
\item Reward is shown on arrow line.
\item Out of two specific paths shown, best (blue) is the winner path of maximizing the profits.
\item $---$ means similar Actions.
\end{itemize}

\end{column}
\begin{column}{0.5\textwidth}  %%<--- here


\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rl57}

{\tiny (Ref: Fast RL course - Dibya Chakraborty)}

\end{center}
\end{column}
\end{columns}



\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{RL Applications}
 Not just for forecasting demand for ice cream but:
\begin{itemize}
\item Energy usage reduction in data centers.
\item Managing feet of vehicles
\item Stock trading
\item Games
\item Robotics
\item Autonomous cars
\end{itemize}

Typically, Control-Feedback based Sequential Decision making tasks.

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{ML vs RL}

\begin{itemize}
\item In supervised ML: labels ie correct answers are known upfront, eg. for prediction if image is cat or a dog, hundreds of cats and dogs images have to be provided for training.
\item In Reinforcement Learning,  nothing is known upfront. Need to act first, then you get some reward. The training is to decide such actions that gives probability of more rewards. Trial and Error. Being stochastic, same action may not result in same reward.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{ML vs RL}

\begin{columns}
\begin{column}{0.5\textwidth}

\begin{itemize}
\item In supervised ML data is i.i.d (identical and independently distributed data), means, each data (row) is independent of any part/previous data. 
\item In Reinforcement Learning, State may depend on previous State. Even if some actions may result in lesser immediate rewards, in the long run they may turn out to be the best. Because, some intermediate states can be better or worse. Long Term Thinking, Strategy with harmonized/coordinated Actions.
\end{itemize}

\end{column}
\begin{column}{0.5\textwidth}  %%<--- here


\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rl58}

{\tiny (Ref: Fast RL course - Dibya Chakraborty)}

\end{center}
\end{column}
\end{columns}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{ML vs RL}

No labeled data needed but just nudging whether model is in right direction or not. Outcome is a policy (recipe) for success.

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rl45}
\end{center}

{\tiny (Ref: CS 285 Fall 2020 Deep Reinforcement Learning)}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Deep Learning}

Deep Learning replaced Machine Learning by automatically discovering features. Similarity, a classical RL would need to device features, policies or value table etc to come up with next action. Deep RL automates this all.

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rl46}
\end{center}


{\tiny (Ref: CS 285 Fall 2020 Deep Reinforcement Learning)}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Deep Learning}

End-to-End system does not do stage wise recognition, meaning, image recognition first then action prediction, but does everything in a single go.

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rl47}
\end{center}


{\tiny (Ref: CS 285 Fall 2020 Deep Reinforcement Learning)}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Deep Learning a step towards Artificial General Intelligence?}

\begin{itemize}
\item Deep Learning perceives and represents environment/data well but can bot deal with long term planning, strategy, etc.
\item Reinforcement Learning is good at long term planning, strategy, etc.
\item Combining both can be close to better intelligence. 
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{To RL or not to RL?}

\begin{itemize}
\item Difficult to tell the correct action before actually executing it (and getting reward) but easy to score once taken.
\item Is current data/state dependent on previous data/state?
\item Cumulative Reward more important than the immediate ones.
\end{itemize}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{RL Libs}

\begin{itemize}
\item Libraries like `rllib` have production have implementations of Deep RL algorithms.
\item High level APIs abstract the details
\item E.g. often the solution is stated as : `` Use DQN on Cart Pole' where CartPole is Environment and DQN is the algorithm.
\end{itemize}

Comparison of RL frameworks:

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rl63}

{\tiny (Ref: Fast RL course - Dibya Chakraborty)}
\end{center}

\end{frame}