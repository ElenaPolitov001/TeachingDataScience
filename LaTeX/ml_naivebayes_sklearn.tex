%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Gaussian Naive Bayes with Scikit-Learn}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Naive Bayes Template Code}
\begin{lstlisting}
from sklearn.naive_bayes import GaussianNB 

#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset 

model = GaussianNB() # there is other distribution for multinomial classes like Bernoulli Naive Bayes, 

model.fit(X, y) 
predicted= model.predict(x_test) 
\end{lstlisting}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Gaussian Naive Bayes}

{\tiny (Ref: Machine Learning Algorithm Recipes in scikit-learn - Jason Brownlee)}


\begin{lstlisting}
# Gaussian Naive Bayes
from sklearn import datasets
from sklearn import metrics
from sklearn.naive_bayes import GaussianNB
# load the iris datasets
dataset = datasets.load_iris()
# fit a Naive Bayes model to the data
model = GaussianNB()
model.fit(dataset.data, dataset.target)
print(model)
# make predictions
expected = dataset.target
predicted = model.predict(dataset.data)
# summarize the fit of the model
print(metrics.classification_report(expected, predicted))
print(metrics.confusion_matrix(expected, predicted))
\end{lstlisting}



\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Test case - Diabetes of Pima Indians}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Comparing Multinomial and Gaussian Naive Bayes}
Dataset: Pima Indians Diabetes from the UCI Machine Learning Repository

\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{pima1}
\end{center}

\begin{lstlisting}
import pandas as pd

url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/ \
pima-indians-diabetes/pima-indians-diabetes.data'
col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']
pima = pd.read_csv(url, header=None, names=col_names)
print(pima.head())
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Training}
\begin{lstlisting}
X = pima.drop('label', axis=1)
y = pima.label

from sklearn.cross_validation import train_test_split
from sklearn.naive_bayes import MultinomialNB, GaussianNB
from sklearn import metrics

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# testing accuracy of Multinomial Naive Bayes
mnb = MultinomialNB()
mnb.fit(X_train, y_train)
y_pred_class = mnb.predict(X_test)
print(metrics.accuracy_score(y_test, y_pred_class)) #0.54166

# testing accuracy of Gaussian Naive Bayes
gnb = GaussianNB()
gnb.fit(X_train, y_train)
y_pred_class = gnb.predict(X_test)
print(metrics.accuracy_score(y_test, y_pred_class)) # 0.7916
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Conclusion}
\begin{itemize}
\item When applying Naive Bayes classification to a data-set with continuous features, it is better to use Gaussian Naive Bayes than Multinomial Naive Bayes.
\item  The latter is suitable for data-sets containing discrete features (e.g., word counts).
\end{itemize}
\end{frame}