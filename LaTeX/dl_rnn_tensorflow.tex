%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \begin{center}
    {\Large RNN with TensorFlow}
    
  \end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \begin{center}
    {\Large Toy Example: Predict sum of 3 numbers}
    
  \end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Problem Description}
\begin{itemize}
\item Aim: to predict the sum of 3 numbers with RNN
\item Inputs: samples like $[x_0,x_1,x_2][x_0,x_1,x_2]$
\item Outputs: like $y = x_0 + x_1 + x_2$
\end{itemize}


\begin{lstlisting}
import numpy as np
from tf.keras.models import Model
from tf.keras.layers import Input, SimpleRNN
# Data and model parameters
seq_len = 3   #Length of each sequence 
rnn_size = 1  #Output shape of RNN
input_size = 10000 #Numbers of instances
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Creating Data}
\begin{lstlisting}
all_feat = np.random.randint(low=0, high=10, size=(input_size,3,1))
print(all_feat[:2, :])  # top 2, all values

array([[[9],
        [0],
        [1]],
       [[7],
        [6],
        [4]]])
				
all_label = np.apply_along_axis(func1d=np.sum, axis=1, arr=all_feat)
print(all_label[:2])

array([[10],
       [17]])
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Define model}
\begin{itemize}
\item Using only a Simple RNN
\item Expectation with RNN is that it will learn to pass the input as it is to next layer and do the SUM of the sequence.
\item LINEAR activation is used (obviously)
\end{itemize}


\begin{lstlisting}
x = Input(shape=(3,1,), name='Input_Layer') # 1, as each number is single and not a vector
y = SimpleRNN(rnn_size, activation='linear', name='RNN_Layer')(x)
model = Model(inputs=x, outputs=y)
model.summary()

________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Input_Layer (InputLayer)     (None, 3, 1)              0         
_________________________________________________________________
RNN_Layer (SimpleRNN)        (None, 1)                 3         
=================================================================
Total params: 3.0
Trainable params: 3.0
Non-trainable params: 0.0
_________________________________________________________________
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Fit model}

\begin{lstlisting}
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['acc'])
history = model.fit(x=all_feat, y=all_label, batch_size=4, epochs=5, validation_split=0.2, verbose=1)

Train on 8000 samples, validate on 2000 samples
Epoch 1/5
8000/8000 [==============================] - 9s - loss: 52.2708 - acc: 0.1204 - val_loss: 2.8959 - val_acc: 0.2050
Epoch 2/5
8000/8000 [==============================] - 8s - loss: 2.3057 - acc: 0.2139 - val_loss: 1.5666 - val_acc: 0.2775
Epoch 3/5
8000/8000 [==============================] - 8s - loss: 0.9501 - acc: 0.3466 - val_loss: 0.4068 - val_acc: 0.5105
Epoch 4/5
8000/8000 [==============================] - 9s - loss: 0.1705 - acc: 0.7825 - val_loss: 0.0324 - val_acc: 1.0000
Epoch 5/5
8000/8000 [==============================] - 9s - loss: 0.0084 - acc: 1.0000 - val_loss: 1.4700e-04 - val_acc: 1.0000
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Prediction}

\begin{lstlisting}
print('\nInput features: \n', all_feat[-2:,:])
print('\nLabels: \n', all_label[-2:,:])
print('\nPredictions: \n', model.predict(all_feat[-2:,:]))

Input features: 
 [[[0]
  [9]
  [3]]
 [[4]
  [2]
  [5]]]
Labels: 
 [[12]
 [11]]
Predictions: 
 [[ 12.00395012]
 [ 11.0082655 ]]
 
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Deep Dive into RNN}

\begin{itemize}
\item W: Input to RNN weight Matrix
\item U: RNN to RNN (or hidden layer to RNN) weight Matrix
\item b: Bias matrix
\end{itemize}

\begin{lstlisting}
wgt_layer = model.get_layer('RNN_Layer')
wgt_layer.get_weights()

[array([[ 0.99675155]], dtype=float32),
 array([[ 1.00106668]], dtype=float32),
 array([ 0.01110852], dtype=float32)]
 
\end{lstlisting}
RNN equation is $h_t = f(X \times W + h_{t-1} \times U + b)$.

With linear $f$, it becomes $h_t = X \times W + h_{t-1} \times U + b$

So, $W=1,U=1,b=0$ looks more or less correct.
\end{frame}