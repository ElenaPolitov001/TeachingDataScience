%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}

\begin{center}
{\Large Pytorch Implementation}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
\frametitle{Datasets and Dataloaders}

\begin{itemize}
\item Need our dataset code to be decoupled from our model training code for better readability and modularity. 
\item PyTorch provides two data primitives: \lstinline|torch.utils.data.DataLoader| and \lstinline|torch.utils.data.Dataset| that allow you to use pre-loaded datasets as well as your own data. 
\item Dataset stores the samples and their corresponding labelse
\item  DataLoader wraps an iterable around the Dataset to enable easy access to the samples.
\item PyTorch domain libraries provide a number of pre-loaded datasets (such as FashionMNIST) that subclass \lstinline|torch.utils.data.Dataset|
\end{itemize}

\end{frame} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
\frametitle{Ready Datasets}

\begin{lstlisting}
import torch
from torch.utils.data import Dataset
from torchvision import datasets
from torchvision.transforms import ToTensor, Lambda
import matplotlib.pyplot as plt

training_data = datasets.FashionMNIST(
    root="data",
    train=True,
    download=True,
    transform=ToTensor()
)

test_data = datasets.FashionMNIST(
    root="data",
    train=False,
    download=True,
    transform=ToTensor()
)
\end{lstlisting}

\end{frame} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
\frametitle{Iterating and Visualizing the Dataset}

Can index Datasets manually like a list: \lstinline|training_data[index]|.
Use matplotlib to visualize some samples in the training data.

\begin{lstlisting}
labels_map = {
    0: "T-Shirt",
    1: "Trouser",
    2: "Pullover",
    3: "Dress",
    4: "Coat",
    5: "Sandal",
    6: "Shirt",
    7: "Sneaker",
    8: "Bag",
    9: "Ankle Boot",
}
figure = plt.figure(figsize=(8, 8))
cols, rows = 3, 3
for i in range(1, cols * rows + 1):
    sample_idx = torch.randint(len(training_data), size=(1,)).item()
    img, label = training_data[sample_idx]
    figure.add_subplot(rows, cols, i)
    plt.title(labels_map[label])
    plt.axis("off")
    plt.imshow(img.squeeze(), cmap="gray")
plt.show()
\end{lstlisting}

\end{frame} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
\frametitle{Creating a Custom Dataset for your files}

Implement \lstinline|__init__, __len__, __getitem__|.

For images are stored in a directory \lstinline|img_dir|, and their labels are stored separately in a CSV file \lstinline|annotations_file|.

\begin{lstlisting}
import os
import pandas as pd
import torchvision.io as tvio

class CustomImageDataset(Dataset):
    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.img_labels)

\end{lstlisting}

\end{frame} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
\frametitle{Creating a Custom Dataset for your files}

\begin{lstlisting}
import os
import pandas as pd
import torchvision.io as tvio

class CustomImageDataset(Dataset):
    :
    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
        image = tvio.read_image(img_path)
        label = self.img_labels.iloc[idx, 1]
        if self.transform:
            image = self.transform(image)
        if self.target_transform:
            label = self.target_transform(label)
        sample = {"image": image, "label": label}
        return sample
\end{lstlisting}

\end{frame} 



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{DataLoaders}
% Simple data-loader, manual, and feeding ALL data to the model. No batch!!
% \begin{lstlisting}
% xy = np.loadtxt('./data/diabetes.csv', delimiter=',', dtype=np.float32)
% x_data = Variable(torch.from_numpy(xy[:, 0:-1]))
% y_data = Variable(torch.from_numpy(xy[:, [-1]]))

% # Training loop
% for epoch in range(100):
        % # Forward pass: Compute predicted y by passing x to the model
    % y_pred = model(x_data)

    % # Compute and print loss
    % loss = criterion(y_pred, y_data)
    % print(epoch, loss.data[0])

    % # Zero gradients, perform a backward pass, and update the weights.
    % optimizer.zero_grad()
    % loss.backward()
    % optimizer.step()
% \end{lstlisting}

% \tiny{(Ref: PyTorchZeroToAll  - Sung Kim)}
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{DataLoaders}
\begin{itemize}
\item For big-data, we can not feed ALL. 
\item Need to divide into batches. 
\item Feed one batch at a time. 
\item Compute gradients.
\item Update weights.
\item \textbf{epoch}: One forward pass and one backward pass for ALL training rows.
\item \textbf{batch\_size}: Number of training rows in one forward/backward pass.
\item \textbf{iterations}: Number of passes in one epoch. One batch iteration. Training rows divided by batch\_size
\end{itemize}

\tiny{(Ref: PyTorchZeroToAll  - Sung Kim)}

\begin{lstlisting}
# Training loop
for epoch in range(100):
   for i in range(total_batches):
   	batch_xs, batch_ys = ...
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{DataLoaders}
Data loader takes care of batching and scuffling and gives out iterable for batches.
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pyhun28}
\end{center}

\tiny{(Ref: PyTorchZeroToAll  - Sung Kim)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{DataLoaders}
For our own dataset, create own dataloader.
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pyhun29}
\end{center}

\tiny{(Ref: PyTorchZeroToAll  - Sung Kim)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{DataLoaders}
For diabetics csv, here is the dataloader
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pyhun30}
\end{center}

\tiny{(Ref: PyTorchZeroToAll  - Sung Kim)}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{DataLoaders}
Some famous datasets are in pytorch itself.
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pyhun31}
\end{center}

\tiny{(Ref: PyTorchZeroToAll  - Sung Kim)}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Preparing data for training}

\begin{itemize}
\item The Dataset retrieves our dataset's features and labels one sample at a time.
\item While training a model, we typically want to pass samples in ``minibatches''.
\item Reshuffle the data at every epoch to reduce model overfitting
\item Use Python's multiprocessing to speed up data retrieval.
\end{itemize}

\tiny{(Ref: Microsoft - Intro to Machine Learning using Pytorch)}

\begin{lstlisting}
from torch.utils.data import DataLoader

train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Iterate through the DataLoader}

\begin{itemize}
\item Each iteration below returns a batch of \lstinline|train_features| and \lstinline|train_labels|(containing \lstinline|batch_size=64| features and labels respectively).
\item With \lstinline|shuffle=True| data is shuffled after all batches are iterated.
\end{itemize}

\tiny{(Ref: Microsoft - Intro to Machine Learning using Pytorch)}


\begin{lstlisting}
# Display image and label.
train_features, train_labels = next(iter(train_dataloader))
print(f"Feature batch shape: {train_features.size()}")
print(f"Labels batch shape: {train_labels.size()}")
img = train_features[0].squeeze()
label = train_labels[0]
plt.imshow(img, cmap="gray")
plt.show()
print(f"Label: {label}")

Feature batch shape: torch.Size([64, 1, 28, 28])
Labels batch shape: torch.Size([64])
Label: 9
\end{lstlisting}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Transforms}

\begin{itemize}
\item All TorchVision datasets have two parameters (\lstinline|transform| to modify the features and \lstinline|target_transform| to modify the labels) that accept callables containing the transformation logic. 
\item The FashionMNIST features are in PIL Image format, and the labels are integers. 
\item For training, we need the features as normalized tensors, and the labels as one-hot encoded tensors.
\end{itemize}

\tiny{(Ref: Microsoft - Intro to Machine Learning using Pytorch)}

\begin{lstlisting}
from torchvision import datasets
from torchvision.transforms import ToTensor, Lambda

ds = datasets.FashionMNIST(
    root="data",
    train=True,
    download=True,
    transform=ToTensor(),
    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))
)
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Transforms}


\begin{itemize}
\item ToTensor converts a PIL image or NumPy ndarray into a FloatTensor and scales the image's pixel intensity values in the range [0., 1.]
\item Lambda transforms apply any user-defined lambda function. Here, we define a function to turn the integer into a one-hot encoded tensor. It first creates a zero tensor of size 10 (the number of labels in our dataset) and calls scatter which assigns a value=1 on the index as given by the label y.
\end{itemize}

\tiny{(Ref: Microsoft - Intro to Machine Learning using Pytorch)}

\begin{lstlisting}
target_transform = Lambda(lambda y: torch.zeros(
    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Build a neural network}

\begin{itemize}
\item Neural networks comprise of layers/modules that perform operations on data. 
\item The torch.nn namespace provides all the building blocks 
\item Every module/network/layers in PyTorch subclasses the nn.Module. 
\item A neural network is a module itself that consists of other modules (layers). 
\item This nested structure allows for building and managing complex architectures easily.
\end{itemize}


\tiny{(Ref: Microsoft - Intro to Machine Learning using Pytorch)}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Define the class}

\begin{itemize}
\item Define own neural network by subclassing \lstinline|nn.Module|, and initialize the neural network layers in \lstinline|__init__|. 
\item Every \lstinline|nn.Module| subclass implements the operations on input data in the \lstinline|forward| method.
\end{itemize}

\tiny{(Ref: Microsoft - Intro to Machine Learning using Pytorch)}

\begin{lstlisting}
class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10),
            nn.ReLU()
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{To Device}

\begin{itemize}
\item Set the device
\item Create an instance of NeuralNetwork, and move it to the device, so that all operations are done on that device ie either CPU or GPU
\end{itemize}

\tiny{(Ref: Microsoft - Intro to Machine Learning using Pytorch)}

\begin{lstlisting}
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = NeuralNetwork().to(device)

>>>NeuralNetwork(
  (flatten): Flatten()
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=784, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=10, bias=True)
    (5): ReLU()
  )
)
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Setup}

\begin{itemize}
\item To use the model, pass it the input data. This AUTOMATICALLY/INTERBALLY executes the model's forward, along with some background operations. 
\item Do not call \lstinline|model.forward()| directly!
\item Calling the model on the input returns a 10-dimensional tensor with raw predicted values for each class. These are called `logits'.
\item Get the prediction densities by passing it through an instance of the \lstinline|nn.Softmax module|.
\end{itemize}

\tiny{(Ref: Microsoft - Intro to Machine Learning using Pytorch)}

\begin{lstlisting}
X = torch.rand(1, 28, 28, device=device)
logits = model(X) 
pred_probab = nn.Softmax(dim=1)(logits)
y_pred = pred_probab.argmax(1)
print(f"Predicted class: {y_pred}")

>>>Predicted class: tensor([2], device='cuda:0')
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Model Layers: Input}

Lets trace what happens to a sample minibatch of 3 images of size $28x28$ and see what happens to it as we pass it through the network.

\tiny{(Ref: Microsoft - Intro to Machine Learning using Pytorch)}

\begin{lstlisting}
input_image = torch.rand(3,28,28)
print(input_image.size())

>>>torch.Size([3, 28, 28])
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Model Layers: nn.Flatten}

Flatten to convert each 2D 28x28 image into a contiguous array of 784 pixel values (the minibatch dimension (at $dim=0$, which is 3) is maintained).

\tiny{(Ref: Microsoft - Intro to Machine Learning using Pytorch)}

\begin{lstlisting}
flatten = nn.Flatten()
flat_image = flatten(input_image)
print(flat_image.size())

>>>torch.Size([3, 784])
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Model Layers: nn.Linear}

Linear applies a linear transformation on the input using it's stored weights and biases. Num output nodes are specified as 20.

\tiny{(Ref: Microsoft - Intro to Machine Learning using Pytorch)}

\begin{lstlisting}
layer1 = nn.Linear(in_features=28*28, out_features=20)
hidden1 = layer1(flat_image)
print(hidden1.size())

>>>torch.Size([3, 20])
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Model Layers: nn.ReLU}

Relu activation is applied after linear transformations to introduce nonlinearity, helping neural networks learn a wide variety of phenomena.

\tiny{(Ref: Microsoft - Intro to Machine Learning using Pytorch)}

\begin{lstlisting}
print(f"Before ReLU: {hidden1}\n\n")
hidden1 = nn.ReLU()(hidden1)
print(f"After ReLU: {hidden1}")

>>>Before ReLU: tensor([[ 0.2190, .. -0.4028],
        [-0.3531,  .. -0.4500], .. ], grad_fn=<AddmmBackward>)


After ReLU: tensor([[0.2190, .. 0.0358, 0.3173, 0.0000, 0.0000],
        ..]], grad_fn=<ReluBackward0>)

\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Model Layers: nn.Sequential}

Sequential is a container to hold modules/layers in order so that previous sets its output nodes dimension to the next as inputs dimension.

\tiny{(Ref: Microsoft - Intro to Machine Learning using Pytorch)}

\begin{lstlisting}
seq_modules = nn.Sequential(
    flatten,
    layer1,
    nn.ReLU(),
    nn.Linear(20, 10)
)
input_image = torch.rand(3,28,28)
logits = seq_modules(input_image)

\end{lstlisting}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Model Layers: nn.Softmax}

\begin{itemize}
\item The last linear layer of the neural network returns logits - raw values in $[-\infty, \infty]$ - which are passed to the \lstinline|nn.Softmax| module. 
\item The logits are scaled to values $[0, 1]$ representing the model's predicted densities for each class. 
\item \lstinline|dim| parameter indicates the dimension along which the values must sum to $1$.
\end{itemize}

\tiny{(Ref: Microsoft - Intro to Machine Learning using Pytorch)}

\begin{lstlisting}
softmax = nn.Softmax(dim=1)
pred_probab = softmax(logits)
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Model parameters}

\begin{itemize}
\item Many layers inside a neural network are parameterized, i.e. have associated weights and biases that are optimized during training. 
\item  Subclassing \lstinline|nn.Module| automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model's \lstinline|parameters()| or \lstinline|named_parameters()| methods.
\end{itemize}

\tiny{(Ref: Microsoft - Intro to Machine Learning using Pytorch)}

\begin{lstlisting}
for name, param in model.named_parameters():
    print(f"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \n")
		
>>>Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0195, ..]], device='cuda:0', grad_fn=<SliceBackward>) 

Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0013,  0.0138], device='cuda:0', grad_fn=<SliceBackward>) 

Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0112, ..]], device='cuda:0', grad_fn=<SliceBackward>) 

Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0345, 0.0220], device='cuda:0', grad_fn=<SliceBackward>) 
:
\end{lstlisting}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Operations}
Additions
\begin{lstlisting}
x = torch.tensor(5, 3)
y = torch.rand(5, 3)
print(x + y)

print(torch.add(x, y))

result = torch.tensor(5, 3)
torch.add(x, y, out=result)

y.add_(x)
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Numpy Bridge}
\begin{itemize}
\item Converting a torch Tensor to a numpy array and vice versa. 
\item The torch Tensor and numpy array will share their underlying memory locations, and changing one will change the other.
\end{itemize}

\begin{lstlisting}
a = torch.ones(5)
b = a.numpy()
a.add_(1)

print(a)
print(b)
# both are with same values
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Numpy Bridge}
Converting numpy Array to torch Tensor

All the Tensors on the CPU except a CharTensor support converting to NumPy and back.

\begin{lstlisting}
import numpy as np
a = np.ones(5)
b = torch.from_numpy(a)
np.add(a, 1, out=a)
print(a)
print(b)

[ 2.  2.  2.  2.  2.]

 2
 2
 2
 2
 2
[torch.DoubleTensor of size 5]
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{CUDA Tensors}
Tensors can be moved onto GPU using the .cuda function.

\begin{lstlisting}
# let us run this cell only if CUDA is available
if torch.cuda.is_available():
    x = x.cuda()
    y = y.cuda()
    x + y

\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Dynamic Computation Graphs}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pyt38}
\end{center}
PyTorch 0.4 merges the Variable and Tensor class into one, and Tensor can be made into a “Variable” by a switch rather than instantiating a new object. 

{\tiny (Ref: Getting Started with PyTorch Part 1: Understanding how Automatic Differentiation works - Ayoosh Kathuria )}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Variable}
\begin{itemize}
\item autograd.Variable wraps a Tensor, and supports nearly all of operations defined on it.
\item Records the history of operations applied to it. 
\item Has the same API as a Tensor, with some additions like backward(). 
\item Also holds the gradient w.r.t. the tensor.
\item call .backward() and have all the gradients computed automatically.
\end{itemize}
\begin{center}
\includegraphics[width=0.4\linewidth,keepaspectratio]{pyt1}
\end{center}
\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Define the network}
 With TensorFlow each layer operation has to be explicitly named:

  {\tiny (Ref: How Pytorch gives the big picture with deep learning - Déborah Mesquita)}

 \begin{lstlisting}
def multilayer_perceptron(input_tensor, weights, biases):
    layer_1_multiplication = tf.matmul(input_tensor, weights['h1'])
    layer_1_addition = tf.add(layer_1_multiplication, biases['b1'])
    layer_1_activation = tf.nn.relu(layer_1_addition)
    
    layer_2_multiplication = tf.matmul(layer_1_activation, weights['h2'])
    layer_2_addition = tf.add(layer_2_multiplication, biases['b2'])
    layer_2_activation = tf.nn.relu(layer_2_addition)
    
    out_layer_multiplication = tf.matmul(layer_2_activation, weights['out'])
    out_layer_addition = out_layer_multiplication + biases['out']
    
    return out_layer_additio
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Define the network}
\begin{itemize}
\item With Pytorch we use torch.nn. 
\item We need to multiply each input node with a weight, and also to add a bias. 
\item The class torch.nn.Linear does the job for us.
\item The base class for all neural network modules is torch.nn.Module.
\item The forward(*input) defines the computation performed at every call, and all subclasses should override it.
\item forward() takes inputs, takes it through all the layers and returns the output. Almost like predict the output, with current weights.
\end{itemize}

  {\tiny (Ref: How Pytorch gives the big picture with deep learning - Déborah Mesquita)}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Define the network}

  {\tiny (Ref: How Pytorch gives the big picture with deep learning - Déborah Mesquita)}


 \begin{lstlisting}
class OurNet(nn.Module):
 def __init__(self, input_size, hidden_size, num_classes):
     super(Net, self).__init__()
     self.layer_1 = nn.Linear(n_inputs,hidden_size, bias=True)
     self.relu = nn.ReLU()
     self.layer_2 = nn.Linear(hidden_size, hidden_size, bias=True)
     self.output_layer = nn.Linear(hidden_size, num_classes, bias=True)
 
 def forward(self, x):
     out = self.layer_1(x)
     out = self.relu(out)
     out = self.layer_2(out)
     out = self.relu(out)
     out = self.output_layer(out)
     return out
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Define the network}
\begin{itemize}
\item In \_\_init\_\_ we need to call super class's constructor and define the layers. This gets called when we say $model = MyModel()$
\item forward() is the call that takes the input and dynamically constructs the graph. This gets called when we call $model(x)$ inside the epoch loop. So, this forward call gets called in each iteration, creating dynamic graph each time.
\item With this, depending on the size of input, we can dynamically construct NN accordingly, useful for variable length sequences.
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Update the weights}
\begin{itemize}
\item The way the neural network ''learns'' is by updating the weight values. With Pytorch we use the torch.autograd package to do that.
\item We didn’t specify the weight tensors like we did with TensorFlow because the torch.nn.Linear class has a variable weight with shape (out\_features x in\_features).
\item torch.nn.Linear(in\_features, out\_features, bias=True)
\end{itemize}

  {\tiny (Ref: How Pytorch gives the big picture with deep learning - Déborah Mesquita)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Update the weights}
\begin{itemize}
\item To compute the gradient, we will use the the method Adaptive Moment Estimation (Adam). Torch.optim is a package that implements various optimization algorithms.
\item To use torch.optim, you have to construct an optimizer object that will hold the current state and also update the parameters based on the computed gradients.
\item To construct an optimizer, you have to give it an iterable that contains the parameters (all should be variables ) to optimize. Then you can specify options that are specific to an optimizer, such as the learning rate, weight decay, etc.
\end{itemize}


  {\tiny (Ref: How Pytorch gives the big picture with deep learning - Déborah Mesquita)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Update the weights}
Typical Optimization workflow

 \begin{lstlisting}
for input, target in dataset:
	optimizer.zero_grad()
	output = model(input) # calls model.forward()
	loss = loss_fn(output, target)
	loss.backward()
	optimizer.step()
\end{lstlisting}

As ``loss'' is formulated in terms of ``output'' which is in terms of inputs and weights, when we say loss.backward(), autograd does backprop and sets gradient values in all input variables. Gradients are evaluated at the predicted output (not target given)

optimizer.step() updates the weights (also called Parameters). Once weights are updated, the gradient values stored in input variables are useless. They are set to zero before next network is back-propagated. Its done by optimizer.zero\_grad()

  {\tiny (Ref: PyTorch: Fast Differentiable Dynamic Graphs in Python - Soumith Chintala)}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Update the weights}


 \begin{lstlisting}
net = OurNet(input_size, hidden_size, num_classes)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)
for t in range(500):
    y_pred = net(x)
    loss = criterion(y_pred, y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
\end{lstlisting}

  {\tiny (Ref: How Pytorch gives the big picture with deep learning - Déborah Mesquita)}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Update the weights}
\begin{itemize}
\item To compute the loss we'll use torch.nn.CrossEntropyLoss
\item 
One important thing about torch.nn.CrossEntropyLoss is that input has to be a 2D tensor of size (minibatch, n) and target expects a class index (0 to nClasses-1) as the target for each value of a 1D tensor of size minibatch.
\end{itemize}

  {\tiny (Ref: How Pytorch gives the big picture with deep learning - Déborah Mesquita)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Update the weights}
\begin{itemize}
\item The method torch.autograd.backward computes the sum of the gradients for given variables. 
\item As the documentation says, this function accumulates gradients in the leaves, so you might need to zero them before calling them. 
\item To update the parameters, all optimizers implement a step() method. 
\item The functions can be called once the gradients are computed, for example you can use backward() to call them.
\end{itemize}

  {\tiny (Ref: How Pytorch gives the big picture with deep learning - Déborah Mesquita)}
\end{frame}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Training}
Why is a need for an entire new class, when python does provide a way to define function?

\begin{itemize}
\item While training neural networks, there are two steps: the forward pass, and the backward pass. 
\item Normally, you would need to define two functions. One, to compute the output during forward pass, and another, to compute the gradient to be propagated.
\item PyTorch abstracts the need to write two separate functions (for forward, and for backward pass), into two member of functions of a single class called torch.autograd.Function.
\end{itemize}

{\tiny (Ref: Getting Started with PyTorch Part 1: Understanding how Automatic Differentiation works - Ayoosh Kathuria )}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Dynamic Computation Graphs}

\begin{itemize}
\item Dynamic Computation Graph, means the graph is generated on the fly.
\item Until the forward function of a Variable is called, there exists no node for the Variable (it's grad\_fn) in the graph.
\item The graph is created as a result of forward function of many Variables being invoked. 
\item Only then, the buffers are allocated for the graph and intermediate values (used for computing gradients later). 
\end{itemize}

{\tiny (Ref: Getting Started with PyTorch Part 1: Understanding how Automatic Differentiation works - Ayoosh Kathuria )}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Dynamic Computation Graphs}

\begin{itemize}
\item When you call backward(), as the gradients are computed, these buffers are essentially freed, and the graph is destroyed. 
\item You can try calling backward() more than once on a graph, and you'll see PyTorch will give you an error. 
\item This is because the graph gets destroyed the first time backward() is called and hence, there's no graph to call backward upon the second time.
\end{itemize}

{\tiny (Ref: Getting Started with PyTorch Part 1: Understanding how Automatic Differentiation works - Ayoosh Kathuria )}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Dynamic Computation Graphs}

\begin{itemize}
\item If you call forward again, an entirely new graph is generated. With new memory allocated to it.
\item By default, only the gradients (grad attribute) for leaf nodes are saved, and the gradients for non-leaf nodes are destroyed. But this behavior can be changed
\item he dynamic graph paradigm allows you to make changes to your network architecture during runtime, as a graph is created only when a piece of code is run. 
\end{itemize}

{\tiny (Ref: Getting Started with PyTorch Part 1: Understanding how Automatic Differentiation works - Ayoosh Kathuria )}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Derivative}
Call .backward() on a Variable
\begin{itemize}
\item Variable and Function are interconnected and build up an acyclic graph, that encodes a complete history of computation.
\item Each variable has a .grad\_fn attribute that references a Function that has created the Variable (except for Variables created by the user - their grad\_fn is None).
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Computing Derivative}
Call .backward() on a Variable
\begin{itemize}
\item Variable scalar (i.e. it holds a one element data): no arguments to backward()
\item More than one elements specify a grad\_output argument that is a tensor of matching shape.
\end{itemize}
\begin{lstlisting}
import torch
from torch.autograd import Variable
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Computing Derivative}
Call .backward() on a Variable
\begin{itemize}
\item Variable scalar (i.e. it holds a one element data): no arguments to backward()
\item More than one elements specify a grad\_output argument that is a tensor of matching shape.
\end{itemize}
\begin{lstlisting}
import torch
from torch.autograd import Variable
x = Variable(torch.ones(2, 2), requires_grad=True) # Create a variable
y = x + 2 # Do an operation of variable

\end{lstlisting}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Computing Derivative}
y was created as a result of an operation, so it has a grad\_fn.
\begin{lstlisting}
z = y * y * 3 # Do more operations on y
out = z.mean()
out.backward() # equivalent to doing out.backward(torch.tensor([1.0]))
print(x.grad)

Variable containing:
 4.5000  4.5000
 4.5000  4.5000
[torch.FloatTensor of size 2x2]
\end{lstlisting}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pyt2}
\end{center}
\end{frame}


 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
\frametitle{Testing}
\begin{itemize}
\item Not creating a graph is extremely useful when we are doing inference, and don't need gradients.
\end{itemize}
   
\end{frame} 

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
\frametitle{Testing}
 In Pytorch, you can use LSTM model right away. If we apply iterations, it keeps improving weights, thats it. But you can query prediction at the start itself. That will not be very good as the weights are not stable at that point in time. Prediction needs to be under no\_grad() scope.
 
 \begin{lstlisting}
with torch.no_grad():
    for context, target in test_trigrams:
        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)
        log_probs = model(context_idxs)
        max_prob_index = np.argmax(log_probs).numpy()
        print(ix_to_word[max_prob_index.item()], " ", target)
\end{lstlisting}     
\end{frame} 

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
\frametitle{Example}
 \begin{itemize}
\item Lets write a simple regression workflow using Pytorch syntax but without nn module
\item  
 Inputs are 6 (2d) points and output is corresponding real values, total 6.
 \end{itemize}

 \begin{lstlisting}
from torch.autograd import Variable
import torch

x = Variable(torch.tensor([[1.0, 1.0], 
                           [1.0, 2.1], 
                           [1.0, 3.6], 
                           [1.0, 4.2], 
                           [1.0, 6.0], 
                           [1.0, 7.0]]))
y = Variable(torch.tensor([1.0, 2.1, 3.6, 4.2, 6.0, 7.0]))
\end{lstlisting}   

{\tiny (Ref: https://discuss.pytorch.org/t/understanding-how-torch-nn-module-works/122 )} 
\end{frame} 

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
\frametitle{Example}
\begin{lstlisting}
weights = Variable(torch.zeros(2, 1), requires_grad=True) # w1, w2

for i in range(5000):
	weight.grad.data.zero_()
    prediction = x.mm(weights) # matrix multiply
    loss = torch.mean((prediction - y)**2)
    loss.backward()
    weights.data.add_(-0.0001 * weights.grad.data) #add_ is inplace
    
    if loss.data[0] < 1e-3:
        break
print('n_iter', i)
print(loss.data[0])
>>>n_iter 1188
0.0004487129335757345
\end{lstlisting}     
\end{frame} 

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
\frametitle{Example}
\begin{lstlisting}
import torch.nn.functional as F

class Model(torch.nn.Module):
    
    def __init__(self):
        super(Model, self).__init__()
        #self.weights = Variable(torch.zeros(2, 1), requires_grad=True) # Does not work
		#self.weights = Parameter(torch.zeros(2, 1), requires_grad=True) # Works
		self.fc = torch.nn.Linear(2, 1)
    
    def forward(self, x):
        #prediction = x.mm(self.weights)
        #return prediction
		return self.fc(x) 
        
\end{lstlisting}     
\end{frame} 

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
\frametitle{Example}
\begin{lstlisting}
model = Model()
criterion = torch.nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
loss2 = []

for i in range(5000):
    optimizer.zero_grad()
    outputs = model(x)
    
    loss = criterion(outputs, y)
    loss2.append(loss.data[0])
    loss.backward()        
\end{lstlisting}     
\end{frame} 





