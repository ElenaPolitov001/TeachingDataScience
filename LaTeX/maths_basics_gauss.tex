%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Gauss Elimination }
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]{Gaussian Elimination}
\begin{itemize} 
\item We start with the left-most
non-zero column,
\item Working to the right and from the top down.  
\item At 
each stage, we will be working with the portion of the matrix which 
is below or to the right (or both) of the pivot.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]{Gaussian Elimination}
\begin{itemize} 
\item Find the left-most column containing a non-zero entry.  This is 
a pivot column: the pivot position is at the top.   
\item Select a non-zero entry in the column to be the pivot. By 
interchanging rows if necessary, move the pivot into the pivot position.  
\item Use row replacement operations to change all the values below the pivot
to zero.  
\item Move to the next row, and apply steps 1, 2, 3 to the remaining
submatrix, namely the rows below and including the current row.
\end{itemize}

Once we've gone through all the rows, the matrix is in row echelon form.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \begin{frame}[fragile]\frametitle{Gaussian Elimination}

\begin{itemize}   
\item Scale pivots to be 1 
\item Use row replacement operations to change all the values above 
pivots to be zero.  (There are technical reasons for doing this from the 
bottom pivot first).
\end{itemize}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]\frametitle{Gaussian Elimination}

In order to obtain row echelon form, we only need to switch rows and subtract 
multiples below.

This process is often referred to as Gaussian elimination, or 
Gauss-Jordan elimination

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]\frametitle{Example}

 Row reduce 
 \begin{equation*}
   \begin{bmatrix}
   1& 3 & 5 & 7 \\
   3 & 5 & 7 & 9 \\
   5& 7 & 9 & 1
  \end{bmatrix}
  \rightarrow  \cdots
 \end{equation*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]\frametitle{Solutions of Linear Equations}
Recall that there are three possible outcomes:  
\begin{itemize}
 \item No solutions: for example
 \[
  \begin{bmatrix}
   1 & 0 & 0 & 5 \\
   0 & 1 & 1 & 3 \\
   0 & 0 & 0 & 6
  \end{bmatrix}
 \]


\item Unique solution: for example
\[
  \left[\begin{array}{rrrr}
   1 & 0 & 0 & -5 \\
   0 & 1 & 0 & 2 \\
   0 & 0 & 1 & 6
  \end{array}\right]
 \]
 
 \item Infinitely many solutions: for example
 \[
  \begin{bmatrix}
   1 & 2 & 0 & 6 \\
   0 & 0 & 1 & 3 \\
   0 & 0 & 0 & 0
  \end{bmatrix}
 \]
 \end{itemize}
 \end{frame}
 
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]\frametitle{General Form}
 In the last case, we write the solution set as 
 \[
  \left\{ 
  \begin{array}{rl}
   x_1 & =  6-2x_2 \\
   x_2 &  \mbox{is free}\\
   x_3 & =  3
  \end{array}
  \right.
 \]

\begin{itemize} 
\item This is called a {\em general solution}.
\item  $x_1$ and $x_3$ are called dependent variables.
\item $x_2$ is called a parameter or free variable or an independent variable.
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]
 Find the general solution to the linear system with augmented matrix
 \begin{equation*}
   \left[\begin{array}{rrrrrr}
   1 & 6 & 2 & -5 & -1 & -4 \\
   0 & 0 & 2 & -8 & -1 & 3 \\
   0 & 0 & 0 & 0  & 1  & 7
  \end{array}\right]
  \rightarrow \cdots
 \end{equation*}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]
If a linear system is consistent then the solution set contains
either  
\begin{itemize}
\item A single element (-i.e. the solution is unique, there are no free variables).  
\item Infinitely many elements (-i.e. at least one free variable).
\end{itemize}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]

\textbf{Solution Procedures} 
Given a linear system to solve
\begin{itemize}
 \item Write the augmented matrix   
 \item Perform row reduction to obtain echelon form.  If the system is not consistent then there are no solutions and you may stop. 
 \item Perform row reduction to obtain reduced echelon form.  
 \item Write system of equations corresponding to reduced echelon form.  
 \item Basic variables correspond to columns with pivots.  Free variables correspond to
 columns without pivots.  
 \item Write basic variables in terms of free variables.
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]
\textbf{Definition}
 If $A$ is an $m \times n$ matrix with columns 
 $\vec{a}_1, \vec{a}_2, \dots ,\vec{a}_n$,
and $\vec{x} \in \mathbb R^n$, then the product of $A$ and $\vec{x}$, 
which we denote by $A\vec{x}$, is defined to be 
\[ 
 A \vec{x} = 
 \left[\begin{array}{cccc}
  | & | & \dots & | \\
  \vec{a}_1 & \vec{a}_2 & \dots & \vec{a}_n  \\
  | & | & \dots & | \\
 \end{array}\right]
\left[\begin{array}{r} 
 x_1 \\ x_2 \\ \vdots \\ x_n
\end{array}\right]
\]
\[
 = x_1 \vec{a}_1 + x_2 \vec{a}_2 + \dots + x_n \vec{a}_n
\]


\textbf{Note}
\begin{itemize}
\item $A\vec{x} \in \mathbb R^m$.
\item $A\vec{x}$ is the linear combination of the columns of $A$ with weights $x_1, x_2, \dots, x_n$.
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]
\textbf{Example}
\[
 \left[\begin{array}{rr}
  1 & 2 \\
  -1 & 3 \\
  3 & 0 
 \end{array}\right]
\left[\begin{array}{r}
 1 \\ -1
\end{array}\right]
=
\]

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]
\textbf{Theorem}
 If $A$ is an $m \times n$ matrix with columns 
 $\vec{a}_1, \vec{a}_2, \dots \vec{a}_n$, 
 and if $\vec{b}\in \mathbb R^m$, then the matrix equation 
 $A \vec{x} =\vec{b}$ has the same set of solutions 
 as the vector equation
 \[
  x_1\vec{a}_1 + x_2 \vec{a}_2 + \dots + x_n \vec{a}_n = \vec{b}
 \]
which in turn has the same set of solutions as the linear
system with augmented matrix 
\[
 \left[\begin{array}{ccccc}
  | & | & \dots & | & |  \\
  \vec{a}_1 & \vec{a}_2 & \dots & \vec{a}_n & \vec{b} \\
  | & | & \dots & | &| \\
 \end{array}\right]
\]


%\textbf{Proof}
%This follows from the definition of multiplication of a matrix and a vector.
%
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]
\textbf{Theorem} The equation $A \vec{x} = \vec{b}$ has a solution
 if and only if $\vec{b}$ is a linear combination of the columns of $A$.


%
%\textbf{Proof}
%This follows from the definition of multiplication of a matrix and a vector.
%
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]
\textbf{Example}
For which vectors $\vec{b}$ is the equation $A\vec{x}=\vec{b}$
solvable, where 
\[
 A = \left[\begin{array}{rrr}
      1 & 3 & -2 \\
      7 & 2 & 3  \\
      -2 & 13 &-13
     \end{array}\right]?
\]

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]
\textbf{Theorem}
Let $A$ be an $m \times n$ matrix. The following statements are equivalent:
\begin{itemize}
 \item For each $\vec{b}\in \mathbb R^m$, the equation $A\vec{x}=\vec{b}$ has a solution.
 \item Each $\vec{b} \in \mathbb R^m$ is a linear combination of the columns of $A$.
 \item The columns of $A$ span $\mathbb R^m$.
 \item In the row reduction process, $A$ has a pivot position in every row
\end{itemize}


\textbf{Note}
Part 4 refers to the coefficient matrix $A$, not the augmented matrix $[A \ \  \vec{b}]$.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]
\textbf{Proof}
Statements 1, 2 and 3 are equivalent by definition. \\ 
So if we show that 1 and 4 are 
equivalent, we will be done.  \\ 

\ \\
Suppose that $U$ is the reduced echelon form of $A$.  \\ 
Then for some vector $\vec{d}$, we have
\[
  [ A\ \  \vec{b}] \sim \dots \sim [ U \ \ \vec{d} ]
\]  \\ 
If statement 4 is true, then since $U$ has a pivot in every row,
we clearly don't have a row of zeros in $U$ with a non-zero element in $\vec{d}$. \\ 
Hence the equation is solvable no matter what $\vec{b}$ is.  \\  
Thus, if statement 4 is true, then 1 is true.  \\ 

\ \\ 
Conversely, if 4 is false, $U$ has a zero row.   \\ 
Let $\vec{d}$ be the vector with a 
1 in that row, and zeros elsewhere.  \\ 
Reversing the row reduction process we obtain a
vector $\vec{b}$ for which $A \vec{x} = \vec{b}$ has no solution.   \\ 
Hence if statement 4 is false, so is statement 1.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]\frametitle{Computing $A\vec{x}$}
In the computation of 
\[
 \left[\begin{array}{ccc}
  a_{11} & \dots & a_{1n}\\
  \vdots &       & \vdots \\
  a_{i1} & \dots & a_{in} \\
  \vdots &       & \vdots \\
  a_{m1} & \dots & a_{mn}
 \end{array}\right]
\left[\begin{array}{c} x_1 \\ \vdots \\ \vdots \\x_n \end{array}\right] 
=
\left[\begin{array}{c}
 b_1 \\ \vdots\\ b_i \\ \vdots \\ b_m 
\end{array}\right]
\]
we see that 
$$b_i = \sum_{j=1}^n a_{i j}x_j = a_{i1} x_1 + a_{i2} x_2 + \dots a_{in} x_n. $$
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]
\[
 \left[\begin{array}{rrr}
  1 & 3 & 5 \\
  2 & 4  & -1 \\
  -1 & 2 & 2 
 \end{array}\right]
\left[\begin{array}{r}
 11 \\ 1 \\ 3
\end{array}\right]
=
\left[\begin{array}{r}
 \qquad \\ \qquad \\  \qquad
\end{array}\right]
\]

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]
\textbf{Theorem} If $A$ is an $m \times n$ matrix and $\vec{u}, \vec{v}$ are vectors in 
$\mathbb R^n$ and  $c\in \mathbb R$ is a scalar then 
 \begin{itemize}
  \item $A ( \vec{u} + \vec{v}) = A \vec{u} + A\vec{v}$
  \item $A(c\vec{u}) = c(A\vec{u})$
 \end{itemize}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]
\textbf{Definition}  
 A system of linear equations is called {\em homogeneous} if it can be written as 
 $A \vec{x} = \vec{0}$.


\textbf{Note}
$\vec{x}= \vec{0}$ is always solution to $A\vec{x}= \vec{0}$.
It is called the {\em trivial solution}.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]
\textbf{Facts}
The homogeneous equation $A \vec{x} = \vec{0}$ has a non-trivial 
solution if and only if it has free variables.


\textbf{Proof}
($\Rightarrow$):  Suppose that $A \vec{x} = \vec{0}$ has a nontrivial solution
Then we have at least two solutions (-i.e. The trivial one and at least one other).
Thus we must have an infinite number of solutions which requires a free variable. 

($\Leftarrow$):  Now suppose that when we row reduce the augmented matrix $[A : \vec{0}]$ that we have a free variable. 
Since we already know that we have a solution (the trivial one), and that we have a free variable, we must have infinitely many solutions . 
Thus we have nontrivial solutions.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]
\textbf{Exercise}
\begin{itemize}
 \item  Determine if the following homogeneous system has non-trivial solutions: 
\[  \left\{
 \begin{array}{rcrcrcr}
  2x_1 & + & 3x_2 &  + & x_3 & = & 0 \\
       &   & 5x_2 &  - & x_3 & = & 0 \\
  -x_1 & + &  x_2 & -  & x_3 & = & 0 
 \end{array}  \right.
\]   
\item  Describe the solution set.
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]
\textbf{Note}
The solution set of $A \vec{x} = \vec{0}$ can always be 
written as Span$(\vec{v}_1, \dots, \vec{v}_p)$ for some vectors 
$\vec{v}_1, \dots, \vec{v}_p$.


\textbf{Definition} 
 An equation of the form 
 \[
  \vec{x} = s_1\vec{v}_1 + s_2 \vec{v}_2 + \dots + s_k \vec{v}_k
 \]
is said to be in {\em vector parametric form}.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]
\textbf{Example}
Describe the solution sets of $A\vec{x}=\vec{0}$ and  $A \vec{x} = \vec{b}$ where 
\[
 A = \left[\begin{array}{rrr}
      3 &  5 &  -4 \\
     -3 & -2 &   4 \\
      6 &  1  &   -8 
     \end{array}\right]
\qquad  \mbox{and} \qquad
\vec{b} = \left[ \begin{array}{r}
                 7 \\ -1 \\ -4
                \end{array}\right]
\]


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]
\textbf{Theorem} Suppose that the equation $A \vec{x} = \vec{b}$ 
has a solution $\vec{p}$.  Then all solutions to the equation have the 
form 
\[
 \vec{w} = \vec{p } + \vec{v}_h
\]
where $\vec{v}_h$ is a solution to the corresponding homogeneous 
equation $A \vec{x} = \vec{0}$.


\textbf{Note}
That is, if $\vec{p}$ is any solution to $A \vec{x} = \vec{b}$,
and the solution set of $A \vec{x} = \vec{0}$ is 
Span$(\vec{v}_1, \dots , \vec{v}_k)$, then the solution set of 
$A \vec{x} = \vec{b}$ is 
\[ \vec{p} + \mbox{Span}(\vec{v}_1, \dots , \vec{v}_k) \]

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]
\textbf{Definition}  An indexed set of vectors 
$\{ \vec{v}_1, \dots , \vec{v}_p \}$ in $\mathbb R^n$ is \textbf{linearly independent}
if the vector equation 
\[
 x_1 \vec{v}_1 + \dots + x_p \vec{v}_p = \vec{0}
\]
has only the trivial solution $(\vec{x} = \vec{0})$.

Otherwise if there exist $c_1,\dots, c_p \in \mathbb R$ not all zero, so that 
\[
 c_1 \vec{v}_1 + \dots + c_p \vec{v}_p = \vec{0}
\]
then the set is {\em linearly dependent}.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]
\textbf{Example}
 Are the vectors 
$\vec{v}_1 = \left[\begin{array}{r} 1 \\2 \\3 \end{array}\right] $, 
$\vec{v}_2 = \left[\begin{array}{r} -1 \\ 1 \\ 5 \end{array}\right] $, 
$\vec{v}_3 = \left[\begin{array}{r} -1 \\ 7 \\ 21 \end{array}\right] $, 
linearly independent?  If not, find a dependence.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]
\textbf{Note}
The columns of the matrix $A$ are linearly independent if and 
only if the equation $A \vec{x} = \vec{0}$ has only the trivial solution.


\textbf{Example}
Are the columns of $A = 
\left[\begin{array}{rrr}
 0 & 0 & 1 \\ 
 1 & 2 & 2 \\
 5 & 3 & 3 \\
\end{array}\right]
$
linearly independent?

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]
\textbf{Note}
\begin{itemize}
\item A set containing a single vector $\vec{v}$ is linearly independent
if and only if $\vec{v} \neq 0$.
\item A set containing two vectors is linearly independent if and only if neither
vector is a multiple of the other.
\end{itemize}

%
%
%
%\textbf{Proof}
%\ \\
%\ \\
%\ \\
%\ \\
%\ \\
%\ \\
%
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]
\textbf{Theorem} 
 An indexed set $S = \{ \vec{v}_1, \dots , \vec{v}_p \}$ is linearly dependent
 if and only if one of the vectors in $S$ is a linear combination of the others.
 In fact, $S$ is linearly dependent if and only if either 
 $\vec{v}_1 = \vec{0}$, or there is a $j$ so that 
 $\vec{v}_j$ is a linear combination of 
 $\vec{v}_1, \vec{v}_2, \dots, \vec{v}_{j-1}$.

%
%
%
%\textbf{Proof}
%\ \\
%\ \\
%\ \\
%\ \\
%\ \\
%\ \\
%\ \\
%\ \\
%\ \\
%
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]
\textbf{Example}
Let $\vec{u} = \left[\begin{array}{r} 3 \\ 1 \\ 0 \end{array}\right]$, 
$\vec{v} = \left[\begin{array}{r} 1 \\ 0 \\ 0 \end{array}\right]$.  Describe 
Span$(\vec{u}, \vec{v})$.
For this particular $\vec{u}, \vec{v}$ we have 
$\vec{w} \in \mbox{Span}(\vec{u}, \vec{v})$ if and only if 
$\{ \vec{u}, \vec{v}, \vec{w} \}$ is linearly dependent.  Explain.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]
\textbf{Theorem}
 If a set contains more vectors than there are entries (that is, rows) 
 in the vectors, then it is linearly dependent.

%
%\textbf{Proof}
%\ \\
%\ \\
%\ \\
%\ \\
%\ \\
%\ \\
%
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]
\textbf{Theorem}
 If $\vec{0} \in S = \{ \vec{v}_1, \dots , \vec{v}_p \}$ 
 then $S$ is linearly dependent.

%
%
%
%\textbf{Proof}
%\ \\
%\ \\
%\ \\
%\ \\
%\ \\
%\ \\
%
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]\frametitle{Testing for linear dependence}
To test whether vectors $\vec{v}_1, \vec{v}_2, \dots \vec{v}_k$ are linearly 
dependent, construct the matrix $A=[\vec{v}_1, \vec{v}_2, \dots \vec{v}_k]$ having them as columns.  
Perform row reduction on the matrix.

If every column contains a pivot, then the only solution to $A \vec{x} = \vec{0}$
is $\vec{x}=\vec{0}$, and hence the vectors are linearly independent, since then
the only solution to 
\[
 x_1 \vec{v}_1 + x_2 \vec{v}_2  + \dots + x_k \vec{v}_k = \vec{0}
\]
is $x_1=x_2=\dots=x_k=0$.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{frame}[fragile]\frametitle{Testing for linear dependence}
On the other hand, if there is a column without a pivot, then there are infinitely many 
solutions to the equation $A \vec{x} = \vec{0}$ (since there is a free 
variable in the general solution to the equation): pick a non-zero solution: it 
will correspond to a non-trivial solution to
\[
 x_1 \vec{v}_1 + x_2 \vec{v}_2  + \dots + x_k \vec{v}_k = \vec{0}
\]
and hence the vectors are linearly dependent.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  \begin{frame}[fragile]
%We are going to talk about special kinds of functions from $\mathbb R^n$ to $\mathbb R^m$ 
%(note: $n$ comes before $m$ here!)
%
%First: what is a function?  
%\textbf{Definition}
%We write $f : A \longrightarrow B$ to denote that $f$ is a 
%\textbf{function} from the set $A$ to the set $B$.  What this means is that $f$ is a rule, which 
%assigns, for each element $a \in A$, a unique element $b \in B$ so that $b = f(a)$.
%
%We refer to $A$ as the \textbf{domain} of $f$, and to $B$ as the \textbf{co-domain}. 
%
% The set 
%\[
% \{ f(a) : a \in A \}
%\]
%of values taken by the function is called the \textbf{image} or \textbf{range} of $f$.
%
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  \begin{frame}[fragile]
%We will restrict our attention to functions which interact nicely with vector addition and scalar multiplication. \\ 
%\textbf{Definition}
% A {\em linear transformation} from $\mathbb R^n$ to $\mathbb R^m$ is a function $T: \mathbb R^n \longrightarrow \mathbb R^m$
%which satisfies the following two properties: whenever $\vec{u}, \vec{v} \in \mathbb R^n$
%and $c \in \mathbb R$, then 
%\begin{itemize}
% \item $T(\vec{u} + \vec{v}) = T(\vec{u}) + T (\vec{v})$  
% \item $T(c\vec{u}) = cT(\vec{u})$.
%\end{itemize}
%
%
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  \begin{frame}[fragile]
%\textbf{Note}
%If $A$ is  a $m \times n$ matrix, then we can  define $T:\mathbb R^n\rightarrow \mathbb R^m$ by $T(\vec{x})=A\vec{x}$.  \\ 
%We have already proved that $T(\vec{x}+\vec{y})= A(\vec{x}+\vec{y}) =  A\vec{x}+A\vec{y}=T(\vec{x})+T(\vec{y})$ 
%and $T(\alpha\vec{x})=A(\alpha\vec{x})=\alpha A\vec{x}=\alpha T(\vec{x})$ for any $\alpha\in  \mathbb R$.
%
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  \begin{frame}[fragile]
%\textbf{Example}
% Let 
%\[
% A = \left[\begin{array}{rrr}
% 1 & 2 \\
% 2 & 3 \\
% 5 & -1
% \end{array}\right]
%\]
%Define $T : \mathbb R^2 \longrightarrow \mathbb R^3$ by $T(\vec{x}) = A \vec{x}$.
%\begin{itemize}
%\item $ 
% T \left[
%  \left[\begin{array}{rrr}
%   1 \\ 1 
%  \end{array}\right]
%  \right] = 
%$
%\item  Find all $\vec{x}\in \mathbb R^2$ so that $T(\vec{x}) = \left[\begin{array}{rrr} 3 \\5 \\ 4 \end{array}\right]$.
%\end{itemize}
%
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  \begin{frame}[fragile]
%\textbf{Example}
% Let 
%\[
% A = \left[\begin{array}{rrr}
%      1 & 0 & 0 \\
%      0 & 1 & 0 \\
%      0 & 0 & 0 
%     \end{array}\right].
%\]
%Then $T:\mathbb R^3 \longrightarrow \mathbb R^3$ given by $T(\vec{x}) = A \vec{x}$ is called a 
%projection: 
%\[ T \left[  \left[\begin{array}{rrr} x_1 \\ x_2 \\ x_3 \end{array}\right] \right] 
%= 
%\left[\begin{array}{rrr}
% 1 & 0 & 0 \\
%      0 & 1 & 0 \\
%      0 & 0 & 0 
%     \end{array}\right].
%     \left[\begin{array}{rrr} x_1 \\ x_2 \\ x_3 
%\end{array}\right]
%= 
%\left[\begin{array}{rrr}
% \qquad \\ \qquad \\ \qquad
%\end{array}\right]
%\]
%
%
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  \begin{frame}[fragile]
%\textbf{Example}
%Consider the map $T:\mathbb R^2\rightarrow \mathbb R^2$ defined by $T(\vec{x})=A\vec{x}$ where
%$A = \left[\begin{array}{rrr} 1 & 2 \\ 0 & 1 \end{array}\right]$.  This is an example of a \textbf{shear transformation}.
%Draw the image of the box $[0,1]\times[0,1]$ under $T$ to see why.
%
%
%
%\textbf{Note}
%If $T$ is a linear transformation, then 
%\begin{itemize}
% \item $T (\vec{0} = \vec{0}$.
% \item $T (c \vec{u} + d \vec{v}) = c T(\vec{u}) + d T(\vec{v})$. 
% \item $\displaystyle
% T \left[\sum_{i=1}^p c_i \vec{v}_i  \right]
%  = \sum_{i=1}^p c_i T(\vec{v}_i)$
%\end{itemize}
%
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  \begin{frame}[fragile]
%\textbf{Example}
%Define a map $T_r: \mathbb R^n \longrightarrow \mathbb R^n$ by $T \vec{x} = r \vec{x}$.
%If $0 < r < 1$, the map is called a {\em contraction}. If $r > 1$ it is called a dilation.
%Show that $T_r$ is a  linear transformation.
%
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  \begin{frame}[fragile]
%\textbf{Recall}
%A function $T: \mathbb R^n \longrightarrow \mathbb R^m$ is called a 
%linear transformation if for every $\vec{u}, \vec{v}\in \mathbb R^n$ and $c \in \mathbb R$,
%\begin{itemize}
% \item $T(\vec{u} + \vec{v}) = T(\vec{u} ) + T(\vec{v})$.
% \item $T(c\vec{u}) = c T(\vec{u})$
%\end{itemize}
%
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  \begin{frame}[fragile]
%\textbf{Example}
%Suppose $T : \mathbb R^2 \longrightarrow \mathbb R^3$ is a linear transformation, and that we 
%know 
%\[
% T(\vec{e}_1)  = \left[\begin{array}{rrr} 1 \\ 1 \\ 1 \end{array}\right] 
% \mbox{ \qquad and \qquad }
% T(\vec{e}_2)  = \left[\begin{array}{rrr} 1 \\ 2 \\ 3 \end{array}\right] 
%\]
%where $\vec{e}_1 = \left[\begin{array}{rrr} 1 \\ 0 \end{array}\right] $ and 
%$\vec{e}_2 = \left[\begin{array}{rrr} 0 \\ 1 \end{array}\right]$.  
%
%%We can give a complete 
%%description of $T$ as follows:
%%
%%\begin{eqnarray*}
%% T\left[\begin{array}{rrr} x \\ y \end{array}\right] &=& T(x \vec{e_1} + y\vec{e}2)\\
%%  	& \qquad & \\
%% 	& = & \\
%% 	& \qquad & \\
%%        & = &  	   \\	
%% 	& \qquad & 
%% \end{eqnarray*}
%%Can you find a matrix $A$ so that $T(\vec{x})$ is the same vector as $A \vec{x}$?\\
%%(Hint: $A \vec{x}$ is a linear combination of the columns of $A$)
%
%Give a complete description of $T$.
%
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  \begin{frame}[fragile]
%\textbf{Definition}
%Let $\vec{e}_j \in \mathbb R^n$ denote the vector having a 1 in the $j^{th}$ row, and 
%zeros elsewhere.
%
%
%
%\textbf{Theorem}
%Let $T: \mathbb R^n \longrightarrow \mathbb R^m$ be a linear transformation.  Then there exists a unique matrix 
% $A$ so that for every $\vec{x}\in \mathbb R^n$, 
% \[
%  T(\vec{x}) = A \vec{x}.
% \]
%In fact, $A$ is the $m \times n$ matrix whose $j^{th}$ column is $T(\vec{e}_j)$.\\
%-i.e.
%\[
% A = \left[\begin{array}{rrrr}
%      | & | &  & | \\
%      T(\vec{e}_1) & T(\vec{e}_2) & \dots & T(\vec{e}_n)\\
%      | & | &  & | 
%     \end{array}\right].
%\]
%
%
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  \begin{frame}[fragile]
%\textbf{Example}
%Find the matrix representing the transformation
%$T: \mathbb R^2 \longrightarrow \mathbb R^2$ given by $T(\vec{x}) = 3 \vec{x}$.
%\ \\
%\ \\
%\ \\
%
%
%\textbf{Definition}
% \begin{itemize}
%  \item  A linear transformation $T :\mathbb R^n \longrightarrow \mathbb R^m$ is said to be  \textbf{onto} $\mathbb R^m$ if every $\vec{b}\in \mathbb R^m$ is the image of at least one
%  $\vec{x} \in \mathbb R^n$.  ($T$ is onto provided that $\forall \ \vec{b} \in \mathbb R^m, \
%  \exists \ \vec{x} \in \mathbb R^n \ \mbox{ so that } \ T(\vec{x}) = \vec{b}$.)    
%  \item  A linear transformation $T :\mathbb R^n \longrightarrow \mathbb R^m$ is said to be  \textbf{one-to-one} (or 1-1) if every $\vec{b} \in \mathbb R^m$ is the image of at most
%  one $\vec{x}\in \mathbb R^n$.  (That is, if 
%  $T(\vec{x})= T(\vec{y})$ then $\vec{x}= \vec{y}$.)
% \end{itemize}
%
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  \begin{frame}[fragile]
%\textbf{Example}
%Suppose that $T$ is the linear transformation with matrix 
%\[
% \left[\begin{array}{rrrr}
%  1 & 2 & 5 & 6\\
%  0 & 1 & 3 & 4 \\
%  0 & 0 & 0 & 2
% \end{array}\right]
%\]
%\begin{itemize}
% \item If $T: \mathbb R^n\longrightarrow \mathbb R^m$, what are $m$ and $n$?
% \item Is $T$ 1-1?  What do you have to check?
% \item Is $T$ onto? What do you have to check?
%\end{itemize}
%
%\end{frame}


%
%
%
%  \begin{frame}[fragile]
%\textbf{Theorem}
%A linear transformation $T :\mathbb R^n \longrightarrow \mathbb R^m$ 
% is 1-1 if and only if $T \vec{x}= \vec{0}$ has 
% only the trivial solution.
%
%
%
%\textbf{Proof}
%\ \\
%\ \\
%\ \\
%\ \\
%\ \\
%\ \\
%
%\end{frame}
%
%
%
%
%
%  \begin{frame}[fragile]
%\textbf{Theorem}
%Let $T : \mathbb R^n \longrightarrow \mathbb R^m$ be a linear transformation and let $A$ be its matrix.
% \begin{itemize}
%  \item $T$ is onto if and only if the span of the columns of $A$ is all of $\mathbb R^m$.
%  \item T is 1-1 if and only if the columns of $A$ are linearly independent.
% \end{itemize}
%
%
%
%\textbf{Proof}
%\ \\ 
%\ \\
%\ \\
%\ \\
%\ \\
%\ \\
%
%\end{frame}
%
%
%
%
%
%  \begin{frame}[fragile]\frametitle{Detecting 1-1 and Onto conditions}
%\textbf{Note}
%As a consequence of this theorem we can check whether $T$ is 1-1 or onto
%by row-reducing its matrix $A$.  \\ 
%
%If there is a pivot in every row, then $T$ is onto.  \\ 
%
%if there is a pivot in every column, then $T$ is 1-1.
%
%
%
%\textbf{Example}
%Let $T \left[\begin{array}{rrr} x_1 \\ x_2 \end{array}\right] = 
%\left[\begin{array}{rrr} 3x_1 + x_2 \\ 5x_1 + 7 x_2 \\ x_1 + 3 x_2 \end{array}\right]$.
%Is $T$ 1-1 and/or  onto?
%
%\end{frame}
%
%
%
%  \begin{frame}[fragile]
%\textbf{Notation}
%Let $A$ be an $m \times n$ matrix, that is, $m$ rows and $n$ columns.
%We'll refer to the entries of $A$ by their row and column indices.  The entry in
%the $i^{th}$ row and $j^{th}$ column is denoted by $a_{ij}$, and is called the 
%$(i,j)$-entry of $A$.  
%\[
% \left[\begin{array}{rrrrr}
%  a_{11} & \dots & a_{1j} & \dots & a_{1n} \\
%  \vdots &       & \vdots &       & \vdots \\
%  a_{i1} & \dots & a_{ij} & \dots & a_{in} \\
%  \vdots &       & \vdots &       & \vdots \\
%  a_{m1} & \dots & a_{mj} & \dots & a_{mn}
% \end{array}\right]
%\]
%
%\end{frame}
%
%
%
%
%
%  \begin{frame}[fragile]
%\textbf{Notation}
%The columns of $A$ are vectors in $\mathbb R^m$, and are denoted in the book
%by ${\bf{a}}_1,{\bf{a}}_2,\dots ,{\bf{a}}_n$ and in my notes by 
%$\vec{a}_1, \vec{a}_2, \dots , \vec{a}_n$.
%In order to focus attention on the columns we write 
%\[
% A = [ \vec{a}_1\ \ \  \vec{a}_2 \ \ \ \dots \ \ \  \vec{a}_n]
%\]
%
%
%\end{frame}
%
%
%
%
%
%  \begin{frame}[fragile]
%\textbf{Definition}
%Suppose that $A= [ \vec{a}_1\ \ \  \vec{a}_2 \ \ \ \dots \ \ \  \vec{a}_n]$ and $B=[ \vec{b}_1\ \ \  \vec{b}_2 \ \ \ \dots \ \ \  \vec{b}_n]$ are both $m\times n$ matrices.
%Then we define their sum as
%\begin{eqnarray*}
%A+B &=&
% [ \vec{a}_1\ \ \  \vec{a}_2 \ \ \ \dots \ \ \  \vec{a}_n]
%+ [ \vec{b}_1\ \ \  \vec{b}_2 \ \ \ \dots \ \ \  \vec{b}_n]   \\
%&=& [ (\vec{a}_1+\vec{b}_1)\ \ \  (\vec{a}_2+\vec{b}_2) \ \ \ \dots \ \ \  (\vec{a}_n+\vec{b}_n)]
%\end{eqnarray*}
%
%
%
%\textbf{Alternative Definition}
%Given two $m\times n$ matrices $A$ and $B$, we can define their sum $C=A+B$ as the $m\times n$ matrix whose entries are $c_{i j} = a_{i j} + b_{i j}$.
%
%\end{frame}
%
%
%
%
%
%
%  \begin{frame}[fragile]
%\textbf{Example}
%\[
% \left[\begin{array}{rrrr}
%  1 & 2\\
%3 & -1 \\
%2 & 4 
% \end{array}\right]
%+
% \left[\begin{array}{rrrr}
%  -1 & 3\\
%2 & -1 \\
%-3 & 4 
% \end{array}\right]
%=
%\left[\begin{array}{rrrr}
% \mbox{ } & \qquad \qquad\mbox{} \\
% \mbox{ } & \mbox{} \\
% \mbox{ } & \mbox{}
%\end{array}\right]
%\]
%
%\end{frame}
%
%
%
%
%
%  \begin{frame}[fragile]
%\textbf{Definition}
%We denote by $0$ the matrix  all of whose elements are  zero.
%
%
%
%\textbf{Theorem}
% Let $A, B , C$ be matrices of the same size, and let $r,s$ be scalars.
%Then
%\begin{itemize}
%\item $A+B = B+A$
%\item $(A+B)+C = A + (B+C)$
%\item $A+ 0= A$
%\item $r(A+B)=rA+rB$
%\item $(r+s)A = rA + sA$
%\item $r(sA)= (rs)A$
%\end{itemize}
%
%
%
%\textbf{Note}
%This is very similar to the corresponding theorem for vector 
%addition.
%
%\end{frame}
%
%
%
%
%
%  \begin{frame}[fragile]\frametitle{Composition of Functions}
%\textbf{Recall}
%If $f$ and $g$ are functions and the image of $g$ is contained in the domain of $f$, then we define the \textbf{composition} of $f$ and $g$ by
%\begin{equation*}
%f\circ g (x) = f(g(x))
%\end{equation*}
%
%
%
%\textbf{Example}
%Suppose that $f,g :\mathbb R\rightarrow \mathbb R$ are defined by
%$f(y) = y^2$ and $g(x)=\sin(x)$,  then   \\  $f\circ g(x) = f(g(x))= (\sin(x))^2$
%
%\end{frame}
%
%
%
%
%
%  \begin{frame}[fragile] \frametitle{Composition of Linear Transformations}
%\textbf{Note}
%\begin{itemize}
%\item  We will specialize our attention to linear transformations from $\mathbb R^n$ to $\mathbb R^m$.
%\item  We have seen that a linear transformation $T :\mathbb R^n \longrightarrow \mathbb R^m$ corresponds to multiplying
%a vector $\vec{x}\in \mathbb R^n$ by an $m \times n$ matrix $A$.  
%\item  We will consider linear transformations 
%\[
% \mathbb R^p  \xrightarrow{\ U\ } \mathbb R^n \xrightarrow{\ T\ } \mathbb R^m,
%\]
%where $U:\mathbb R^p \longrightarrow \mathbb R^n$, and $T: \mathbb R^n \longrightarrow \mathbb R^m$.
%So, since $U(\vec{x})$ is in the domain of $T$, we can compute $T(U(\vec{x}))$.
%\end{itemize}
%
%\end{frame}
%
%
%
%
%
%
%  \begin{frame}[fragile]
%\textbf{Facts}
%The composition of two linear transformations is again a linear transformation and thus can be written as multiplication by a matrix.
%
%\end{frame}
%
%
%
%
%
%  \begin{frame}[fragile]\frametitle{Matrix Multiplication}
%Suppose that 
%\[
% \mathbb R^p  \xrightarrow{\ U\ } \mathbb R^n \xrightarrow{\ T\ } \mathbb R^m.
%\]
%Let $B$ be the $n \times p$ matrix corresponding to
%the transformation $U$. \\ 
%Let $A$ be the $m\times n$ matrix corresponding to the transformation $T$.  \\ 
%(-i.e. $U(\vec{x})=B\vec{x}$, and $T(\vec{v})=A \vec{v}$).  \\ 
%Then we have
%\begin{eqnarray*}
% T\circ U(\vec{x}) &=& T(U(\vec{x}))=  A(B\vec{x})  =  A ( x_1 \vec{b}_1 + x_2 \vec{b}_2 + \dots + x_p \vec{b}_p) \\   
% & = & A x_1 \vec{b}_1 + Ax_2 \vec{b}_2 + \dots + Ax_p \vec{b}_p \\  
%& = & x_1 A \vec{b}_1 + x_2 A \vec{b}_2 + \dots + x_p A\vec{b}_p\\   
%& = & [ A \vec{b}_1 \ \ \  A \vec{b}_2 \ \ \  \dots \ \ \  A \vec{b}_p ] \ \vec{x}
%\end{eqnarray*}
%
%
%Thus, $T\circ U(\vec{x}) =  [ A \vec{b}_1 \ \ \  A \vec{b}_2 \ \ \  \dots \ \ \  A \vec{b}_p ] \ \vec{x}$.
%\end{frame}






  % \begin{frame}[fragile]\frametitle{Elementary Row Operations}
% \textbf{Recall}
% Denoting rows $r$ and $s$ by $R_r$ and $R_s$, the row operations are:
 % \begin{description} 
   % \item[$R_r\leftrightarrow R_s$] Interchange rows  $R_r$ and $R_s$ of a matrix.
   % \item[$cR_r$] For a non-zero $c\in \mathbb R$, replace $R_r$ by $cR_r$.
   % \item[$R_r+cR_s$] Replace $R_r$  by $R_r + cR_s$ 
% \end{description}



% \textbf{Definition}
% An \textbf{elementary matrix} is any $n\times n$ matrix that can be obtained by performing a single elementary row operation to $I_n$.

% \end{frame}






  % \begin{frame}[fragile]
% \textbf{Example}
% We construct three elementary matrices below.
% \begin{eqnarray*}
% \left[\begin{array}{rrr} 1& 0& 0\\ 0& 1& 0\\ 0& 0& 1\end{array}\right] &\xrightarrow{ R_2+2R_3 }& \left[\begin{array}{rrr}1& 0& 0\\ 0& 1& 2\\ 0& 0& 1\end{array}\right]\\
% \left[\begin{array}{rrr} 1& 0& 0\\ 0& 1& 0\\ 0& 0& 1\end{array}\right] &\xrightarrow{ R_2\leftrightarrow R_3 }& \left[\begin{array}{rrr}1& 0& 0\\ 0& 0& 1\\ 0& 1& 0\end{array}\right]\\
% \left[\begin{array}{rrr} 1& 0& 0\\ 0& 1& 0\\ 0& 0& 1\end{array}\right] &\xrightarrow{ 3R_1 }& \left[\begin{array}{rrr}3& 0& 0\\ 0& 1& 0\\ 0& 0& 1\end{array}\right]\\
% \end{eqnarray*}

% \end{frame}






  % \begin{frame}[fragile]
% \textbf{Example}
% Multiply the general $3\times 3$ matrix on the left by each of the above matrices.
% \begin{eqnarray*}
 % \left[\begin{array}{rrr}1& 0& 0\\ 0& 1& 2\\ 0& 0& 1\end{array}\right]
                % \left[\begin{array}{rrr} a_{1 1}& a_{1 2} & a_{1 3}\\a_{2 1}& a_{2 2} & a_{2 3}\\a_{3 1}& a_{3 2} & a_{3 3}\end{array}\right]
                % &=& \left[\begin{array}{rrr} \quad &\quad & \quad \\ \quad &\quad & \quad \\ \quad &\quad & \quad \end{array}\right] \\
 % \left[\begin{array}{rrr}1& 0& 0\\ 0& 0& 1\\ 0& 1& 0\end{array}\right]
                 % \left[\begin{array}{rrr} a_{1 1}& a_{1 2} & a_{1 3}\\a_{2 1}& a_{2 2} & a_{2 3}\\a_{3 1}& a_{3 2} & a_{3 3}\end{array}\right] 
                % &=& \left[\begin{array}{rrr} \quad &\quad & \quad \\ \quad &\quad & \quad \\ \quad &\quad & \quad \end{array}\right] \\       
 % \left[\begin{array}{rrr}3& 0& 0\\ 0& 1& 0\\ 0& 0& 1\end{array}\right]
                         % \left[\begin{array}{rrr} a_{1 1}& a_{1 2} & a_{1 3}\\a_{2 1}& a_{2 2} & a_{2 3}\\a_{3 1}& a_{3 2} & a_{3 3}\end{array}\right]
                % &=& \left[\begin{array}{rrr} \quad &\quad & \quad \\ \quad &\quad & \quad \\ \quad &\quad & \quad \end{array}\right] \\  
% \end{eqnarray*}                    

% \end{frame}








  % \begin{frame}[fragile]
% \textbf{Exercise}
% For a matrix having 4 rows, write down the elementary matrices which perform the following elementary row operations.
% \begin{itemize}
 % \item $R_1\leftrightarrow R_3$

 % \item $3R_2$

 % \item $R_2 + 7R_4$

% \end{itemize}



% \textbf{Exercise}
% Write down the inverse for each of the elementary matrices above.

% \end{frame}






  % \begin{frame}[fragile]
% \textbf{Note}
% \begin{itemize}
% \item If $I_n \xrightarrow{ \mathcal R } E$, then  for any matrix $A$ with $n$ rows, $A \xrightarrow{ \mathcal R } EA$.  
% \item So, if $A$ can be row reduced to $B$ by a sequence of row operations $\mathcal R_1, \mathcal R_2, \dots, \mathcal R_k$ and $I_n\xrightarrow{\mathcal R_i} E_i$, then 
          % $B=E_k E_{k-1} \cdots E_2 E_1A$.    \\
          % (i.e. $A\xrightarrow{\mathcal R_1} E_1A \xrightarrow{\mathcal R_2} E_2(E_1A) \xrightarrow{\mathcal R_3} E_3E_2E_1A \rightarrow\cdots\xrightarrow{\mathcal R_k} E_kE_{k-1} \cdots E_2 E_1A=B$).     
% \item  Since each row operation is invertible, each elementary matrix is invertible.  
% \end{itemize}

% \end{frame}






  % \begin{frame}[fragile]
% \textbf{Theorem}
% An $n \times n$ matrix $A$ is invertible if and only if $A \sim I_n$, in which case
% the sequence of elementary row operations which transform $A$ to the identity also
% transform the identity matrix $I_n$ to $A^{-1}$. 



% \textbf{Note}
% Thus if $A\xrightarrow{\mathcal R_1} \xrightarrow{\mathcal R_2} \cdots \xrightarrow{\mathcal R_k} I_n$ then   \\ 
% $[A : I_n] \xrightarrow{\mathcal R_1} \xrightarrow{\mathcal R_2} \cdots \xrightarrow{\mathcal R_k} [I_n : A^{-1}]$.

% \end{frame}





  % \begin{frame}[fragile]
% \textbf{Proof}
% Recall that an $n \times n$ matrix $A$ is invertible if and only if 
% every equation $A\vec{x}=\vec{b}$ has a unique solution.  \\ 
% This is true if and only if the row reduced echelon form of $A$ has a pivot in every row (existence of solution) and 
% column (uniqueness of solution).  \\ 
% Thus $A$ is invertible if and only if the row reduced echelon form of $A$ is $I_n$.  \\ 

% Now suppose that $A$ is invertible and that $A\xrightarrow{\mathcal R_1} \xrightarrow{\mathcal R_2} \cdots \xrightarrow{\mathcal R_k} I_n$.  \\ 
% Suppose also that $I_n\xrightarrow{\mathcal R_i}E_i$.  \\ 
% Then $A\xrightarrow{\mathcal R_1}E_1A\xrightarrow{\mathcal R_2}E_2E_1A\rightarrow\cdots\xrightarrow{\mathcal R_k} E_k\cdots E_1A=I_n$. \\ 
% Thus $A= E_1^{-1}E_2^{-1}\cdots E_k^{-1} \Rightarrow A^{-1}= E_k\cdots E_1$.

% \end{frame}






  % \begin{frame}[fragile]
% \textbf{Example}
% Let $A = \begin{bmatrix} 1 & 2 & -1 \\ 2& 3  &1 \\ 3 & 5 & 1 \end{bmatrix}$. 
% Find $A^{-1}$.

% \end{frame}

% %
% %  \begin{frame}[fragile]
% %
% %Let $A$ be an $n \times n$ matrix.  The following are equivalent.
% %\begin{itemize}
% % \item $A$ is invertible.
% %\item $A \sim I_n$.
% %\item $A$ has $n$ pivots.
% %\item $A\textbf{x}= \textbf{0}$ has only the trivial solution.
% %\item The columns of $A$ are linearly independent.
% %\item The linear transformation $T: \textbf{x} \longmapsto A \textbf{x}$ is 1-1.
% %\item For every $\textbf{b} \in \mathbb R^n$, the equation
% %$A\textbf{x}= \textbf{b}$ has at least one solution.
% %\item The columns of $A$ span $\mathbb R^n$
% %\item The linear transformation $T: \textbf{x} \longmapsto A \textbf{x}$ is onto.
% %\item $\exists C$ so that $CA = I_n$.
% %\item $\exists D$ so that $AD = I_n$.
% %\item $A^T$ is invertible.
% %\end{itemize}
% %
% %
% %\end{frame}
% %
% %
% %
% %
% %
% %  \begin{frame}[fragile]
% %\textbf{Proof} To prove a number of statements are equivalent, it is often easiest to 
% %show a chain of beginning and ending at one of the statements.   Here, it is easiest
% % to do the following chains:
% %\[
% % (1) \implies (10) \implies (4) \implies (3) \implies (2) \implies (1)
% %\]
% %\[
% % (1) \implies (11) \implies (7) \implies (1)
% %\]
% %\[
% % (7) \iff (8) \iff (9)
% %\]
% %\[
% % (4) \iff (5) \iff (6)
% %\]
% %\[
% % (1) \iff (12)
% %\]
% %
% %\end{frame}






  % \begin{frame}[fragile]
 
% \textbf{Example}
 % Is the matrix $A=\left[\begin{array}{rrr} 0 & 7 & 1 \\ 1 & 2 & 4 \\ 2& 3& 6 \end{array}\right]$ invertible?

% \end{frame}







  % \begin{frame}[fragile]
% \textbf{Theorem}
 % Let $T: \mathbb R^n \longrightarrow \mathbb R^n$ be a linear transformation, and let 
% $A$ be the corresponding matrix. Then $T$ is invertible if and only if 
% $A$ is invertible, in which case $T^{-1}\textbf{x}=A^{-1}\textbf{x}$.

% \end{frame}





  % \begin{frame}[fragile]
% \textbf{Exercise}
% Perform row reduction on the following matrices in order to characterize invertibility.
% \begin{itemize}
 % \item $\left[\begin{array}{rr} a& b \\ c& d\end{array}\right]$

 
 % \item $\left[\begin{array}{rrr} a & b& c \\ d& e& f\\  g& h& i \end{array}\right]$

% \end{itemize}

% \end{frame}





  % \begin{frame}[fragile]
 
% \textbf{Definition}
 % Suppose that $A$ is a $3\times 3$ matrix as in the previous exercise.  The determinant of $A$ is defined by 
 % $$\Delta(A)= aei + bfg + cdh - ceg - afh - dbi.$$



% \textbf{Facts}
 % $A$ is invertible if and only if $\Delta(A)\neq 0$.


% \textbf{Note}
 % Suppose that $A$ is a $3\times 3$ matrix as before.  Then,
 % $$\Delta(A) = a\cdot \det\left[\begin{array}{rr} e&f\\h&i\end{array}\right]
                          % - b\cdot \det\left[\begin{array}{rr} d&g\\f&i\end{array}\right]
                          % + c\cdot \det\left[\begin{array}{rr} d&e\\g&h\end{array}\right].
  % $$

% \end{frame}





  % \begin{frame}[fragile]
 
% \textbf{Definition}
 % We define the $ij^{\mbox{th}}$ minor of and $n\times n$ matrix $A$ as
 % $$
 % A_{ij} = \left[\begin{array}{cccccc} 
                                   % a_{1,1}  & \dots & a_{1,j-1}  & a_{1,j+1} & \dots & a_{1,n} \\
                                   % a_{2,1}  & \dots & a_{2,j-1}  & a_{2,j+1} & \dots & a_{2,n} \\
                                   % \vdots    &            &   \vdots   & \vdots     &    &\vdots \\
                                   % a_{i-1,1}  & \dots & a_{i-1,j-1}  & a_{i-1,j+1} & \dots & a_{i-1,n} \\
                                   % a_{i+1,1}  & \dots & a_{i+1,j-1}  & a_{i+1,j+1} & \dots & a_{i+1,n} \\
                                    % \vdots    &            &   \vdots   & \vdots     &    &\vdots \\
                                    % a_{n,1}  & \dots & a_{n,j-1}  & a_{n,j+1} & \dots & a_{n,n} \\
                % \end{array}\right]
 % $$



% \textbf{Definition}
 % Suppose that $A$ is an $n\times n$ matrix.  Then, 
 % $$\det(A) =\sum_{j=1}^n (-1)^{1+j} a_{1j} \cdot \det(A_{1j}). $$

% \end{frame}





  % \begin{frame}[fragile]
 
% \textbf{Example}
 % Compute the $\det(A)$ where $A=\left[\begin{array}{rrrr} 
                                                                   % 1&0&2&0 \\
                                                                   % 2&0&3&0 \\
                                                                   % 1&2&0&3\\
                                                                   % 1&1&1&1
                                                              % \end{array}\right].$

% \end{frame}





  % \begin{frame}[fragile]
 
% \textbf{Definition}
  % We define the $(i,j)^{\mbox{th}}$ cofactor of $A$ to be
  % $$C_{i j} = (-1)^{i+j} \det(A_{ij}).$$



% \textbf{Theorem}
 % Suppose that $A$ is an $n\times n$ matrix.  Then we can compute the  determinant of $A$ by expanding by cofactors along any row or column of $A$.  That, is,
% \begin{itemize}
 % \item $\det(A) = \sum_{j=1}^n a_{ij}C_{ij}  \qquad \mbox{$i$ is fixed}$.
 % \item $\det(A) = \sum_{i=1}^n a_{ij}C_{ij}  \qquad \mbox{$j$ is fixed}$.
% \end{itemize}

% \end{frame}





  % \begin{frame}[fragile]
 
% \textbf{Example}
 % Compute $\det(A)$ where $A=\left[\begin{array}{rrr} 
                                                 % 1&-1&0 \\
                                                 % 2&2&0\\
                                                 % 3&3& 1
                                                 % \end{array}\right].$



% \textbf{Theorem}
 % If $A$ is triangular, then $\det(A) = a_{1 1} \cdot a_{2 2} \cdot \dots \cdot a_{n n}$.

% \end{frame}




  % \begin{frame}[fragile]
 
% \textbf{Theorem}
 % Suppose that $A$ and $B$ are $n\times n$ matrices and that $B$ is obtained from $A$ by performing a single elementary row operation $\mathcal R$. 
 
% \begin{itemize}
 % \item If $\mathcal R$ is $R_i\leftrightarrow R_j$ then $\det(A)= -\det(B)$.
 % \item If $\mathcal R$ is $R_i + cR_j$ then $\det(A)= \det(B)$.
 % \item If $\mathcal R$ is $ cR_i$ then $\det(A)= \frac{1}{c} \det(B)$.
% \end{itemize}



% \textbf{Example}
 % Compute $\det(A)$ where $A=\left[ \begin{array}{rrr} 1&-4&2\\ -2& 8&-9 \\ -1&7&0\end{array}\right].$

% \end{frame}






  % \begin{frame}[fragile]
% \textbf{Corollary}
% \begin{itemize}
 % \item $\det\left[E_{R_i\leftrightarrow R_j}\right] = -1$.
 % \item $\det\left[E_{R_i + cR_j}\right] = 1$.
 % \item $\det\left[E_{cR_j}\right] = c$.
 % \item If $A\xrightarrow{\mathcal R} B$, then $\det(B)=\det(E_{\mathcal R})\det(A)$.
% \end{itemize}



% \textbf{Note}
 % Given an $n\times n$ matrix $A$, we can obtain a row echelon form of $A$, using only row replacement and row interchange operations.

% \end{frame}






  % \begin{frame}[fragile]
% \textbf{Corollary}
 % Suppose that $A$ is an $n\times n$ matrix and that $U$ is an echelon form of $A$ obtained through the use of row replacements and row interchanges only.  Let $r$ denote the number of row interchanges used to obtain $U$.  Then,
% \begin{eqnarray*}
 % \det(A) &=& (-1)^r\cdot u_{11}\cdot u_{22} \cdot \dots \cdot u_{nn} \\ 
            % &=& \begin{cases}
                      % (-1)^r *\mbox{ product of pivots in $U$ } & \mbox{if $A$ is invertible,} \\
                      % 0 &\mbox{if $A$ in singular.} 
                 % \end{cases}
% \end{eqnarray*}



% \textbf{Corollary}
 % Suppose that $A$ is an $n\times n$ matrix.  Then $A$ is invertible if and only if $\det(A)\neq 0$.

% \end{frame}





  % \begin{frame}[fragile]
 
% \textbf{Corollary}
  % If $A$ has two rows or two columns the same, then $\det(A)=0$ and $A$ is singular.


% %
% %\textbf{Proof}
% % \ \\
% % \ \\
% %


% \textbf{Note}
% When computing $\det(A)$, we can exploit the presence of many zeros in $A$.



% \textbf{Exercise}
 % Compute $\det(A)$ where $A=\left[\begin{array}{rrrr} 0&0&0&1 \\ 0&1&2&0\\ 0&8&-1&5\\ 1&2&3&4\end{array}\right]$.

% \end{frame}






  % \begin{frame}[fragile]
 
% \textbf{Theorem}
 % Suppose that $A$ is an $n\times n$ matrix.  Then $A$ and its transpose have the same determinant 
 % (-i.e.  $\det(A^T) = \det(A)$).



% \textbf{Theorem}
 % Suppose that $A$ and $B$ are $n\times n$ matrices.  Then 
 % $$ \det(AB)=\det(A)\det(B).$$



% \textbf{Example}
 % Compute $\det(A)$, $\det(B)$ and $\det(AB)$ where 
 % $A=\left[\begin{array}{rr} 1&2 \\ 3&1\end{array}\right]$ and 
 % $B= \left[\begin{array}{rr} 1&2 \\ -1&1\end{array}\right]$.

% \end{frame}




  % \begin{frame}[fragile]
 
% \textbf{Definition}
 % Suppose that $A=[\vec{a_1}, \vec{a_2}, \dots, \vec{a_n}]$ is an $n\times n$ matrix.  
 % For any $\vec{b}\in \mathbb R^n$, we define 
 % $$A_i(\vec{b}) = [\vec{a_1},\dots,\vec{a_{i-1}},\vec{b}, \vec{a_{i+1}}, \dots, \vec{a_n}].$$



% \textbf{Theorem}[Cramer's Rule]
 % Suppose that $A$ is an $n\times n$ invertible matrix.  For any $\vec{b}\in \mathbb R^n$, the unique solution to $A\vec{x}=\vec{b}$ has entries given by 
 % $$
 % x_i =\frac{\det(A_i(\vec{b}))}{\det(A)}.
 % $$

% \end{frame}






  % \begin{frame}[fragile]
 
% \textbf{Proof}
 % We have 
 
% \begin{eqnarray*}
 % A\cdot I_i(\vec{x}) &=&  \    [A\vec{e_1}, A\vec{e_2}, \dots A\vec{e_{i-1}}, A\vec{x}, A\vec{e_{i+1}}, \dots, A\vec{e_n}] \\
          % &=&  \  [\vec{a_1},\dots, \vec{a_{i-1}}, \vec{b}, \vec{a_{i+1}}, \dots , \vec{a_n}] \\
          % &=&  \  A_i(\vec{b}).  
% \end{eqnarray*}
 
 
 % So, we have $\det(A)\det(I_i(\vec{x})) = \det(A_i(\vec{b}))$.   \\ 
 % Since $A$ is invertible, we may write $\det( I_i(\vec{x}) ) = \frac{ \det(A_i(\vec{b}))}{\det(A)}$. \\ 
 % The theorem follows from noticing that $\det( I_i(\vec{x}) ) = x_i$.  \\ 
 % To see this, compute $\det( I_i(\vec{x}) )$ by expanding by cofactors along the $i^{\mbox{\rm th}}$ row.

% \end{frame}






  % \begin{frame}[fragile]
 
% \textbf{Example}
 % Use Cramer's rule to solve $A\vec{x}=\vec{b}$ where $A=\left[\begin{array}{rr}2&-1\\3&4\end{array}\right]$ and $\vec{b}=\left[\begin{array}{r}3\\ 43\end{array}\right]$.



% \textbf{Example}
 % Consider the linear system
 % $$
 % \left\{
  % \begin{array}{rrrrr}
    % 4sx_1 &+&2x_2 &=& 1 \\
    % 5x_1   &+&x_2 & = &-1 
  % \end{array} \right.
 % $$
 % For which $s$ is there a unique solution.  For such $s$ describe the solution.

% \end{frame}






  % \begin{frame}[fragile]
% \textbf{Definition}
 % Suppose that $A$ is an $n\times n$ matrix.  We define the $n\times n$ adjoint of $A$ as 
% \begin{eqnarray*}
 % Adj(A) &=& \left[\begin{array}{cccc}C_{11}&C_{21}& \dots & C_{n1} \\
                         % C_{12}& C_{22}& \dots& C_{n2} \\
                         % \vdots & \vdots & \ddots  & \vdots \\
                         % C_{1n} & C_{2n} & \dots & C_{nn} \
                         % \end{array}\right]    \\  
                         % &=&  \left[\begin{array}{cccc}C_{11}&C_{12}& \dots & C_{1n} \\
                         % C_{21}& C_{22}& \dots& C_{2n} \\
                         % \vdots & \vdots & \ddots  & \vdots \\
                         % C_{n1} & C_{n2} & \dots & C_{nn} \
                         % \end{array}\right]^T,
% \end{eqnarray*}
% where $C_{ij}=(-1)^{i+j}\det(A_{ij})$.

% \end{frame}






  % \begin{frame}[fragile]
 
% \textbf{Theorem}
 % Suppose that $A$ is an invertible $n\times n$ matrix.  Then,
 % $$
 % A^{-1}= \frac{1}{\det(A)}Adj(A).
 % $$


% % 
% %\textbf{Proof}
% % \ \\
% % \ \\
% %


% \textbf{Note}
 % If $A=\left[\begin{array}{rr} a&b\\c&d\end{array}\right]$, then $Adj(A)=\left[\begin{array}{rr} d &-b\\-c&a\end{array}\right]$.

% \end{frame}





  % \begin{frame}[fragile]
% \textbf{Exercise}
  % Compute $Adj(A)$ and $A^{-1}$ where $A= \left[\begin{array}{rrr}1&0&0\\2&3&0\\3&2&1\end{array}\right]$.

% \end{frame}





  % \begin{frame}[fragile]
% \textbf{Theorem}
% \begin{itemize}
 % \item If $A$ is a $2\times 2$ matrix, then the area of the parallelogram determined by its columns (-i.e. having vertices at $\vec{0}$ at at the columns of $A$) is $|\det(A)|$. 
 % \item If $A$ is a $3\times 3$ matrix, then the volume of the parallelepiped determined by its columns is $|\det(A)|$.
% \end{itemize}  



% \textbf{Theorem}
% \begin{itemize}
 % \item  Let $T:\mathbb R^2\rightarrow \mathbb R^2$ be a linear transformation with $2\times 2$ matrix $A$.  If $S$ is a parallelogram in $\mathbb R^2$, then $\mbox{Area}(T(S)) = |\det(A)|\mbox{Area}(S)$.
 % \item If $T:\mathbb R^3\rightarrow \mathbb R^3$, is a linear transformation and $S$ is a parallelepiped, then $\mbox{Vol}(T(S))= |\det(A)|\mbox{Vol}(S).$
% \end{itemize}



% \textbf{Note}
 % The result of theorem 10, holds for any region $S$ of $\mathbb R^2$ for $\mathbb R^3$.

% \end{frame}






  % \begin{frame}[fragile]
% \textbf{Exercise}
 % Suppose that $a,b\in \mathbb N$. Find the area bounded by the ellipse 
 % $$ \frac{x^2}{a^2} +\frac{y^2}{b^2} = 25.$$
 
% \end{frame}
