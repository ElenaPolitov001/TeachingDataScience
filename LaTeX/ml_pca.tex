%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large PCA}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{ Too much of anything is good for nothing!}
What happens when a data set has too many variables ? Here are few possible situations which you might come across:
\begin{itemize}
\item  You find that most of the variables are correlated.
\item  You lose patience and decide to run a model on whole data. This returns poor accuracy and you feel terrible.
\item  You become indecisive about what to do
\item  You start thinking of some strategic method to find few important variables
\end{itemize}

More number of variables means High Dimensionality.

\end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{}
% \begin{center}
% {\Large High Dimensionality}
% \end{center}
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Curse	of Dimensionality}
\begin{itemize}
\item  Data-sets typically high dimensional	
 \item Images of 20x20 bitmaps have 400 dimensions. Mega pixel images has far too much.
 \item In text, all words are features, say, $10^6$.
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Curse	of Dimensionality}
\begin{itemize}
 \item Machine Learning methods are statistical, and data sparse.
 \item As the Dimensionality grows, fewer observations per region, ie density.
 \item Actual/True content is in the very small subset of this high dimensional space.
 \item More dense the space, better for the algorithm.
\end{itemize}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{ Dimensionality Reduction Algorithms}
\begin{itemize}
\item  How'd  you  identify  highly  significant variable(s) out 1000 or 2000?
\item Feature selection: Select only relevant features (based on domain, correlation, etc)
\item Feature Extraction: Compose new features based on combination of all existing ones.
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{ Dimensionality Reduction Algorithms}
\begin{itemize}
\item Principal Components Analysis (PCA) is a Feature Extraction method.
\item Projects data from high-dimensional space into a lower-dimensional space
% \item Not necessarily interested in ``losing information''; rather eliminate some of the sparsity
\end{itemize}
\end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Feature Selection Methods}
% \begin{itemize}
% \item Manually via Data Analyst. Intuition about problem domain
% \item Systematic Approach: Try all possible combinations of feature subsets?. See which combination results in best model
% \item For n features, there are 2n possible combinations of subsets. Infeasible to try each of them
% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Feature Selection Methods}
% Feature Construction
% \begin{itemize}
% \item Example: combining two separate features (\# of full baths, \# of half baths) into one feature (``total baths'')
% \item Example: combining features (mass) and (volume) into one feature (density), where density = mass / volume
% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Feature Selection Methods}
% Feature Subset Selection
% \begin{itemize}
% \item Reducing number of features by only using a subset of features
% \item Losing information if we only consider a subset of features?
% \item By eliminating unnecessary features, we hope for a better model.
% \end{itemize}

% \end{frame}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Three Systematic Approaches}
% \begin{itemize}
% \item Embedded Approaches
	% \begin{itemize}
	% \item Algorithm specific
	% \item Occurs naturally as part of the data mining algorithm. Example: present in decision tree induction
	% \end{itemize}
% \item Filter Approaches
	% \begin{itemize}
	% \item Features are selected before the data mining algorithm is run
	% \item Example: (trying to eliminate redundant features) Look at pairwise correlation between variables
	% \end{itemize}
% \item Wrapper Approaches
	% \begin{itemize}
	% \item Data mining algorithm is a “black box” for finding best subset of features
	% \item Final model uses the specific subset that evaluates the best
	% \end{itemize}
% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Top-Down Wrapper}
% \begin{itemize}
% \item Assuming n number of features
% \item Start with no attributes
% \item Train classifier n times, each time with a different feature
% \begin{itemize}
% \item Each classifier only has a single predictor
% \item See which of the n classifiers performs the best
% \end{itemize}
% \item Add to the best classifier. Recursively use remaining attributes to find which attribute that improves performance the most. Keep including best attribute 
% \item Stopping criterion:  Stop if no improvement to classifier performance, or increase in classifier performance is less than some threshold
% \end{itemize}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Bottom-Up Wrapper}
% \begin{itemize}
% \item Assuming n number of features
% \item Start with all n attributes in model
% \item Create n models, each with a different predictor omitted.
% \begin{itemize}
% \item Each classifier has n-1 predictors
% \item See which of the n classifiers affects performance the least
% \item Throw that attribute out
% \end{itemize}
% \item Recursively find the attribute that affects performance the least
% \item Stopping criterion:  Stop if classifier performance begins to degrade
% \end{itemize}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{} 

% As we have started to see, the curse of dimensionality stops
% us from being able to fit arbitrarily complex models in high
% dimensional spaces.

% \textbf{Additive models} try to avoid this by fixing the structure
% of the learned models to limit interactions between the input variables.

% \textbf{Tree-based models} attempt to use the data itself to greedily
% learn which interactions are actually important.

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{} 

% Today we are going to look at another technique called
% \textbf{principal components} (PCs), or principal component
% analysis (PCA), a specific example of dimensionality
% reduction.

% Like trees, these use the data to find lower dimensional structures
% hidden in higher dimensional space. They differ from trees, however,
% because principal components use \textbf{only the predictor variables} (not
% the responses) and attempt to capture \textbf{global and linear structure},
% rather than local ones.

% \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Dimensionality Reduction Example}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Body Measurements}

Say that we have a data-set of the following measurements from
a large set of human volunteers with the following variables:
\begin{itemize}
\item height
\item weight
\item waist size
\item shoe size
\item length of right arm
\item length of left arm

\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Body Measurements}
\begin{itemize}
\item length of torso
% \item pant inseam length
\item hat size
\item left hand ring size
\item right hand ring size
\end{itemize}
Technically we have $10$ variables, though most of the variation
in the data-set can be summarized by at most 2-3 variables.

What can you do? Domain knowledge?
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Reduction in Dimensions} 
In decreasing order of variation, consider the following measurements
that can be derived from these $10$ variables
\begin{itemize}
\item height: captures a large amount of the variation in the total
dataset. 
\item body mass index: should be
relatively uncorrelated with overall height, captures much of the
next largest variation in the data. 
\item ratio of torso length to total height: attempts
to capture the remaining variation based on how height is distributed
over a given individuals frame.
\end{itemize}

This is dimension reduction is based on domain knowledge, but is there anything generic? Yes, PCA (Principal Component Analysis).

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large PCA Algorithm}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{PCA} 
\begin{itemize}
\item One of the oldest (1901!) 
\item To understand ``important'' dimensions
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{PCA} 
\begin{itemize}
\item PCA looks at the data and tries to find a direction-line along which data is spread the most.
\item Once done, you look for another (perpendicular) dimension which has second most variation.
\item These are called as Principal components.
\item So, if we TRANSFORM the data to the coordinate system formed by Principal Components, then as most of the variations are captured, a few axes capture most and thats the Dimension Reduction.
\end{itemize}
\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile] \frametitle{PCA} 
%\begin{itemize}
%\item Each principal component, however, must be a linear function of the
%input variables (so BMI would not be allowed). 
%\item They all the data points are re-coded in terms of new basis vectors. A new coordinate system.
%\end{itemize}
%\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{PCA in a nutshell} 
\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{pca6}
\end{center}
\begin{itemize}
\item Orthogonal directions of greatest variance in data
\item Projections along PC1 discriminate the data most along any one axis
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{PCA} 
\begin{itemize}
\item  First principal component is the direction of greatest
variability (co-variance) in the data
\item Second is the next orthogonal (uncorrelated) direction of greatest variability. So first remove all the variability along the first
component, and then find the next direction of greatest variability 
\item  And so on
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{PCA} 
 Principle
\begin{itemize}
\item   Linear projection method to reduce the number of parameters
\item   Transfer a set of correlated variables into a new set of uncorrelated variables
\item   Map the data into a space of lower Dimensionality
\item   Form of unsupervised learning
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile] \frametitle{PCA} 
% Properties
%\begin{itemize}
%\item   It can be viewed as a rotation of the existing axes to new positions in the 
%space defined by original variables
%\item   New axes are orthogonal and represent the directions with maximum 
%variability
%\end{itemize}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Why greatest variability?	} 
\adjustbox{valign=t}{
\begin{minipage}{0.5\linewidth}
\begin{itemize}
\item Example: reduce 2-dimensional data to 1
\item Data points in 2D $x_1,x_2$ space are represented by projection points on $z$ axis, so, only 1-D distances.
\item See how two red points on different z axes are ad different distances.
\item The one with max variance preserves original intent-structure.
%\item Variance = $ 1/n \sum (\sum projections - mean)^2$
\item So, blue Z is better!!
\end{itemize}

\end{minipage}
}
\hfill
\adjustbox{valign=t}{
\begin{minipage}{0.4\linewidth}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pca4}
\end{center}
%Remember Eigen values and vectors $| A - \lambda I | = 0$
\end{minipage}
}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile] \frametitle{PCA} 
%\begin{itemize}
%\item    Finding $u$ such that $E(u.x)^2)$ is max for all $x$
%\item $E((u.x)^2) = E((u.x)(u.x)^T) = E(u.x.x^T.u^T)$
%\item Matrix $S = x.x^T$ contains the co-variances among pairs of x features, in the original coordinates system
%\item So we are looking to maximize $u.S.u^T$
%\end{itemize}
%\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large PCA in Steps}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{PCA in Steps} 
\begin{itemize}

\item Standardize the data.
\item Compute Co-variance matrix.
\item Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Value Decomposition.
\item Sort eigenvalues in descending order and choose the $k$
 eigenvectors that correspond to the $k$ largest eigenvalues where $k$
 is the number of dimensions of the new feature subspace ($k \leq d$).
\item Construct the projection matrix $W$ from the selected $k$  eigenvectors.
\item Transform the original dataset $X$ via $W$ to obtain a $k$dimensional feature subspace $X'$.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Standardizations} 
\begin{itemize}
\item Center the data by subtracting mean.
\item Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable.

$z = \frac{value - mean}{standard deviation}$
\item Once the standardization is done, all the variables will be transformed to the same scale.
\item Subtracting the mean makes variance and covariance 
calculation easier by simplifying their equations. 
\item The 
variance and co-variance values are not affected by 
the mean value.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Covariance Matrix Computation} 
\begin{itemize}
\item To see if there is any relationship between them.
\item Variables are highly correlated in such a way that they contain redundant information.
\item The covariance matrix is a $p \times p$ symmetric matrix (where $p$ is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables
\item For example, for a 3-dimensional data set with 3 variables $x$, $y$, and $z$, the covariance matrix is a $3 \times 3$ matrix of this from:

\begin{center}
\includegraphics[width=0.4\linewidth,keepaspectratio]{pca22}
\end{center}
\item Diagonal is same variable, and matrix is symmetric.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Whats Eigen Values/Vectors?} 
\begin{itemize}
\item Example: We are provided with 2-dimensional vectors $v_1, v_2, \ldots, v_n$. 
\item Then, if we apply a linear transformation $T$ (a 2x2 matrix) to our vectors, we will obtain new vectors, called $b_1, b_2,\ldots,b_n$.
\item $b_1, b_2,\ldots,b_n$ are obviously different in values and direction from $v_1, v_2, \ldots, v_n$ as we are transforming them (remember homogeneous transformation matrix?)
\item But, there are special vectors, once applied the transformation $T$, they change length but not direction. 
\item Those vectors are called eigenvectors, and the scalar which represents the multiple of the eigenvector is called eigenvalue.
$Tv_i=\lambda v_i$
\item Thus, each eigenvector has a correspondent eigenvalue.
\item  if we consider our co-variance matrix $C$ and collect all the corresponding eigenvectors into a matrix $V$ (where the number of columns, which are the eigenvectors, will be equal to the number of rows of $C$), we will obtain something like that: $CV = LV$, where L is a vector where all the eigenvalues (as many as the eigenvectors) are stored
\end{itemize}

\begin{center}
\includegraphics[width=0.4\linewidth,keepaspectratio]{pca23}
\end{center}

{\tiny (Ref: PCA: Eigenvectors and Eigenvalues - Valentina Alto)}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Detour: How to Compute Eigen values and vectors}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
 \begin{itemize}
\item Let $A$ be an $n \times n$ matrix.  A {\em non-zero} vector $\vec{x}\in \mathbb{R}^n$ is 
called an \underline{eigenvector} of $A$ if there exists some scalar $\lambda \in \mathbb{R}$ so that 
$A \vec{x}= \lambda \vec{x}$.  \\ 
\item If $\vec{x}$ is an eigenvector of $A$,
the corresponding value $\lambda$ is called an \underline{eigenvalue} of $A$, and we say that 
$\lambda$ is an eigenvalue of $A$ with eigenvector $\vec{x}$.  

\item 
While an eigenvector $\vec{x}$ must be non-zero (so that we are always 
excluding the trivial case $A \vec{0}=\vec{0}$), it is possible for the value
$\lambda$ to be zero. 
\item  If $\lambda$ is an eigenvalue for $A$, the eigenvectors for $A$ corresponding to $\lambda$ along with $\vec{0}$ form a subspace of $\mathbb{R}^n$.
\end{itemize}


% \begin{definition}
% $\left( \begin{array}{rrrrr} 1 & 2 \\ 2 & 4 \end{array} \right)$
% has eigenvector $\left( \begin{array}{rrrrr}2 \\-1 \end{array} \right)$ with eigenvalue $0$.
% \end{definition}


 \begin{definition}
 \[
 \left( \begin{array}{rrrrr} 
  1 & 6 \\ 5 & 2
 \end{array} \right)
 \left( \begin{array}{rrrrr} 
  6 \\-5 
 \end{array} \right)
=
 \left( \begin{array}{rrrrr} 
  -24 \\ 20
 \end{array} \right)
=
-4 \left( \begin{array}{rrrrr} 
  6 \\ -5
 \end{array} \right).
\]


Thus we see that $\left( \begin{array}{rrrrr} 
  6 \\ -5
 \end{array} \right)$ is an eigenvector of this matrix, and $-4$ is the corresponding eigenvalue.

 \end{definition}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Derivation}
% \begin{definition}
 % Show that 7 is an eigenvalue for 
 % \[
 % A = \left( \begin{array}{rrrrr} 2 & 4 \\ 5 & 3 \end{array} \right).
% \]
% \end{definition}


\begin{definition}
For any scalar $\lambda$,
\begin{eqnarray*}
 A \vec{x} &=& \lambda \vec{x}   \\
 &\Leftrightarrow&  
A \vec{x} -\lambda \vec{x} = \vec{0}  \\
 &\Leftrightarrow&  
 A \vec{x} -\lambda I \vec{x} = \vec{0}  \\
  &\Leftrightarrow& 
  \left( A - \lambda I \right) \vec{x} =\vec{0} \\
   &\Leftrightarrow&  
\vec{x} \in (A -\lambda I)
\end{eqnarray*}
\end{definition}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Derivation}
\begin{theorem}
 A scalar $\lambda$ is an eigenvalue of an $n\times n$ matrix $A$ if and only if 
$\lambda$ satisfies the \underline{characteristic equation} 
\[
 \det(A -\lambda I) = 0.
\]
\end{theorem}


\begin{theorem}
$\det(A-\lambda I)$ is a polynomial in $\lambda$ of degree $n$.   \\ 
Hence, an $n\times n$ matrix has at most $n$ eigenvalues. 
\end{theorem}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Example}

\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{pca24}
\end{center}

{\tiny (Ref: Eigenvector and Eigenvalue - Maths is fun)}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Example}

\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{pca25}
\end{center}

{\tiny (Ref: Eigenvector and Eigenvalue - Maths is fun)}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Example}

\begin{center}
\includegraphics[width=0.5\linewidth,keepaspectratio]{pca26}
\end{center}

Now it is your turn to find the eigenvector for the other eigenvalue of -7

{\tiny (Ref: Eigenvector and Eigenvalue - Maths is fun)}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Why?}
What is the purpose of these?

\begin{itemize}
\item Wrt, transformation matrices, eigenvector is ``the direction that doesn't change direction'' !
\item And the eigenvalue is the scale of the stretch:
\begin{itemize}
\item 1 means no change,
\item 2 means doubling in length,
\item -1 means pointing backwards along the eigenvalue's direction
\item etc
\end{itemize}
\end{itemize}

{\tiny (Ref: Eigenvector and Eigenvalue - Maths is fun)}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Why `Eigen'?}

\begin{itemize}
\item Eigen is a German word meaning "own" or "typical": "das ist ihnen eigen" is German for "that is typical of them"
\item Sometimes in English we use the word "characteristic", so an eigenvector can be called a "characteristic vector".
\end{itemize}

{\tiny (Ref: Eigenvector and Eigenvalue - Maths is fun)}
\end{frame}



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Derivation}
% \begin{definition}
% \begin{enumerate}
% \item  The degree $n$ polynomial $\det(A-\lambda I)$ is called the \underline{characteristic polynomial}.  
% \item  The \underline{(algebraic) multiplicity} of the eigenvalue $\lambda_0$ is the power of $(\lambda-\lambda_0)$ appearing in the factorization of the characteristic polynomial.
% \end{enumerate}
% \end{definition}


% \begin{example}
% Find the eigenvalues and their multiplicities of 
% \[A = \left( \begin{array}{rrrrr} 
 % 6 & 5 & 0 & -5 \\ 0 & -3 & 1 & 2 \\ 0 & 0 & 6 & 3 \\ 0& 0 & 0 & 7     
     % \end{array} \right).
% \]
% \end{example}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Derivation}
% \begin{definition}
 % Two $n\times n$ matrices $A$ and $B$ are similar if there is an invertible matrix
% $P$ so that $P^{-1}AP=B$, or equivalently $A=PBP^{-1}$.   
% \end{definition}  

% \begin{theorem}
 % If $n\times n$ matrices $A$ and $B$ are similar, then they have the same characteristic
% polynomials.
% \end{theorem}  
%
%{\bf Proof:}  If $A=PBP^{-1}$ then 
%\begin{eqnarray*}
% \det(A-\lambda I) &=& \det( PBP^{-1} -\lambda I)= \det(PBP^{-1} -\lambda PIP^{-1})\\
%&=& \det(P(B-\lambda I)(P^{-1}) = \det(P)\det(B-\lambda I) \det(P^{-1})\\
%&=& \det(P)\det(P^{-1}) \det(B-\lambda I) = \det(B-\lambda I)
%\end{eqnarray*}
%as claimed.


% \begin{example}
% Let $A=\left( \begin{array}{rr} .95& .03 \\ .05& .97\end{array}\right)$.  \\ 
% Take $x_0=\left(\begin{array}{r} .6\\ .4\end{array}\right)$, and $x_{k+1}=Ax_k$ for $k\ge 0$.  \\ 
% Analyze the long term behavior of this sequence.
% \end{example}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{}
% % \Nul(A - \lambda I)
% \begin{definition}
 % If $\dim(A - \lambda I)>0$, then $(A-\lambda I)$ is called the \underline{eigenspace } for 
% $A$ corresponding to the eigenvalue $\lambda$, since any 
% $\vec{x}\in (A-\lambda I) $ satisfies $A\vec{x} =  \lambda \vec{x}$.
% \end{definition}


% \begin{definition}
 % Find a basis for the eigenspace corresponding to the eigenvector $2$ for the 
% matrix
% \[
 % A = \left( \begin{array}{rrrrr}
      % 4 & -1 & 6 \\
      % 2 & 1 & 6 \\
      % 2 & -1 & 8 
     % \end{array} \right).
% \]
% \end{definition}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{}
 % \begin{theorem}
 % The eigenvalues of an upper triangular matrix (or of a lower triangular matrix) are its
% diagonal entries. 
% \end{theorem}


% \begin{block}{Invertible Matrix Theorem}
 % \begin{itemize}
 % \item[] The number 0 is an eigenvalue of $A$   
% \item[] $\iff$ \pause there exists $\vec{x} \neq \vec{0}$ so that 
% $A \vec{x} = 0 \vec{x}$  
% \item[] $\iff$  \pause there exists $\vec{x} \neq \vec{0}$ so that 
% $A \vec{x} - 0 \vec{x} = \vec{0}$.
% \item[] $\iff$ \pause There exists $\vec{x} \neq \vec{0} \in (A)$.
% \item[] $\iff$ \pause $A$ is not invertible. 
% \end{itemize}
% \end{block}


% \begin{theorem}
 % If $\vec{v}_1, \dots, \vec{v}_r$ are eigenvectors corresponding to 
% distinct eigenvalues $\lambda_1, \dots, \lambda_r$ of an $n\times n$ matrix $A$,
% then the set $\{\vec{v}_1, \dots,\vec{v}_r\}$ is linearly independent.
% \end{theorem}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{}
% \begin{definition}
% In many applications, one is interested in studying repeated application of a linear map.  \\ \pause
% Suppose we would like to study the long term behavior of a sequence $\{\vec{x}_k\}$ satisfying $\vec{x}_{k+1}=A\vec{x}_k$. \\ \pause
% Describe the long term behavior of such a sequence where $\vec{x}_0$ is an eigenvector of $A$ with eigenvalue $\lambda$. \\ \pause
% \end{definition}
% \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Back to PCA}
\end{center}
\end{frame}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Practical Scenario for PCA} 

If we want to reduce the dimensionality of our dataset? Imagine we start with 5 features and we want to handle 2 features instead. So, the procedure will be the following:

\begin{itemize}
\item Computing the $C$ matrix our data, which will be $5x5$
\item Computing the matrix of Eigenvectors and the corresponding Eigenvalues
\item Sorting our Eigenvectors in descending order
\item Building the so-called projection or transformation matrix $W$, where the $k$ eigenvectors we want to keep (in this case, 2 as the number of features we want to handle) will be stored. 
\item Hence, in our case, our $W$ will be a 5x2 matrix (in general, it is a $dxk$ matrix, where $d=$number of original features and $k=$number of desired features).
\item Transforming the original data, which can be represented as an $nxd$ matrix $X$ (where $n=$number of observations and $d=$number of features), via the projection matrix $W$, obtaining a new dataset or matrix $X'$ which will be $nxk$.
\end{itemize}

{\tiny (Ref: PCA: Eigenvectors and Eigenvalues - Valentina Alto)}

\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{PCA in Steps} 
% \begin{center}
% \includegraphics[width=\linewidth,keepaspectratio]{pca19}
% \end{center}
% \tiny{(Reference: Principal Component Analysis - Victor Lavrenko)}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{PCA in Steps} 
% First way
% \begin{itemize}
% \item Initialize seed vector
% \item Successive multiplication with Co-variance matrix
% \item Transforms the vector 
% \item Finally, the Principal Components.
% \end{itemize}
% \begin{center}
% \includegraphics[width=\linewidth,keepaspectratio]{pca20}
% \end{center}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{PCA in Steps} 
% Another way
% \begin{itemize}
% \item Calculate the eigen vectors and eigenvalues of 
% the co-variance matrix (Manually, Python, Matlab, etc)
% \item Since the non-diagonal elements in this co-variance 
% matrix are positive, we should expect that both the x 
% and y variable increase together.
% \end{itemize}
% \begin{center}
% \includegraphics[width=\linewidth,keepaspectratio]{pca21}
% \end{center}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{PCA in Steps} 
% \begin{itemize}
% \item Retain top Principal components 
% \item Project original X data on them
% \end{itemize}
% \end{frame}

% % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % \begin{frame}[fragile]\frametitle{}
% % \begin{center}
% % {\Large PCA Graphically}
% % \end{center}
% % \end{frame}

% % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % \begin{frame}[fragile] \frametitle{Geometric view of PCA}
% % Goal:  Find variation in data with as less components as possible
% % \begin{center}
% % \includegraphics[width=0.6\linewidth,keepaspectratio]{pca10}
% % \end{center}
% % \end{frame}

% % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % \begin{frame}[fragile] \frametitle{Geometric view of PCA}
% % Try directions, pick the one giving max variance
% % \begin{center}
% % \includegraphics[width=0.6\linewidth,keepaspectratio]{pca11}
% % \end{center}
% % \end{frame}

% % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % \begin{frame}[fragile] \frametitle{Geometric view of PCA}
% % Again try directions, pick the next one giving max variance
% % \begin{center}
% % \includegraphics[width=0.6\linewidth,keepaspectratio]{pca12}
% % \end{center}
% % \end{frame}

% % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % \begin{frame}[fragile] \frametitle{Geometric view of PCA}
% % \begin{itemize}
% % \item 1st Direction: maximum variance
% % \item 2nd Direction: Perpendicular to first, with next max variance
% % \end{itemize}
% % \begin{center}
% % \includegraphics[width=0.8\linewidth,keepaspectratio]{pca13}
% % \end{center}
% % \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large PCA Example Working}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{PCA Example in Steps} 
Subtract the mean
\begin{itemize}
\item From each of the data dimensions. 
\item All the $x$ values have their $\bar{x}$ subtracted and $y$ values have $\bar{y}$ subtracted 
from them. 
\item This produces a data set whose mean is zero.
\item Subtracting the mean makes variance and covariance 
calculation easier by simplifying their equations. 
\item The 
variance and co-variance values are not affected by 
the mean value.
\end{itemize}
\end{frame}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{PCA Example in Steps} 
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pca8}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{PCA Example in Steps} 
Original Data
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pca14}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{PCA Example in Steps} 
\begin{itemize}
\item Calculate the co-variance matrix (Manually, Python, Matlab, etc)
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pca9}
\end{center}
\item Since the non-diagonal elements in this co-variance 
matrix are positive, we should expect that both the x 
and y variable increase together.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{PCA Example in Steps} 
\begin{itemize}
\item Calculate the eigen-vectors and eigenvalues of the co-variance  (Manually, Python, Matlab, etc)
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pca15}
\end{center}
\item Note they are perpendicular to each other
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{PCA Example in Steps} 
\begin{itemize}
\item Highest eigenvalue is the principle component.
\item Here, it is pointed down the middle of the data. 
\item Order eigen vectors by eigenvalue, highest to lowest. 
\item The order of significance. 
\item Keep important ones. 
\item Small eigenvalues ignorable
\end{itemize}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{PCA Example in Steps} 
% $NewData = RowFeatureVector_{nf \times nf} \times RowZeroMeanData_{nRows \times nf transposed}$
% \begin{itemize}
% \item RowFeatureVector: Matrix with eigen-vectors as rows, first on top
% \item RowZeroMeanData is the mean-adjusted data transposed
% \end{itemize}
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{PCA Example in Steps} 
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pca16}
\end{center}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{PCA in a nutshell} 
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{pca5}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile] \frametitle{PCA} 
%The image below shows the transformation of a high dimensional data (3 dimension) to low dimensional data (2 dimension) using PCA. Not to forget, each resultant dimension is a linear combination of p features
%\begin{center}
%\includegraphics[width=\linewidth,keepaspectratio]{pca2}
%\end{center}
%\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{PCA} 

\begin{itemize}
\item A principal component is a normalized linear combination of the original predictors in a data set.
\item The first principal component can be written as:
$Z^1 = w^{11}X^1 + w^{21}X^2 + \ldots +  w^{p1}X^p$
\item It captures the maximum variance in the data set. 
\item It determines the direction of highest variability in the data. 
\item Larger the variability captured in first component, larger the information captured by component. 
\item No other component can have variability higher than first principal component.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{PCA} 

\begin{itemize}
\item The first principal component results in a line which is closest to the data i.e. it minimizes the sum of squared distance between a data point and the line.
\item Similarly, we can compute the second principal component also.
$Z^2 = w^{12}X^1 + w^{22}X^2 + \ldots +  w^{p2}X^p$
\item If the two components are uncorrelated, their directions should be orthogonal
\begin{center}
\includegraphics[width=0.7\linewidth,keepaspectratio]{pca3}
\end{center}
\item All succeeding principal component capture the remaining variation without being correlated with the previous component. 
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{PCA} 
Can ignore the components of lesser significance.
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{pca7}
\end{center}
You do lose some information, but if the eigenvalues are small, you don't lose 
much.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile] \frametitle{Principal components} 
%
%Formally, the principal components of the matrix $X$ are a linear
%re-parametrization $T=wX$ of the matrix $X$. The first column of
%$T$ is the first principal component, the second column is the second
%principal component, and so on.
%
%\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile] \frametitle{Principal components} 
%
%Specifically, the matrix $w$ is defined uniquely by the following
%conditions:
%\begin{itemize}
%\item Each column of $T$ must be uncorrelated with the others;
%specifically, $w$ is an orthogonal matrix called the \textit{loadings}
%\item The first column of $T$ has the largest variance of all
%linear combinations of the columns of X, the second column has the
%highest variance conditioned on being uncorrelated with the
%first, and so forth.
%\end{itemize}
%
%\end{frame}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile] \frametitle{Mathematically} 
%
%\begin{itemize}
%\item 
%It can be shown that the matrix $w$ is equal to the eigenvectors
%of the matrix $X^tX$. 
%\item From this relationship, there are many
%results from numerical linear algebra that can be used to develop
%theoretical results about principal components.
%\end{itemize}
%\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
\frametitle{Principal Component Analysis}
Advantages
\begin{itemize}
	\item Useful Pre-processing step
	\item Reduces computational complexity
	\item Noise reduction
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
\frametitle{Principal Component Analysis}
Limitations
\begin{itemize}
	\item Linear manifold only
	\item 
Co-variance Matrix huge, finding eigen-vectors is slow
	\item 
SVD comes to rescue

\end{itemize}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{A look ahead} 

\begin{itemize}
\item The main shortcoming of principal components are that they only
capture global linear structures in the data. This tends to be
a larger problem for prediction than it is for visualization.

\item Figuring out how to get non-linear extensions of principal
components is a wide open problem in statistic and machine learning.
Some avenues of research include:
\begin{itemize}
\item locally linear embedding
\item factor models
\item diffusion maps
\item mixture models
\end{itemize}
\end{itemize}


\end{frame}

