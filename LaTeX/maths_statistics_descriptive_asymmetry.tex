
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Measure of Asymmetry}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Measures of Shape}	
\begin{itemize}
\item  To have a general idea of its shape, or distribution
\item Helps identifying which descriptive statistic to use
\item  Symmetrical or nonsymmetrical
\item Skewness.
\item Kurtosis.
\end{itemize}
\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{da18}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Symmetric}	
\begin{itemize}

\item  Uniform.
\item  Normal.
\item  Camel-back.
\item  Bow-tie shaped.
\end{itemize}
\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{da19}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Skewness}	
Measures the degree to which the values are symmetrically distributed about the center
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{skewness}
\end{center}
If the distribution of values is skewed, then the median is a better indicator of the middle, compare to the mean.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Skewness}	
For perfectly symmetrical distribution, like Normal Distribution (middle figure):
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{skewness}
\end{center}
\begin{itemize}
\item  Whats the mean?: the middle axis point
\item Whats the mode?: Highest frequency, top most point
\item Whats the median?: Half split of the curve is at the middle.
\end{itemize}
All Points/Axes are same.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Skewness}	
For skewed distribution (left and right figures):
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{skewness}
\end{center}
\begin{itemize}
\item Whats the mean?: towards tail, as most of the heavy (+ve or -ve) points are there
\item Whats the mode?: Highest frequency, top most point
\item Whats the median?: somewhere between these two
\end{itemize}
All Points/Axes are different. Sides of Mean and Mode can decide right/left skewness.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Pearson's Skewness Coefficient}	
Karl Pearson coefficient of Skewness 
$sk_p = \frac{3(\mu - median)}{\sigma}$

\begin{itemize}
\item  The direction of skewness is given by the sign.
\item The coefficient compares the sample distribution with a normal distribution. The larger the value, the larger the difference.
\item A value of zero means no skewness at all.
\item A large negative value means the distribution is negatively skewed.
\item A large positive value means the distribution is positively skewed.
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{3rd Moment Skewness Coefficient}	
$sk_t = \frac{\sum (x_i - \mu)^3}{\sigma^3}$

\begin{itemize}
\item If the power would have been 1 (instead of 3) then $\sum (x_i -\mu)$ would have been 0. +ve and -ve will cancel each other.
\item  Odd moments are increased when there is a long tail to the right and decreased when there is a long tail to the left. 
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Skewness}	
\begin{itemize}
\item  Zero indicates perfect symmetry
\item  Negative value implies left-skewed data
\item Positive value implies right-skewed data.
\end{itemize}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{da20}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Measure of Skewness}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Kurtosis}	
\begin{itemize}
\item  Measures the degree of flatness (or peakness)
\item Clustered around middle? More peak, more kurtosis value
\item If values spread evenly, flatted, less kurtosis value
\end{itemize}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{da21}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Kurtosis Skewness Coefficient}	
$sk_k = \frac{\sum (x_i - \mu)^4}{\sigma^4}$

\begin{itemize}
\item Since the exponent in the above is 4, the term in the summation will always be positive 
\item Moments of even order are increased when either tail is long. 
\item Kurtosis is a measure of outlier content. High if longer the tails so more the outliers.
\item The third and fourth moments are the smallest examples of these so are used for skewness and kurtosis measures.
\end{itemize}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Weighted Average}	
% \begin{center}
% \includegraphics[width=0.7\linewidth,keepaspectratio]{wtavrg}
% \end{center}
% \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Bi-variate Analysis}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Bi-variate Analysis}	
\begin{center}
\includegraphics[width=0.7\linewidth,keepaspectratio]{bivar}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}

\begin{center}
{\large Correlations and Covariance}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Covariance and Correlation}
Both show association between two variables
\begin{itemize}
\item Positive: If one goes up, the other does too and vice versa.
\item Example: Height and weight
\item Not always, but tendency
\item Another example: Temperature and Ice-creame sales
\item Negative: Temperature and sale of woolen clothes
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Correlation}
\begin{itemize}
\item Correlation is a value standardized between -1 to 1
\item Relation between two variables is linear, 
\item Directly proportional in case of Positive Corr
\item Inversely proportional in case of Negative Corr
\item The value of corr is the factor of proportionality
\item No correlation, ie no dependence so value = 0
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Covariance and Correlation}
\begin{center}
\includegraphics[width=0.55\linewidth,keepaspectratio]{corrplot}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Covariance}
Implement covariance, the paired analogue of variance.
The variance measures how a single variable deviates from its mean, covariance measures how two variables vary in tandem from their means.
\begin{center}
\includegraphics[width=0.5\linewidth,keepaspectratio]{corrcov}
\end{center}
\begin{lstlisting}
x = [2, 3, 0, 1, 3]
y = [ 2, 1, 0, 1, 2]
result1 = covariance(x,y)
result2 = correlation(x,y)print("CoVariance {}, Correlation {}".format(result1,result2))
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Covariance}
Covariance is like a dot product and tell how two quantities (centered, meaning subtracted by Mean) are together/similar.
\begin{lstlisting}
def elemwise_multi(v, w):
    """v_1 * w_1 + ... + v_n * w_n"""
    return sum(v_i * w_i for v_i, w_i in zip(v, w))

def covariance(x, y):
	n = len(x)
	return	elemwise_multi(de_mean(x), de_mean(y)) / (n - 1)

\end{lstlisting}
CoVariance 0.8
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Correlation}
Covariance is like a dot product normalized by standard deviation.
\begin{lstlisting}
def correlation(x, y):
	stdev_x = std_dev(x)
  	stdev_y = std_dev(y)
	if stdev_x > 0 and stdev_y > 0:
		return	covariance(x, y) / stdev_x / stdev_y
	else:
		return	0 # if	no variation, correlation is zero
\end{lstlisting}
Correlation 0.7333587976225691
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{$R^2$}

	\begin{itemize}
	\item Correlation, the regular `R' has values from -1 to 1 and is good enough to tell you that the two quantitative variables are strongly related.
	\item Why do you need $R^2$ then?
	\item Plain $R$ is not easier to interpret. 
	\item Example: $R=0.7$ is twice as good as $R=0.5$
	\item But its more clear when $R^2 = 0.7$ is 1.4 times as good as $R^2=.5$
	\end{itemize}
  
\tiny{(Ref: StatQuest: R-squared explained - Josh Starmer )}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{$R^2$}

\begin{columns}
    \begin{column}[T]{0.6\linewidth}
	\begin{itemize}
	\item $R^2$ is used to decide the quality of the linear fitting.
	\item $Var(mean)$ represents the variation of just the mean line, ie black line.
	\item $Var(line)$ represents the variation calculated using he fitted line, ie blue line.
	\item Taking just relative ratio to make $R^2$ in range 0 t 1 and as a percentage.
	\item If the value is 0.81, it means there is 81\% less variation around fitted line than the benchmark black line.
	\item So, if one variable is input (size) and one is output (weight), then we say that 81\% of weight variation is explained by size.
	\end{itemize}

    \end{column}
    \begin{column}[T]{0.4\linewidth}
      \begin{center}
      \includegraphics[width=\linewidth,keepaspectratio]{statq31}
	  
	   
	  	\end{center}
    \end{column}

  \end{columns}
  
	
\tiny{(Ref: StatQuest: R-squared explained - Josh Starmer )}
\end{frame}