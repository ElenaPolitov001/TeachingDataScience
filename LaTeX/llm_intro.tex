%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Background}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Traditional vs. Machine Learning?}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{tradml}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Why Machine Learning?}
\begin{itemize}
\item Problems with High Dimensionality
\item Hard/Expensive to program manually
\item Techniques to model `ANY' function given `ENOUGH' data.
\item Job \$\$\$
\end{itemize}
%\begin{center}
%\includegraphics[width=0.45\linewidth,keepaspectratio]{hp}
%\end{center}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{ML vs DL: What's the difference?}
Deep learning algorithms attempt to learn (multiple levels of) representation by using a hierarchy of multiple layers
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{dlfeat1}
\end{center}
\tiny{(Reference: https://www.xenonstack.com/blog/static/public/uploads/media/machine-learning-vs-deep-learning.png)}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Use Deep Learning When \ldots}

\begin{itemize}
\item You have lots of data (about 10k+ examples)
\item The problem is ``complex'' - speech, vision, natural language
\item The data is unstructured 
\item You need the absolute ``best'' model
\end{itemize}
\tiny{(Ref: Introduction to TensorFlow 2.0 - Brad Miro)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Relationship between AI, ML, DL}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{ai1}
\end{center}
{\tiny (Ref: https://blogs.nvidia.com/blog/2016/07/29/whats-difference-artificial-intelligence-machine-learning-deep-learning-ai/)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Get Time Sense}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llm117}
\end{center}

{\tiny (Ref: Preparing your board for Generative AI - KPMG)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{What is Deep NLP}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{nlp4}

\tiny{(Ref: Deep Learning and NLP A-Z - Kirill Eremenko)}

\tiny{(Note: Size is not indicative of importance)}
\end{center}

	\begin{itemize}
	\item Green part is NLP (rule based, linguistic)
	\item Blue part is Deep Learning not applied to NLP
	\item Purple is Deep NLP (DNLP), NN applied for NLP use cases
	\item Seq2Seq is heavily used technique of DNLP for sequence to sequence modeling, eg Translation, Q \& A, etc.
	\end{itemize}
	

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Overview of Large Language Models}
\end{center}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Typical Machine Learning Classification}
	\begin{itemize}
	\item Questions are converted to bag of words (a vocab long vector, having frequency of specific words at their places)
		\item Each question thus gets converted to fixed size vector, which acts as list of features.
		\item In training, weights are computed based on the given target.
		\item Once model is ready, it is able to answer Yes or No to the question.
			\end{itemize}
\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{nlp11}

\tiny{(Ref: Deep Learning and NLP A-Z - Kirill Eremenko)}
\end{center}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Context}

Representing words by their context

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{bert6}
\end{center}		  


{\tiny (Ref: CS224n: Natural Language Processing with Deep Learning - Christopher Manning)}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Evolution of Vectorization}

Vectors can be statistical (frequency based) or Machine/Deep Learning (supervised) based. Simple to complex.

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{chatgpt30}
\end{center}				
{\tiny (Ref: Analytics Vidhya https://editor.analyticsvidhya.com/uploads/59483evolution\_of\_NLP.png)}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Word vectors}

\begin{columns}
    \begin{column}[T]{0.5\linewidth}
			\begin{itemize}
			\item Dense vector for each word
			\item Called distributed representation, word embeddings or  word representations 
			\item Test: similar to vectors of words that appear in similar contexts
			\end{itemize}
    \end{column}
    \begin{column}[T]{0.5\linewidth}
			\begin{center}
			\includegraphics[width=0.4\linewidth,keepaspectratio]{bert7}
			\end{center}		  
    \end{column}
  \end{columns}
% {\tiny (Ref: CS224n: Natural Language Processing with Deep Learning - Christopher Manning)}

\end{frame}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Seq2Seq architecture}

% \begin{center}
% \includegraphics[width=\linewidth,keepaspectratio]{nlp13}

% \tiny{(Ref: Deep Learning and NLP A-Z - Kirill Eremenko)}
% \end{center}

% For Seq2seq last 2 options are possible. We are going ahead with the 2nd last. Last one has fixed input and same size output.

% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Seq2Seq architecture}

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{nlp14}

\tiny{(Ref: Deep Learning and NLP A-Z - Kirill Eremenko)}
\end{center}
During training, Encoder is fed with Questions and decoder with Answers. Weights in gates, hidden states get settled. During testing for each sequence of input, encoder results in to a combo vector. Decoder takes this and starts spitting out words one by  one, probabilistically.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Encoder-Decoder (seq2seq) model}

\begin{itemize}
\item The decoder is a language model that generates an 
output sequence conditioned on the input sequence.
	\begin{itemize}
	\item Vanilla RNN: condition on the last hidden state
	\item Attention: condition on all hidden states
	\end{itemize}	 
\end{itemize}	 

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llm118}

\tiny{(Ref: CS447 Natural Language Processing (J. Hockenmaier)}
\end{center}


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Transformers use Self-Attention}

\begin{itemize}
\item Attention so far (in seq2seq architectures): In the decoder (which has access to the complete input 
sequence), compute attention weights over encoder positions 
that depend on each decoder position
\item Self-attention: If the encoder has access to the complete input sequence, 
we can also compute attention weights over encoder positions 
that depend on each encoder position
\end{itemize}	 

\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Popularity}

% Masking the future in self-attention

% \begin{center}
% \includegraphics[width=\linewidth,keepaspectratio]{bert50}
% \end{center}	

 
% % {\tiny (Ref: Niels Rogge on Huggingface contributions)}
% \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Transformers}


\begin{itemize}
\item The TransformerÂ  is a model that uses attention to boost the speed with which seq2seq with attention models can be trained. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. 
\item In its heart it contains an encoding component, a decoding component, and connections between them.
\end{itemize}	 

\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{bert51}
\end{center}	

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Transformer Models}

\begin{columns}
    \begin{column}[T]{0.7\linewidth}
		\begin{center}
		\includegraphics[width=\linewidth,keepaspectratio]{promptengg81}

		{\tiny (Ref: The Complete Prompt Engineering for AI Bootcamp (2023))}
		\end{center}	
    \end{column}
    \begin{column}[T]{0.3\linewidth}
		\begin{itemize}
		\item  No hidden states, no recurrence, so parallelization possible
		\item  Context information captured via attention and positional encodings
		\item Consists of stacks of layers with various sublayers
		\end{itemize}
    \end{column}
  \end{columns}
  
Transformers are basis of (the most) Large Language Models


\end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Transformers}


% \begin{itemize}
% \item Offer a better structure to train a language model, which gave raise to the large language models (LLMs) like GPT and Bart. Its characteristics are:
% \begin{itemize}
% \item Positional encoding: each word is labeled with the number of its position in a sentence.
% \item Self-attention: each word is examined in the context of the whole sentence to generate a representation of the word. This helps the model to understand the linguistic meaning and nuances of a word.
% As the scale of a language model grows, the model builds mastery of our human language, and it does not only know how to perform basic text-based tasks but also gives a structured and logical answer to any user prompt.
% \end{itemize}	 
% \end{itemize}	 

% {\tiny (Ref: Techy Stuff 1: Notes on Transformers, LLMs, and OpenAI - Bill)}
% \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{What is a Language Models?}

\begin{itemize}
\item While typing SMS, have you seen it suggests next word?
\item While typing email, have you seen next few words are suggested?
\item How does it suggest? (suggestions are not random, right?)
\item In the past, for ``Lets go for a \ldots', if you have typed 'coffee' 15 times, 'movie' say 4 times, then it learns that. Machine/Statistical Learning.
\item Next time, when you type ``Lets go for a '', what will be suggested? why?
\item This is called Language Model. Predicting the next word. When done continuously, one after other, it spits sentence, called Generative Model.
\end{itemize}	

\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{chatgpt34}
\end{center}		

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Overview of LLM}


\begin{itemize}
\item Large Language Models (LLMs) are deep neural networks (e.g., GPT-3, BERT) based on the Transformer architecture.
\item LLMs are foundation models trained on large amounts of unsupervised and unstructured data.
\item The Transformer architecture consists of an encoder and decoder, both mostly identical with a few differences.
\item LLMs compute a probability distribution over a vocabulary (list of tokens) given an input prompt.
\item LLMs have limitations like hallucination and issues in chain of thought reasoning, but recent improvements have been made.
\item LLMs are trained for statistical language modeling, which involves predicting the next token based on context.
\end{itemize}

				
{\tiny (Ref: Overview of Large Language Models - Aman AI)}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{How Large Language Models Work}
  
  \begin{itemize}
    \item \textbf{Learning from Lots of Text:} Models like GPT-3 start by reading a massive amount of text from the internet. It's akin to learning from a giant library of information.
    
    \item \textbf{Innovative Architecture:} These models use a unique structure called a transformer, which enables them to understand and remember vast amounts of information.
    
    \item \textbf{Breaking Down Words:} They analyze sentences by breaking words into smaller parts, facilitating more efficient language processing.
    
    \item \textbf{Understanding Words in Sentences:} Unlike simple programs, these models comprehend individual words and their relationships within a sentence, gaining a holistic understanding.
    
    \item \textbf{Getting Specialized:} Following general learning, the models can be further trained on specific topics, enhancing proficiency in tasks like answering questions or writing about particular subjects.
    
    \item \textbf{Doing Tasks:} When provided with a prompt (question or instruction), these models use their acquired knowledge to generate intelligent responses. It's akin to having an assistant that comprehends and generates text.
  \end{itemize}
  
  {\tiny (Ref: What are Large Language Models(LLMs)? -Suvojit Hore)}



\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Large Language Models - Comparison}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{chatgpt31}
\end{center}				
{\tiny (Ref: Deus.ai https://www.deus.ai/post/gpt-3-what-is-all-the-excitement-about)}

\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{How Do LLMs Work?}


% \begin{itemize}
% \item LLMs predict the next token based on previous tokens in an autoregressive manner for generation.
% \item The prompt is tokenized and converted into non-contextualized embeddings.
% \item Layer-by-layer attention and feed-forward computations are performed.
% \item Decoder models assign logits to words in the vocabulary or output contextualized embeddings for encoder models.
% \item For decoder models, logits are converted into a probability distribution using Softmax.
% \item The probability distribution determines the next word in the generated text.
% \end{itemize}

				
% {\tiny (Ref: Overview of Large Language Models - Aman AI)}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{LLM Training Steps}

% At a top-level, here are steps involved in training LLMs:

% \begin{itemize}
% \item Corpus Preparation: Gather a large corpus of text data from various sources.
% Tokenization: Split the text into individual words or subword units (tokens).
% Embedding Generation: Generate embeddings using random initialization or pre-trained embeddings like Word2Vec, GloVe, or FastText.
% Neural Network Training: Train a neural network model on the input tokens.
% \begin{itemize}
% \item For encoder models (e.g., BERT), predict the context of a given word through masked language modeling and next sentence prediction tasks.
% \item For decoder models (e.g., GPT-N), predict the next token in the sequence based on prior context.
% \end{itemize}
% \end{itemize}

				
% {\tiny (Ref: Overview of Large Language Models - Aman AI)}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Computing Similarity Between Embeddings}


% \begin{itemize}
% \item Encoder models provide contextualized embeddings that enable arithmetic operations for various tasks.
% \item Contextualized embeddings can be used for word similarity by comparing the embeddings of the respective words.
% \item For sentence similarity, the output of the [CLS] token or the average of word embeddings can be used.
% \item Sentence BERT variants of encoder models are preferred for optimal performance on sentence similarity tasks.
% \item Word/sentence similarity measures the semantic equivalence between two words/sentences.
% \item Two common measures of word/sentence similarity exist, which are not considered "distance metrics."
% \end{itemize}

				
% {\tiny (Ref: Overview of Large Language Models - Aman AI)}

% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Reasoning}


% \begin{itemize}
% \item Reasoning in LLMs refers to the ability to make inferences using evidence and logic.
% \item There are different types of reasoning, including commonsense reasoning and mathematical reasoning.
% \item Various methods, such as prompting, can be used to elicit reasoning from LLMs.
% \item Determining the extent of reasoning used by LLMs for final predictions is challenging, as separating reasoning from factual information is not straightforward.
% \end{itemize}

				
% {\tiny (Ref: Overview of Large Language Models - Aman AI)}

% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{GPTs Training}

% GPT: Generative Pre-trained Transformers

% \begin{itemize}
% % \item GPT-1 is trained in a self-supervised manner (learn to predict the next word in text data) and fine-tuned in a supervised learning manner. 
% % \item GPT-2 is trained in a fully self supervised way, focusing on zero-shot transfer
% % \item  GPT-3 is pre-trained in a self supervised manner exploring a bit more the few-shots fine-tuning.
% \item GPT-1 is pre-trained on the BooksCorpus dataset, containing ~7000 books amounting to ~5GB of data
% \item GPT-2 is pre-trained using the WebText dataset which is a more diverse set of internet data containing ~8M documents for about ~40 GB of data
% \item GPT-3 uses an expanded version of the WebText dataset, two internet-based books corpora that are not disclosed and the English-language Wikipedia which constituted ~600 GB of data
% \end{itemize}	 

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{GPTs Training compared to human reading}

% \begin{itemize}
% % \item GPT-1 is trained in a self-supervised manner (learn to predict the next word in text data) and fine-tuned in a supervised learning manner. 
% % \item GPT-2 is trained in a fully self supervised way, focusing on zero-shot transfer
% % \item  GPT-3 is pre-trained in a self supervised manner exploring a bit more the few-shots fine-tuning.
% \item GPT-3 was trained on 499B tokens; GPT-4, on 1.4T tokens.
% \item In comparison, if you spent 12 hours a day reading for an entire lifetime (80 years) at average speed (250 words / minute), we would absorb 5.26B words (tokens).
% \item That's a ratio of 100:1 between the training data used for GPT-3 and the amount of data that can ever be read by a human, and 260:1 for GPT-4.
% \end{itemize}	 

% \tiny{(Ref: LinkedIn post by Dr Jennifer Prendki)}

% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Other Use Cases}
	

% \begin{itemize}
% \item Image: the AI will generate a new image based on your prompt and the image provided.
% \item Embeddings: turn input into a vector representation. Itâs very useful when we need to compare the similarity between two texts.
% \item Audio: turn audio into text.
% \end{itemize}	 

% {\tiny (Ref: Techy Stuff 1: Notes on Transformers, LLMs, and OpenAI - Bill)}
			
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Methods to Knowledge-Augment LLMs}

% Letâs look at a few methodologies to knowledge-augment LLMs

% \begin{itemize}
% \item Few-shot prompting: Powerful method for teaching LM desired outputs without weight updates; LM's reasoning and acting abilities tied to provided prompts.
% \item Fine-tuning: Complementary to few-shot prompting; update weights of parameters via supervised learning.
% \item Prompt pre-training: Mixing pre-training data with labeled demonstrations of reasoning to avoid overfitting during fine-tuning; empirical studies on gains compared to separate fine-tuning stage needed.
% \item Bootstrapping: Prompting LM in few-shot setup, discarding examples where actions or reasoning steps didn't lead to correct final prediction.
% \item Reinforcement Learning: Effective for teaching models to reason and act, utilizing supervised learning from human-created prompts.
% \end{itemize}

% {\tiny (Ref: Overview of Large Language Models - Aman AI)}

% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Example LLMs with Front-Ends}

\end{center}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{GPT3}


\begin{columns}
    \begin{column}[T]{0.6\linewidth}
		\begin{center}
		\includegraphics[width=\linewidth,keepaspectratio]{promptengg82}

		{\tiny (Ref: The Complete Prompt Engineering for AI Bootcamp (2023))}
		\end{center}	
    \end{column}
    \begin{column}[T]{0.4\linewidth}
		Created by OpenAI
		Access it with code or without (Playground https://platform.openai.com/playground)
    \end{column}
  \end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{GPT3 Features}


		\begin{center}
		\includegraphics[width=\linewidth,keepaspectratio]{promptengg83}

		{\tiny (Ref: The Complete Prompt Engineering for AI Bootcamp (2023))}
		\end{center}	

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{GPT3 Usecases}


		\begin{center}
		\includegraphics[width=\linewidth,keepaspectratio]{promptengg84}

		{\tiny (Ref: The Complete Prompt Engineering for AI Bootcamp (2023))}
		\end{center}	

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{GPT3 Limitations}




\begin{itemize}
\item  Unlike ChatGPT, GPT-3 doesnât store chat history, which can lead to you needing to re-inject the extra context into the
original prompt.
\item Tokens are currently limited to 4000 tokens per output. This includes both your initial prompt the text generated
from ChatGPT.

\end{itemize}	 

{\tiny (Ref: The Complete Prompt Engineering for AI Bootcamp (2023))}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Bard}


\begin{columns}
    \begin{column}[T]{0.6\linewidth}
		\begin{center}
		\includegraphics[width=\linewidth,keepaspectratio]{promptengg85}

		{\tiny (Ref: The Complete Prompt Engineering for AI Bootcamp (2023))}
		\end{center}	
    \end{column}
    \begin{column}[T]{0.4\linewidth}
		Created by Google
		Access it via chat https://bard.google.com/ or encounter it in search results
    \end{column}
  \end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Bard Features}


		\begin{center}
		\includegraphics[width=\linewidth,keepaspectratio]{promptengg86}

		{\tiny (Ref: The Complete Prompt Engineering for AI Bootcamp (2023))}
		\end{center}	

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Bard Usecases}


		\begin{center}
		\includegraphics[width=\linewidth,keepaspectratio]{promptengg87}

		{\tiny (Ref: The Complete Prompt Engineering for AI Bootcamp (2023))}
		\end{center}	

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Meta LLaMA}


\begin{columns}
    \begin{column}[T]{0.6\linewidth}
		\begin{center}
		\includegraphics[width=\linewidth,keepaspectratio]{promptengg88}

		{\tiny (Ref: The Complete Prompt Engineering for AI Bootcamp (2023))}
		\end{center}	
    \end{column}
    \begin{column}[T]{0.4\linewidth}
		Created by Meta and open-sourced / leaked
		Access via hugging face or lmsys https://chat.lmsys.org/
		Use one of the fine-tuned models (i.e. Vicuna) for better results
    \end{column}
  \end{columns}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Meta LLaMA}

\begin{itemize}
\item  Open-Source. Need to build a UX and any advanced functionality around it, and may need to fine-tune it.
\item Many use-cases in the enterprise canât use OpenAI for fear of sensitive data leaking or being used to train the model (though OpenAI claims to keep API data private).
\item If you have 200+ examples fine-tuning beats prompt engineering for a specific defined task.
\end{itemize}	 

{\tiny (Ref: The Complete Prompt Engineering for AI Bootcamp (2023))}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Anthropic Claude}


\begin{columns}
    \begin{column}[T]{0.6\linewidth}
		\begin{center}
		\includegraphics[width=\linewidth,keepaspectratio]{promptengg90}

		{\tiny (Ref: The Complete Prompt Engineering for AI Bootcamp (2023))}
		\end{center}	
    \end{column}
    \begin{column}[T]{0.4\linewidth}
		Created by Anthropic 
		Access via Claude console https://console.anthropic.com/ or API
		Uses Constitutional AI rather than RLHF
    \end{column}
  \end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Anthropic Claude Features}


\begin{columns}
    \begin{column}[T]{0.6\linewidth}
		\begin{center}
		\includegraphics[width=\linewidth,keepaspectratio]{promptengg91}

		{\tiny (Ref: The Complete Prompt Engineering for AI Bootcamp (2023))}
		\end{center}	
    \end{column}
    \begin{column}[T]{0.4\linewidth}
		100k Token Window! 
		
		Use Cases

		\begin{itemize}
		\item Large Scale Summarization
		\item Book Marketing
		\item Writing Styleguides
		\end{itemize}	 
    \end{column}
  \end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Microsoft Bing (GPT 4)}


\begin{columns}
    \begin{column}[T]{0.6\linewidth}
		\begin{center}
		\includegraphics[width=0.8\linewidth,keepaspectratio]{promptengg92}

		{\tiny (Ref: The Complete Prompt Engineering for AI Bootcamp (2023))}
		\end{center}	
    \end{column}
    \begin{column}[T]{0.4\linewidth}
		Powered by OpenAIâs GPT-4
		Access it via chat https://www.microsoft.com/en-gb/bing or encounter it in search results
    \end{column}
  \end{columns}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Microsoft Bing (GPT 4) Features}


		\begin{center}
		\includegraphics[width=\linewidth,keepaspectratio]{promptengg93}

		{\tiny (Ref: The Complete Prompt Engineering for AI Bootcamp (2023))}
		\end{center}	

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Microsoft Bing (GPT 4) Use Cases}


		\begin{center}
		\includegraphics[width=\linewidth,keepaspectratio]{promptengg94}

		{\tiny (Ref: The Complete Prompt Engineering for AI Bootcamp (2023))}
		\end{center}	

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Falcon}


\begin{columns}
    \begin{column}[T]{0.6\linewidth}
		\begin{center}
		\includegraphics[width=\linewidth,keepaspectratio]{promptengg95}

		{\tiny (Ref: The Complete Prompt Engineering for AI Bootcamp (2023))}
		\end{center}	
    \end{column}
    \begin{column}[T]{0.4\linewidth}
		Access it via HuggingFace transformers library https://share.descript.com/view/Qp6mcWh3V2a
		7B and 40B models as well as instruct fine-tuned
		
		Features:
		\begin{itemize}
		\item Free for commercial use
		\item Open source
		\item Possible to fine-tune
		\end{itemize}	 

    \end{column}
  \end{columns}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Falcon Use Cases}


		% \begin{center}
		% \includegraphics[width=\linewidth,keepaspectratio]{promptengg96}

		% {\tiny (Ref: The Complete Prompt Engineering for AI Bootcamp (2023))}
		% \end{center}	

% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Leader board (May 2023)}


		\begin{center}
		\includegraphics[width=0.8\linewidth,keepaspectratio]{promptengg89}

		{\tiny (Ref: The Complete Prompt Engineering for AI Bootcamp (2023))}
		\end{center}	

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Want to give it a try? - Hugging Face APIs}

  {\tiny (Ref: What are Large Language Models(LLMs)? -Suvojit Hore)}

\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Sentence Completion}


\begin{lstlisting}
import requests
from pprint import pprint

API_URL = 'https://api-inference.huggingface.co/models/bigscience/bloomz'
headers = {'Authorization': 'Entertheaccesskeyhere'}

def query(payload):
    response = requests.post(API_URL, headers=headers, json=payload)
    return response.json()
  
params = {'max_length': 200, 'top_k': 10, 'temperature': 2.5}
output = query({
    'inputs': 'Sherlock Holmes is a',
    'parameters': params,
})

print(output)

[{'generated_text': 'Sherlock Holmes is a private investigator whose cases '
                    'have inspired several film productions'}]
\end{lstlisting}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Question Answers}


\begin{lstlisting}
API_URL = 'https://api-inference.huggingface.co/models/deepset/roberta-base-squad2'
headers = {'Authorization': 'Entertheaccesskeyhere'}


def query(payload):
    response = requests.post(API_URL, headers=headers, json=payload)
    return response.json()
  
params = {'max_length': 200, 'top_k': 10, 'temperature': 2.5}
output = query({
    'inputs': {
            "question": "What's my profession?",
            "context": "My name is Suvojit and I am a Senior Data Scientist"
        },
    'parameters': params
})

pprint(output)

{'answer': 'Senior Data Scientist',
 'end': 51,
 'score': 0.7751647233963013,
 'start': 30}
\end{lstlisting}


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Summarization}


\begin{lstlisting}
API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-cnn"
headers = {'Authorization': 'Entertheaccesskeyhere'}

def query(payload):
    response = requests.post(API_URL, headers=headers, json=payload)
    return response.json()
    
params = {'do_sample': False}

full_text = '''AI applications are summarizing articles, writing stories and 
engaging in long conversations and large language models are doing 
the heavy lifting.

:
'''

output = query({
    'inputs': full_text,
    'parameters': params
})
print(output)

[{'summary_text': 'Large language models - most successful '
                  'applications of transformer models. ...'}]
\end{lstlisting}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Conclusion}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Large Language Models (LLMs) in Natural Language Processing}

  Large Language Models (LLMs) have revolutionized natural language processing, ushering in advancements in text generation and understanding. Key attributes include:

  \begin{itemize}
    \item \textbf{Learning from Extensive Data:} LLMs acquire knowledge from vast datasets, resembling a massive library of information.

    \item \textbf{Grasping Context and Entities:} These models understand context and entities, allowing for a deeper comprehension of language.

    \item \textbf{Proficient User Query Responses:} LLMs excel in responding to user queries, showcasing their ability to apply learned knowledge effectively.

  \end{itemize}

  Despite their versatile applications across industries, ethical concerns and potential biases necessitate a critical evaluation to understand their societal impact.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Key Takeaways}

  \begin{itemize}
    \item LLMs comprehend complex sentences, relationships between entities, and user intent.
    \item Explore LLM architecture: embedding, feedforward, recurrent, and attention layers.
    \item Popular LLMs like BERT, Bloom, and GPT-3 are discussed, along with open-source alternatives.
    \item Hugging Face APIs aid in generating text using LLMs like Bart-large-CNN, Roberta, and Bloom.
    \item LLMs are anticipated to revolutionize job markets, communication, and society, emphasizing the need for careful consideration of their limitations and ethical implications.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Ethical Considerations and Future Impact}

  \textbf{Ethical Considerations:}
  \begin{itemize}
    \item Awareness of potential biases in LLMs is crucial for responsible usage.
    \item Continuous evaluation of ethical implications is necessary to mitigate societal risks.
    \item Balancing the benefits of LLMs with ethical concerns ensures responsible deployment.

  \end{itemize}

  \textbf{Future Impact:}
  \begin{itemize}
    \item LLMs expected to revolutionize domains such as job markets, communication, and society.
    \item Careful use and ongoing development are essential for positive impacts.
    \item Understanding limitations and ethical considerations is vital for responsible integration into various domains.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Landscape of LLMs \& Quiz}

\begin{itemize}
\item Types of models - Foundation models, LLM, SLM, VLMs, etc.
\item Common LLM terms - Prompts, Temperature, Hallucinations, Tokens, etc.
\item LLM lifecycle stages - Pre-training, Supervised Fine Tuning, RLHF, etc.
\item LLM evaluations - ROUGE, BLEU, BIG-bench, GLUE, etc.
\item LLM architecture - Encoder, Decoder, Transformer, Attention, etc.
\item Retrieval augmented generation - Vector DBs, Chunking, Evaluations, etc.
\item LLM agents - Memory, Planning, ReAct, CoT, ToT, etc.
\item Cost \ efficiency - GPU, PEFT, LoRA, Quantization, etc.
\item LLM security - Prompt Injection, Data poisoning, etc.
\item Deployment \& inference - Pruning, Distillation, Flash Attention, etc.
\item Platforms supporting LLMOps
\end{itemize}	 

{\tiny (Ref: LinkedIn post by Abhinav Kimothi - 23 Jan 2024)}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Types of Models}

% \begin{itemize}
% \item Foundation Models: Designed to be general-purpose, providing a foundation
% for various AI applications. Examples: GPT 3.5, GPT4, Stable Diffusion, Llama, BERT,
% Gemini
% \item Large Language Models (LLMs): Foundation models, trained on the ``Transformer
% Architecture'', that can perform a wide array of Natural
% Language Processing (NLP) tasks like text generation,
% classification, summarization etc. Examples: GPT 3.5, Llama2, Mistral, Falcon
% \item Small Language Models (SLMs): SLMs are like LLMs but with lesser number of trained
% parameters (therefore called ``Small''): Example : TinyLlama, Pythia, Phi-2
% \item Large Multimodal Models (LMMs): not just process and generate text, but also other data modalities
% like image, video, speech, audio, etc. Example : LLaVA, LLaMA-Adapter V2, LAVIN
% \item Vision Language Models (VLMs): Image generation with text input/ Examples : GPT4, Gemini, Mixtral, Flamingo, BLIP, Macaw
% LLM
% \end{itemize}	 

% {\tiny (Ref: LinkedIn post by Abhinav Kimothi - 23 Jan 2024)}
% \end{frame}

