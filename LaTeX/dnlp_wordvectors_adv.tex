%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}

\begin{center}
{\Large Advanced Word Embeddings \ldots}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{What if \ldots}
We want to use subword
information?
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{FastText}
Adding subword information
\begin{itemize}
\item  Model: SG-NS (skip-gram with negative sampling)
\item Change the way word vectors are formed
\item  each word represented as a bag of character 
n-gram: eg ``where'' : "wh","whe","her","ere","re"
\item  associate a vector representation to each n-
gram 
\item  represent a word by the sum of the vector 
representations of its n-grams
\end{itemize}

{\tiny (Ref: Bojanovsky et al, TACL 2017 http://aclweb.org/anthology/Q17-1010)}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{What if \ldots}
We abstract the skip-gram 
model to the sentence level?
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Sentence Embedding}

\begin{itemize}
\item  Before: use a word to predict its 
surrounding context
\item Now: encode a sentence to predict 
the sentences around it 
\end{itemize}

{\tiny (Ref: Kiros et al., NIPS 2015 https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Latest embeddings}

\begin{itemize}
\item  The year 2018 has been an inflection point for machine learning models in NLP 
\item It’s been referred to as NLP’s ImageNet moment,
\end{itemize}

\begin{center}
\includegraphics[width=0.7\linewidth,keepaspectratio]{emb16}
\end{center}

{\tiny (Ref: The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) -Jay Alammar )}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{ELMo: Context Matters}
\begin{itemize}
\item  Say, a word “stick” would be represented by a Word2Vec vector no-matter what the context was. 
\item  “stick”” has multiple meanings depending on where it’s used. 
\item Why not give it an embedding based on the context it’s used in – to both capture the word meaning in that context as well as other contextual information?”. 
\item Instead of using a fixed embedding for each word, ELMo looks at the entire sentence before assigning each word in it an embedding. It uses a bi-directional LSTM trained on a specific task to be able to create those embeddings.

\end{itemize}


{\tiny (Ref: The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) -Jay Alammar )}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{ULM-FiT: Nailing down Transfer Learning in NLP}
\begin{itemize}
\item  ULM-FiT introduced methods to effectively utilize a lot of what the model learns during pre-training – more than just embeddings, and more than contextualized embeddings. ULM-FiT introduced a language model and a process to effectively fine-tune that language model for various tasks.
\item NLP finally had a way to do transfer learning probably as well as Computer Vision could
\end{itemize}


{\tiny (Ref: The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) -Jay Alammar )}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{The Transformer: Going beyond LSTMs}
\begin{itemize}
\item The Encoder-Decoder structure of the transformer made it perfect for machine translation. But how would you use it for sentence classification? 
\item How would you use it to pre-train a language model that can be fine-tuned for other tasks?
\end{itemize}


{\tiny (Ref: The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) -Jay Alammar )}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{OpenAI Transformer}
Pre-training a Transformer Decoder for Language Modeling 
\begin{itemize}
\item It turns out we don’t need an entire Transformer to adopt transfer learning and a fine-tunable language model for NLP tasks. 
\item We can do with just the decoder of the transformer. The decoder is a good choice because it’s a natural choice for language modeling (predicting the next word) since it’s built to mask future tokens – a valuable feature when it’s generating a translation word by word.
\item Now that the OpenAI transformer is pre-trained and its layers have been tuned to reasonably handle language, we can start using it for downstream tasks.
\end{itemize}


{\tiny (Ref: The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) -Jay Alammar )}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{BERT: From Decoders to Encoders}
\begin{itemize}
\item The openAI transformer gave us a fine-tunable pre-trained model based on the Transformer. But something went missing in this transition from LSTMs to Transformers. 
\item ELMo’s language model was bi-directional, but the openAI transformer only trains a forward language model. 
\item Could we build a transformer-based model whose language model looks both forward and backwards (in the technical jargon – “is conditioned on both left and right context”)?
\item 
“Hold my beer”, said R-rated BERT. Everybody knows bidirectional conditioning would allow each word to indirectly see itself in a multi-layered context.” “We’ll use masks”, said BERT confidently.
\end{itemize}


{\tiny (Ref: The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) -Jay Alammar )}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{BERT}
So, BERT builds on top of a number of clever ideas that have been bubbling up in the NLP community recently – 
including but not limited to 
\begin{itemize}
\item  Semi-supervised Sequence Learning (by Andrew Dai and Quoc Le), 
\item  ELMo (by Matthew Peters and researchers from AI2 and UW CSE), 
\item  ULMFiT (by fast.ai founder Jeremy Howard and Sebastian Ruder), 
\item  the OpenAI transformer (by OpenAI researchers Radford, Narasimhan, Salimans, and Sutskever), and 
\item  the Transformer (Vaswani et al).
\end{itemize}

You can use the pre-trained BERT to create contextualized word embeddings

{\tiny (Ref: The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) -Jay Alammar )}
\end{frame}
