%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \begin{center}
    {\Large Recurrent Neural Network}
  \end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Toy Example: Scenario I: Cook}
An ideal roommate who cooks:
\begin{itemize}
\item Can cook 3 dishes: Apple Pie, Burger, Chicken
\item Dish depends on the outside weather (only Sunny or Rainy)
\begin{itemize}
\item Sunny: Apple Pie
\item Rainy: Burger
\end{itemize}
\end{itemize}
This can be easily modeled as Neural network with known inputs and outputs.

{\tiny (Ref: Luis Serrano)}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Toy Example: Scenario I}
Lets model food and weather as one-hot encoding vectors
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{rnn6}
\end{center}
Network takes weather vector in and outputs food vector
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Toy Example: Scenario I}
\begin{itemize}
\item This Neural Network is represented by a matrix (3x2)g
\item Individual weather vector when post multiplies this matrix, resultant 3x1 vector represents food
\item For Sunny, you get Apple Pie
\item For Rainy, you get Burger.
\end{itemize}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{rnn7}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Toy Example: Scenario I}
\begin{itemize}
\item Matrix can be seen as network as well.
\item First node emits 3 rays, each to each output node: 1, 0, 1. This is nothing but first column of the matrix
\item Second node emits 3 rays, each to each output node: 0, 1, 0. This is nothing but second column of the matrix
\item Input weather vector values are put into input nodes, multiplication happens and then output values are generated.
\end{itemize}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rnn8}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Toy Example: Scenario II}

\begin{itemize}
\item Same roommate, but instead of weather he decides the dish based on previous day.
\item One day Apple Pie, next Burger, next Chicken.
\end{itemize}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{rnn9}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Toy Example: Scenario II}

\begin{itemize}
\item This is not normal Neural network, its called Recurrent Neural Network (RNN)
\item There is no Weather input, but previous day's output is its input. Shown as loop.
\end{itemize}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rnn10}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Toy Example: Scenario II}
As a network:
\begin{itemize}
\item 3 inputs, 3 outputs
\item Both representing food
\item Its basically looping back. So, its Recurrent.
\end{itemize}
\begin{center}
\includegraphics[width=0.5\linewidth,keepaspectratio]{rnn11}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Toy Example: Scenario III}
\begin{itemize}
\item Same roommate, but now the rules of cooking are dependent on both, weather as well as previous day dish
\item If Sunny, he goes out, has fun, no cooking, so dish is leftover, same as yesterday
\item If Rainy, he is at home, next dish on the sequence.
\item Below, for Tuesday, it's Sunny weather is shown under Monday (just for illustration) as input
\end{itemize}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rnn12}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Toy Example: Scenario III}
Food Matrix
\begin{itemize}
\item Food matrix artificially cut into two
\item Top represents Sunny, bottom as Rainy
\item Apple Pie comes in, top results in Apple Pie as TODAY's food (same), bottom results in next dish, ToMORROW's food.
\item Nothing about Weather as of now.
\end{itemize}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rnn13}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Toy Example: Scenario III}
Weather Matrix
\begin{itemize}
\item Top results are all 1's. Same as Today, Sunny as 1 represents Sunny, 0 Rainy.
\item Bottom result are all 0's. For Next day.
\item Weather matrix tells, if he as to cook TODAY's food or TOMORROW's food. As the bottom results are all 1's, or YES's.
\end{itemize}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rnn14}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Toy Example: Scenario III}
\begin{itemize}
\item Combining two matrices should give clear signal on what to cook.
\item Food Matrix: Whats the food for today? Whats for tomorrow?
\item Weather Matrix: Should I cook today? or tomorrow?
\end{itemize}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rnn15}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Toy Example: Scenario III}
Testing
\begin{itemize}
\item Say, we had Apple Pie yesterday and its rainy today
\item Food Matrix is computed and added to Weather Matrix
\end{itemize}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rnn16}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Toy Example: Scenario III}
Merge
\begin{itemize}
\item We apply non linear function that takes input of the combined results
\item Maps it to uniform one-hot like result.
\item It make the LARGEST as 1, else all to 0
\item Add sub-components  and produces the result
\end{itemize}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rnn17}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Toy Example: Scenario III}
Summary (shown arrows are 1)
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{rnn18}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Toy Example: Scenario III}
Recurrent
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{rnn19}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Toy Example: Scenario IV: Summary}
\begin{itemize}
\item We can group together inputs and outputs.
\item Two inputs: yesterday's predictions and yesterday's dish
\item One output: today's prediction
\end{itemize}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rnn20}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Toy Example: Scenario IV: Summary}
\begin{itemize}
\item Prediction is recycled as input
\item The dotted line, holds it to be used today
\end{itemize}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rnn21}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Toy Example: Scenario IV: Summary}
\begin{itemize}
\item So, even if we missed few time steps in between, 
\item We can play the actions forward from the known past time
\end{itemize}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rnn22}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{What is RNN?}
Neural network
\begin{itemize}
\item When data is sequential
\item Next depends on previous + extra input
\item Example: Stock Predictions, Language Model (next word prediction), etc.
\end{itemize}
\end{frame}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{What is RNN?}

A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor. 
The diagram above shows what happens if we unroll the loop. 

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rnn}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{What is RNN?}

RNN allow us to operate over sequences of vectors. Many types
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rnn23}
\end{center}
Each rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red, output vectors are in blue and green vectors hold the RNN's state (hidden layer nodes are called `state' here as it has memory)

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{What is RNN?}

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rnn28}

\tiny{(Ref: RNN - Udacity)}

\end{center}

RNN can be imagined to be a usual ANN, with additional nodes from next layer back into inputs.
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{What is RNN?}

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rnn23}
\end{center}
One-to-One: Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification).
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{What is RNN?}

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rnn23}
\end{center}
One-to-Many: Sequence output (e.g. image captioning takes an image (fixed size) and outputs a sentence of words).
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Recurrent neural networks}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rnn23}
\end{center}
Many-to-One: Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment).
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Recurrent neural networks}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rnn23}
\end{center}
Many-to-Many: Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French).
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Recurrent neural networks}
% \begin{center}
% \includegraphics[width=0.8\linewidth,keepaspectratio]{rnn23}
% \end{center}
% Many-to-Many: Synced sequence input and output (e.g. video classification where we wish to label each frame of the video).
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Recurrent neural networks}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rnn23}
\end{center}
Notice that in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like.
\end{frame}



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{RNN: Sequential processing in absence of sequences.}
% \begin{center}
% \includegraphics[width=0.3\linewidth,keepaspectratio]{rnn24}
% \end{center}
% Images are fixed size. But for sequential numbers in it, RNN learns to read out house numbers from left to right.
% \end{frame}


%% http://karpathy.github.io/2015/05/21/rnn-effectiveness/
%% https://www.youtube.com/watch?v=iX5V1WpxxkY&index=10&list=LLsBKTrp45lTfHa_p49I2AEQ








% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{RNNs Intuition}

% \begin{itemize}
% \item Human thoughts have persistence; humans don't start their thinking from scratch every second. 
% \item As you read this sentence, you understand each word based on your understanding of previous words.  
% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{RNNs Intuition}

% \begin{itemize}
% \item One of the appeals of RNNs is the idea that they are able to connect previous information to the present task
% \item E.g., using previous video frames to inform the understanding of the present frame.
% \item E.g., a language model tries to predict the next word based on the previous ones.
% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{What is Neural Memory?}

% \begin{itemize}
% \item List the alphabet forward\ldots. you can do it, yes?
% \item List the alphabet backward\ldots. hmmm\ldots perhaps a bit tougher.
% \item Try with the lyrics of a song you know?
% \item Why is it easier to recall forward than it is to recall backward? Can you jump into the middle of the second verse?\ldots hmm\ldots also difficult. Why?
% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{What is Neural Memory?}

% \begin{itemize}
% \item There's a very logical reason for this
% \item You learned them as a sequence. 
% \item You are really good at indexing from one letter to the next.
% \item It's a kind of conditional memory. (Once you given a start of poem, you can recite it further)
% \item All these  tasks are easier when you know what happened earlier in the sequence
% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{What is Neural Memory?}

% \begin{itemize}
% \item However, it's not that you don't have the song in your memory except when you're singing it. 
% \item Instead, when you try to jump straight to the middle of the song, you simply have a hard time finding that representation in your brain (perhaps that set of neurons).
% \item It starts searching all over looking for the middle of the song, but it hasn't tried to look for it this way before, so it doesn't have a map to the location of the middle of the second verse.
% \end{itemize}
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{What is Neural Memory?}

\begin{itemize}
\item If you wanted to train a neural network to predict whats the next scene would be in the next frame, it would be really helpful to know where the scene was in the last frame! 
\item Sequential data like this is why we build recurrent neural networks.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{What is Neural Memory?}

\begin{itemize}
\item Neural networks have hidden layers. Normally, the state of your hidden layer is based ONLY on your input data. 
\item So, normally a neural network's information flow would look like this:
$input \rightarrow hidden  \rightarrow  output$
\item This is straightforward. Certain types of input create certain types of hidden layers. 
\item Certain types of hidden layers create certain types of output layers. 
\item It's kind-of a closed system. Memory changes this. 
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{What is Neural Memory?}
\begin{itemize}
\item Memory means that the hidden layer is a combination of your input data at the current time-step and the hidden layer of the previous timestep.
$ (input + prev\_hidden) \rightarrow hidden \rightarrow output$
\item Why the hidden layer? Well, we could technically do this. 
$(input + prev\_input) \rightarrow hidden \rightarrow output$
\end{itemize}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{What is Neural Memory?}
% Here, we have 4 timesteps of a recurrent neural network pulling information from the previous hidden layer.
% \begin{itemize}
% \item $(input + empty\_hidden) \rightarrow hidden \rightarrow output$
% \item $(input + prev\_hidden) \rightarrow hidden \rightarrow output$
% \item $(input + prev\_hidden) \rightarrow hidden \rightarrow output$
% \item $(input + prev\_hidden) \rightarrow hidden \rightarrow output$
% \end{itemize}
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{What is Neural Memory?}
Colored show how hidden layer gets updated. 
4 timesteps with hidden layer recurrence:
\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{rnn1}
\end{center}
4 timesteps with input  layer recurrence:
\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{rnn2}
\end{center}
Focus on the last hidden layer (4th line).
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{What is Neural Memory?}
% \begin{itemize}
% \item  In the hidden layer recurrence, we see a presence of every input seen so far. In the input layer recurrence, it's exclusively defined by the current and previous inputs. 
% \item This is why we model hidden recurrence.
% \item Hidden recurrence learns what to remember whereas input recurrence is hard wired to just remember the immediately previous datapoint. 
% \end{itemize}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{What is Neural Memory?}
% \begin{itemize}
% \item  Now compare and contrast these two approaches with the backwards alphabet and middle-of-song exercises. 
% \item The hidden layer is constantly changing as it gets more inputs. 
% \item Furthermore, the only way that we could reach these hidden states is with the correct sequence of inputs.
% \end{itemize}
% \end{frame}



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{What is Neural Memory?}
% \begin{itemize}
% \item  What's the practical difference? Let's say we were trying to predict the next word in a song given the previous. 
% \item The ``input layer recurrence'' would break down if the song accidentally had the same sequence of two words in multiple places. 
% \item Think about it, if the song had the statements ''I love you'', and ''I love carrots'', and the network was trying to predict the next word, how would it know what follows ''I love''? 
% It could be carrots. It could be you. The network REALLY needs to know more about what part of the song its in.
% \end{itemize}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{What is Neural Memory?}
% \begin{itemize}
% \item  It could be carrots. 
% \item It could be you. 
% \item The network REALLY needs to know more about what part of the song its in.
% \item However, the ''hidden layer recurrence'' doesn't break down in this way. 
% \item It subtely remembers everything it saw (with memories becoming more subtle as it they fade into the past).
% \end{itemize}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{RNN - Neural Network Memory}
% Normal NN process:
% \begin{itemize}
% \item Our input layer to the neural network is determined by our input dataset. 
% \item Each row of input data is used to generate the hidden layer (via forward propagation). 
% \item Each hidden layer is then used to populate the output layer (assuming only 1 hidden layer)
% \end{itemize}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{RNN - Neural Network Memory}
% \begin{itemize}
% \item Just saw, memory means that the hidden layer is a combination of the input data and the previous hidden layer. 
% \item How is this done?
% \item Much like every other propagation in neural networks, it's done with a matrix. 
% \item This matrix defines the relationship between the previous hidden layer and the current one.
% \end{itemize}
% \end{frame}

% %%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%\begin{frame}[fragile] \frametitle{RNN - Neural Network Memory}
% %%%\begin{center}
% %%%\includegraphics[width=0.6\linewidth,keepaspectratio]{rnn3}
% %%%\end{center}
% %%%\begin{itemize}
% %%%\item SYNAPSE\_0 propagates the input data to the hidden layer. 
% %%%\item SYNAPSE\_1 propagates the hidden layer to the output data. 
% %%%\item The new matrix (SYNAPSE\_h, the recurrent one), propagates from the hidden layer (layer\_1) to the hidden layer at the next timestep (still layer\_1). 
% %%%\end{itemize}
% %%%\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{RNN - Summary}
\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{rnn4}
\end{center}
\begin{itemize}
\item It depicts 4 timesteps. 
\item The first is exclusively influenced by the input data. 
\item The second one is a mixture of the first and second inputs. 
\item This continues on. 
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{RNN - Summary}
\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{rnn4}
\end{center}
\begin{itemize}
\item You should recognize that, in some way, network 4 is ''full''. 
\item Presumably, timestep 5 would have to choose which memories to keep and which ones to overwrite. 
\item It's the notion of memory ''capacity''. As you might expect, bigger layers can hold more memories for a longer period of time.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{RNN - Summary}
\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{rnn4}
\end{center}
\begin{itemize}
\item Also, this is when the network learns to forget irrelevant memories and remember important memories. 
\item What significant thing do you notice in timestep 3? 
\item Why is there more green in the hidden layer than the other colors?
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{RNN - Summary}
\begin{itemize}
\item Also notice that the hidden layer is the barrier between the input and the output. 
\item In reality, the output is no longer a pure function of the input. 
\item The input is just changing what's in the memory, 
\item and the output is exclusively based on the memory. 
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{RNN - Summary}
\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{rnn4}
\end{center}
\begin{itemize}
\item Another interesting takeaway. 
\item If there was no input at timesteps 2,3,and 4, the hidden layer would still change from timestep to timestep. 
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Back-propagation Through Time}
\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{rnn5}
\end{center}
\begin{itemize}
\item So, how do recurrent neural networks learn? 
\item Errors are bright yellow, derivatives are mustard colored.
\item Picture shows back propagation coming from 4th step upto 3rd.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{Back-propagation Through Time}
\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{rnn5}
\end{center}
\begin{itemize}
\item They learn by fully propagating forward from 1 to 4 and then back-propagating all the derivatives from 4 back to 1. 
\item Its normal neural network, except that we're re-using the same weights (synapses 0,1,and h) in their respective places. 
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%\begin{frame}[fragile] \frametitle{Recurrent neural networks}
%%
%%\begin{itemize}
%%\item Recurrent Neural Networks are networks with loops in them, allowing information to persist.
%%\item In the above diagram, a chunk of neural network, A, looks at some input xt and outputs a value ht. 
%%\item A loop allows information to be passed from one step of the network to the next. 
%%\end{itemize}
%%\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{RNNs advantages}

\begin{itemize}
\item The applications of standard Neural Networks (and also Convolutional Networks) are limited due to:
\item They only accepted a fixed-size vector as input (e.g., an image) and produce a fixed-size vector as output (e.g., probabilities of different classes). 
\item These models use a fixed amount of computational steps (e.g. the number of layers in the model).
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{RNNs advantages}

\begin{itemize}
\item Recurrent Neural Networks are unique as they allow us to operate over sequences of vectors.
\item Sequences in the input, the output, or in the most general case both
\end{itemize}
\end{frame}


%%%
%%%
%%%
%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%\begin{frame}[fragile] \frametitle{Stateful models}
%%%
%%%
%%%\begin{itemize}
%%%\item Using this idea, we can think of variable width inputs
%%%such that each new word simply updates our current prediction.
%%%In this way an RNN has two types of data inside of it:
%%%\begin{itemize}
%%%\item fixed weights, just as we have been using with CNNs
%%%\item stateful variables that are updated as it observes words
%%%in a document
%%%\end{itemize}
%%%\item Can also think of this as giving `memory' to the neural
%%%network.
%%%\end{itemize}
%%%\end{frame}
%%%
%%%
%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%\begin{frame}[fragile] \frametitle{RNNs Memory}
%%%\begin{center}
%%%\includegraphics[width=0.8\linewidth,keepaspectratio]{rnnmem}
%%%\end{center}
%%%\end{frame}
%%%
%%%
%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%\begin{frame}[fragile] \frametitle{}
%%%
%%%
%%%
%%%A third way of thinking about recurrent neural networks is to
%%%think of a network that has a loop in it. However, the self-input
%%%get's applied the \textit{next} time it is called.
%%%
%%%\begin{center}
%%%\includegraphics[width=0.3\linewidth,keepaspectratio]{cloah01.png}
%%%\end{center}
%%%
%%%\end{frame}
%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%\begin{frame}[fragile] \frametitle{}
%%%
%%%A fourth way of thinking about a recurrent neural network is
%%%mathematically. We now have two parts to the update function
%%%in the RNN:
%%%\begin{align*}
%%%h_{t} &= {W x_t + b} + {U h_{t-1}}
%%%\end{align*}
%%%Notice that $U$ must always be a square matrix,
%%%because we could unravel this one time further to yield:
%%%\begin{align*}
%%%h_{t} &= W x_t + b + U W x_{t-1} + U b + U^2 h_{t-2}
%%%\end{align*}
%%%
%%%\end{frame}
%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%\begin{frame}[fragile] \frametitle{What's different?}
%%%\begin{itemize}
%%%\item At each time step, output of hidden layers $H$ are added back for the next input.
%%%\item Outputs are also calculated at each time step ($Y_t$)
%%%\item Same wts and biases are shared across iterations
%%%\item Its like a state machine cell (remembering something)
%%%\item Can be used for sequences
%%%\end{itemize}
%%%
%%%
%%%\end{frame}
%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%\begin{frame}[fragile] \frametitle{What's different?}
%%%\begin{center}
%%%\includegraphics[width=0.8\linewidth,keepaspectratio]{rnnwts}
%%%\end{center}
%%%
%%%\end{frame}
%%%
%%%
%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%\begin{frame}[fragile] \frametitle{RNN}
%%%
%%%\begin{center}
%%%\includegraphics[width=0.8\linewidth,keepaspectratio]{rnnsamewts}
%%%\end{center}
%%%Output $y^i$ depends on $x_1, x_2,\ldots x_i$
%%%
%%%\end{frame}
%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%\begin{frame}[fragile] \frametitle{On Sequences}
%%%
%%%
%%%\begin{itemize}
%%%\item Each cell, gets input, does iterations to try to match with given outputs. In the process, calculates wts and biases, as usual. May not work. 
%%%\item Instead of one cell, have a series and see if $Y_5$ can be forced.
%%%\item It is achieved by backpropagating through all cells, backwards.
%%%\item All $H$''s are adjusted, so $W$''s and $b$''s
%%%\item Can train on sequence of characters and verify open and close brackets
%%%\end{itemize}
%%%\begin{center}
%%%\includegraphics[width=0.8\linewidth,keepaspectratio]{rnnseq}
%%%\end{center}
%%%
%%%\end{frame}
%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%\begin{frame}[fragile] \frametitle{How to train RNN?}
%%%
%%%\begin{center}
%%%\includegraphics[width=0.8\linewidth,keepaspectratio]{rnntrain}
%%%\end{center}
%%%Find the network parameters to minimize the total cost
%%%
%%%\end{frame}
%%%
%%%
%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%\begin{frame}[fragile] \frametitle{Of course it can be deep}
%%%\begin{center}
%%%\includegraphics[width=0.8\linewidth,keepaspectratio]{rnndeep}
%%%\end{center}
%%%\end{frame}
%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%\begin{frame}[fragile] \frametitle{Bidirectional RNN}
%%%\begin{center}
%%%\includegraphics[width=\linewidth,keepaspectratio]{rnnbi}
%%%\end{center}
%%%\end{frame}
%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%\begin{frame}[fragile] \frametitle{Bidirectional RNN}
%%%\begin{center}
%%%\includegraphics[width=\linewidth,keepaspectratio]{rnnbi}
%%%\end{center}
%%%\end{frame}
%%%
%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%\begin{frame}[fragile] \frametitle{Many to Many (Output is shorter)}
%%%Both input and output are both sequences, but the output is shorter.
%%%E.g. Speech Recognition
%%%\begin{center}
%%%\includegraphics[width=\linewidth,keepaspectratio]{rnnmm}
%%%\end{center}
%%%\end{frame}
%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%\begin{frame}[fragile] \frametitle{Can also Stack them}
%%%\begin{center}
%%%\includegraphics[width=\linewidth,keepaspectratio]{rnnstack}
%%%\end{center}
%%%\end{frame}
%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%\begin{frame}[fragile] \frametitle{RNN Types a summary}
%%%Input vectors are in red, output vectors are in blue and green vectors hold the RNN's state
%%%\begin{center}
%%%\includegraphics[width=\linewidth,keepaspectratio]{rnntypes}
%%%\end{center}
%%%
%%%\end{frame}
%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%\begin{frame}[fragile] \frametitle{RNN Types a summary}
%%%\begin{itemize}
%%%\item Standard mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification). 
%%%\item Sequence output (e.g. image captioning takes an image and outputs a sentence of words). 
%%%\item Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment). 
%%%\item Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French). 
%%%\item Synced sequence input and output (e.g. video classification where we wish to label each frame of the video).
%%%\end{itemize}
%%%\end{frame}
%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%\begin{frame}[fragile] \frametitle{RNN Apps}
%%%\begin{center}
%%%\includegraphics[width=\linewidth,keepaspectratio]{rnnapps}
%%%\end{center}
%%%\end{frame}
%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%\begin{frame}[fragile] \frametitle{Name Entity Recognition}
%%%Detecting named entities like name of people, locations, organization, etc. in a sentence.
%%%\begin{center}
%%%\includegraphics[width=\linewidth,keepaspectratio]{rnnner}
%%%\end{center}
%%%\end{frame}
%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%\begin{frame}[fragile] \frametitle{Name Entity Recognition}
%%%\begin{center}
%%%\includegraphics[width=\linewidth,keepaspectratio]{rnnnerlayer}
%%%\end{center}
%%%\end{frame}
%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%\begin{frame}[fragile] \frametitle{Unfortunately}
%%%RNN-based network is not always easy to learn
%%%\begin{center}
%%%\includegraphics[width=\linewidth,keepaspectratio]{rnndisadv}
%%%\end{center}
%%%\end{frame}
%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%\begin{frame}[fragile] \frametitle{Errors}
%%%The error surface is rough.
%%%\begin{center}
%%%\includegraphics[width=\linewidth,keepaspectratio]{rnnerr}
%%%\end{center}
%%%\end{frame}
%%%
%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%\begin{frame}[fragile] \frametitle{Anything better}
%%%\begin{itemize}
%%%\item ``Michael was born in Paris.<sentences> His mother tongue is ''
%%%\item Clue is far away. You need to have very long chain of such cells. That's limitation.
%%%\item Just few words is ok.
%%%\item Solution: LSTM (Long Short Term Memory)
%%%\end{itemize}
%%%\end{frame}
%%%
%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%\begin{frame}[fragile] \frametitle{Anything better}
%%%\begin{center}
%%%\includegraphics[width=\linewidth,keepaspectratio]{rnn2lstm}
%%%\end{center}
%%%\end{frame}



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}
  % \begin{center}
    % {\Large RNN variations}
    
    % LSTM-GRU
  % \end{center}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Recap: Simple RNN}
% \begin{center}
% \includegraphics[width=0.5\linewidth,keepaspectratio]{lstm49}

% \tiny{(Ref: Intro to Deep Learning - Coursera, National Research University Higher School of Economics)}
% \end{center}
% RNN takes previous hidden state ($h_{t-1}$), current input ($x_t$) and generates new/next hidden state ($h_t$) by formula $h_t = f_h ( Vx_t + Wh_{t-1} + b_h)$
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile] \frametitle{RNN Shortcomings}
\begin{itemize}
\item RNN does not work in some situations
\item Especially when long past words need to be accounted for
\item Due to Vanishing Gradient problem.
\end{itemize}
\end{frame}
