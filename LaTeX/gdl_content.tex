%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Introduction}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{It's all about Data}

Training data is of two main domains:
\begin{itemize}
\item Euclidean: Multi-dimensional linear/planar space. Distance between two data-points is a straight line. E.g. tabular data. Current Deep Learning models work very well with this.
\item Non-euclidean: Distance between two data-points is not a straight line. E.g. Social Networks, Meshed 3D surfaces. Recent Geometric Deep Learning are for this type of data.
\end{itemize}
	  
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{gdl1}
\end{center}

{\tiny (Ref: http://graphics.stanford.edu/courses/cs468-20-fall/)}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Why Geometric?}

\begin{itemize}
\item Euclidean geometry works on planes. But, say, for curved surfaces, like, earth, the straight line distance between two points in not really the correct (arc) distance.
\item Different types of surfaces, different rules of geometries.
\item Can there be any unification?
\item In 1872, professor Felix Klein started 'Erlangen Program' to unify all these geometries by using the concepts of Invariance and Symmetry.
\end{itemize}
	  
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{gdl2}
\end{center}

{\tiny (Ref: A gentle introduction to Geometric Deep Learning - Vitale Sparacello)}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Why Geometric?}

DNNs usually work with fixed-size structured inputs like vectors. Things get complicated with non-Euclidean data
	  
\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{gdl3}
\end{center}

{\tiny (Ref:https://distill.pub/2021/understanding-gnns/)}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Example: Convolution}

\begin{itemize}
\item Convolutions work well on fixed size images and filters which share weights.
\item How can we formalize and extend this idea for other domains? 
\item Maybe applying geometric principles \ldots
\end{itemize}
	  
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Universal Approximation Theorem}

\begin{itemize}
\item Goal of Machine Learning is to learn a 'function' that approximately fits training data.
\item Universal Approximation Theorem: with just one hidden layer, they can represent combinations of step functions, allowing to approximate any continuous function with arbitrary precision
\end{itemize}
	  
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{gdl4}
\end{center}

{\tiny (Ref: https://towardsdatascience.com/geometric-foundations-of-deep-learning-94cdd45b451d)}		
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Dimensionality Curse}

\begin{itemize}
\item Things get complicated in high dimensions
\item We need an exponential number of samples to approximate even the most straightforward multidimensional function.
\item Example (below) Approximation of a continuous function in multiple dimensions
\end{itemize}
	  
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{gdl5}
\end{center}

{\tiny (Ref: https://towardsdatascience.com/geometric-foundations-of-deep-learning-94cdd45b451d)}		
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Dimension Reduction}

\begin{itemize}
\item Dimension reduction (by Principal Component Analysis, pCA) can be done, but thats not without losing information.
\item To overcome this problem, we can use the geometric structure of input data. This structure is formally called Geometric prior and it is useful to formalize how we should process the input data.
\end{itemize}
	  
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{gdl6}
\end{center}

{\tiny (Ref: https://medium.com/@jamesim2077/introduction-to-pca-principal-component-analysis-c26dffe2a857)}		
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Geometric Prior}

\begin{itemize}
\item For 2D image, its flattened/projected(?) d-dimensional vector of pixels can be its Geometric Prior.
\item Can process images independently of any shift (CNNs)
\item Can process data projected on spheres independently of rotations
\item Can process graph data independently of isomorphism (Graph Neural Networks).
\end{itemize}
	
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Geometric Deep Learning}

Identifies two main priors our functions must respect to simplify the problem and injecting geometric assumptions to input data:

\begin{itemize}
\item Symmetry: is respected by functions that leave an object invariant, they must be composable, invertible, and they must contain the identity;
\item Scale Separation: the function should be stable under slight deformation of the domain.
\end{itemize}
	
	 If the class of functions we define respect these properties we can tackle any data domain.
	 
\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{gdl7}
\end{center}

{\tiny (Ref: A gentle introduction to Geometric Deep Learning - Vitale Sparacello)}	
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Graphs}

See the progression \ldots

\begin{itemize}
\item In tabular data, rows are independent of each other. E.g. Regression, Classification.
\item In time-series or sequential data, the next point is dependent on previous point(s).
\item In images or spatial data, one point is dependent on the neighbors of fixed size.
\item In social networks, points are connected to each other, do not have order, can have cycle, can have direction, the most generic case, thats Graphs \ldots.
\end{itemize}
	
	{\em Data has shape, and shape has a meaning}
	


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Permutations}

\begin{itemize}
\item  Geometric Deep Learning tries to find functions to be  able to process graph data independently of node organization

\item  All the possible node combinations are defined by a mathematical operation called Permutation. The functions must respect: 
\begin{itemize}
\item Permutation Invariance: applying permutations shouldn’t modify the result. $f(PX,PAP^T )=f(X,A)$
\item Permutation Equivalence: it doesn’t matter if we apply permutations before or after the function. $f(PX, PAP^T)=Pf(X, A)$
\item E.g. in Convolutional Neural Networks (it doesn't matter if pixels are translated) by using Convolutions on pixels neighbors (more formally: using local equivalent processing layers).
\end{itemize}
	\end{itemize}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Neighborhood}


\begin{itemize}
\item Let’s define the set $N_i$ of the neighbors of node $i$ as $X_{N_i}={x_j: j  N_i}$ ie. $x_1$ has neighbors called $X_{N_1}$
\item E.g. local neighborhood of the node $x_b$ is possible to create hidden representation $h_b$ using the aggregation function $g(x_b, X_{N_b})$.
 
\end{itemize}
	
\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{gdl8}
\end{center}

{\tiny (Ref: https://petar-v.com/talks/5G-CS224W.pdf)}	

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Application}


\begin{itemize}
\item Deep Graph Neural Networks are built by stacking multiple of these layers!
\item Using this blueprint is possible to train GNNs able to solve the following tasks:
\end{itemize}
	
\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{gdl9}
\end{center}

{\tiny (Ref: https://petar-v.com/talks/5G-CS224W.pdf)}	

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Neighborhood Aggregation}

Defining how we compute the latent representations, using the aggregation function $g$ (aka the hidden layers) we can have different flavors of GNNs:

\begin{itemize}
\item Convolutional GNNs: they use the neighbours to compute the hidden representation of the current node. Using this approach is possible to mock CNNs by computing localized convolutions.
\item Attentional GNNs: they use a self attention mechanism similar to Transformer models to learn weights between each couple of connected nodes.
\item Message passing GNNs: they propagate node features by exchanging information between adjacent nodes.
\end{itemize}
	
\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{gdl10}
\end{center}

{\tiny (Ref: https://petar-v.com/talks/5G-CS224W.pdf)}	

\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Sample Picture Inclusion}

% \begin{center}
% \includegraphics[width=0.8\linewidth,keepaspectratio]{myphoto}
% \end{center}	  
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{Sample Code Listing}
% \begin{lstlisting}
% import aaa
% \end{lstlisting}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Sample Two Columns Slide}
% \begin{columns}
    % \begin{column}[T]{0.6\linewidth}
      % \begin{itemize}
		% \item aaa
	  % \end{itemize}

    % \end{column}
    % \begin{column}[T]{0.4\linewidth}
      % \begin{itemize}
		% \item bbb
	  % \end{itemize}
    % \end{column}
  % \end{columns}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Sample Tabular Data}

% aaa

% \begin{tabular}{|c|c|}
	% \hline
	% Platform & Time (s) \\
	% \hline \hline
	% Python & $\sim$1500.0 \\
	% \hline
	% NumPy & 29.3 \\
	% \hline
	% Matlab & $\sim$29.0 \\
	% \hline
	% Octave & $\sim$60.0 \\
	% \hline
	% Blitz (C++) & 9.5 \\
	% \hline
	% Fortran & 2.5 \\
	% \hline
	% C & 2.2 \\
	% \hline
% \end{tabular}

% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{References}

\begin{itemize}
\item A gentle introduction to Geometric Deep Learning - Vitale Sparacello
\end{itemize}
	  
\end{frame}
