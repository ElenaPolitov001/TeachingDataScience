%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Transformer \\ \small ``Attention is all you need''}
\end{center}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Goal: Study Transformer}


	\begin{center}
	\includegraphics[width=0.35\linewidth,keepaspectratio]{transformer}
	\end{center}		

{\tiny (Ref: ``Attention is all you need''. 2017.  Aswani, Shazeer, Parmar, Uszkoreit,  Jones, Gomez, Kaiser, Polosukhin  https://arxiv.org/pdf/1706.03762.pdf}


			
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{A High-Level Look}

Lets start with a very high-level view \ldots and then Zoom in each sub-blocks.

Here is the Transformer block and its i/o.

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{transformer_jay}

{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}		

\begin{itemize}
\item Main architecture: encoder-decoder ie sequence to sequence
\item Task: machine translation with parallel corpus (encoder-decoder)
\item Sub-tasks: Word Embedding (Encoder), Predict each translated word (Decoder)
\item can be done by RNNs, LSTMs, etc. But they had issues.
\item Here we bring parallelization.
\item Challenges? Different input-output sizes, different attention correspondence, word order matters, semantic preservation, compute cost, storage cost, etc.
\end{itemize}

			
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{One level down}

Within Transformer block we have

\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{transformer_encoders_decoders_jay}

{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}		

Inference time flow: 
\begin{itemize}
\item Encoder takes input
\item Does some processing, creates a `latent` representation, sends it to decoder
\item Decoder processes this input to generate the output
\end{itemize}

What would be ML training flow?
			
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{One level down}

Each sub-block ie Encoder-Decoder blocks are actually stacks of encoders decoders.

\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{transformer_encoder_decoder_stack_jay}

{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}		

 
\begin{itemize}
\item Encoder has 6 blocks, so does Decoder. 
\item Why 6? Why not 7? They found it to be good. Like Hyper-parameter.
\end{itemize}

How many blocks GPT 3.5 has?
			
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{One level down in Encoder Cell}

Each Encoder sub-block is actually stack of \ldots

\begin{columns}
    \begin{column}[T]{0.5\linewidth}
		\begin{center}
		\includegraphics[width=\linewidth,keepaspectratio]{transformer_encoder_jay}
		
		{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
		\end{center}		
	\end{column}
	\begin{column}[T]{0.5\linewidth}
		\begin{itemize}
		\item Self Attention:  a layer that helps the encoder look at other words in the input sentence as it encodes a specific word.
		\item Feed Forward: The outputs of the self-attention layer are fed to a feed-forward neural network. The exact same feed-forward network is independently applied to each position.
		\end{itemize}
	\end{column}
\end{columns}
Input Embedding? Positional Embedding? Residual connections? batch normalization?
			
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Similarly on Decoder side}

Each Decoder sub-block is actually stack of \ldots


		\begin{center}
		\includegraphics[width=\linewidth,keepaspectratio]{transformer_decoder_jay}
		
		{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}

		\end{center}		
		

		\begin{itemize}
		\item The decoder also has both those layers (Self but Masked Attention and Feed Forward), but between them is an (cross) attention layer that helps the decoder focus on relevant parts of the input sentence
		\end{itemize}

		
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Transformer Encoder}

Actual architecture in the research paper.

\begin{columns}
    \begin{column}[T]{0.4\linewidth}
			\begin{center}
			\includegraphics[width=0.6\linewidth,keepaspectratio]{bert70}
			\end{center}		
		\end{column}
    \begin{column}[T]{0.6\linewidth}
      \begin{itemize}
			\item For encoder block, initial input, tokenization, embedding, positional encoding, happens at the start, then the first Encoder cell starts.
			\item In each cell, we have multi-head self attention, residual connections, batch normalization and then feed forward then again residual + batch normalization.
			\item Output of this cell is passed to the next encoder cell.
			\item Output of the last encoder cell in the encoder block is passed to the Decoder block.
			\end{itemize}
    \end{column}
  \end{columns}
		
     \begin{itemize}
			\item Mind well, that each word here gets processed parallelly, unlike RNN or LSTM.
			\item Cells are repeated 6 times (in vertical stack)
			\end{itemize}		
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Transformer Decoder}


			\begin{center}
			\includegraphics[width=\linewidth,keepaspectratio]{bert71}
			\end{center}		

			
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Transformer Encoder-Decoder}

			\begin{center}
			\includegraphics[width=\linewidth,keepaspectratio]{bert72}
			\end{center}		

			
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Transformer Encoder-Decoder}
Next, let’s look at the Transformer Encoder and Decoder Blocks in action


      \begin{itemize}
			\item Imagine Machine Translation: French to English
			\item What would be ML training flow?
			\item What would be ML inference flow?
			\item Can we use input or output text as is, ie in string format?
			\item Whats the NLP way of making text available for ML or DL algorithms?
	\end{itemize}
			
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Block: Input Embedding}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Convert Word to Vectors}


\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{embeddings_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}		


\begin{itemize}
\item Either by Frequency based ie one-hot or tf-idf or Neural way ie Word2Vec (in paper)
\item Or put embedding layer (like in TensorFlow or Keras NLP, which also gets trained)
\item Size: 512, a hyper-parameter we can set – basically it would be the length of the longest sentence in our training dataset.
\item After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Flow of Vectors in Encoder Cell}


\begin{columns}
    \begin{column}[T]{0.5\linewidth}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{encoder_with_tensors_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}		

		\end{column}
    \begin{column}[T]{0.5\linewidth}
\begin{itemize}
\item Key property : the word in each position flows through its own path in the encoder. 
\item Calculations in the self-attention layer do need other words. 
\item But later, in the feed-forward layer does not have those dependencies, \item thus the various paths can be executed in parallel while flowing through the feed-forward layer.
\end{itemize}
    \end{column}
  \end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Flow of Vectors in Encoder Block}


\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{encoder_with_tensors_block_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}		


\begin{itemize}
\item The word at each position passes through a self-attention process. 
\item Then, they each pass through a feed-forward neural network -- the exact same network with each vector flowing through it separately.
\item Meaning ALL weights in these cells, block are same for ALL words going through in each epoch, of course they get adjusted during back propagation.
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Block: Positional Encoding}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Why?}


\begin{itemize}
\item In languages like English (unlike Sanskrit) word order matters, meaning 'Suresh ate chicken' and 'chicken ate Suresh' are two different things.
\item As input is all word vectors together, parallel, (unlike RNN or LSTM where they are sequential ie one after another, there the order is preserved), the sense of position is lost.
\item Need to add that information by unique position value/signature.
\item A cool mechanism (similar to Fourier Transform) was employed.
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Representing The Order of The Sequence Using Positional Encoding}


\begin{columns}
    \begin{column}[T]{0.5\linewidth}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{transformer_positional_encoding_vectors_jay}

\includegraphics[width=\linewidth,keepaspectratio]{transformer_positional_encoding_example_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}		

		\end{column}
    \begin{column}[T]{0.5\linewidth}

\begin{itemize}
\item Special pattern based vectors encode position or the distance between different words in the sequence. 
\item Using sine waves of different phase lags and amplitude to create unique set of basis functions, equal to number of words in the sentence.
\item They are added to input word embedding.
\end{itemize}
    \end{column}
  \end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Positional Patterns}

TBD
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Self Attention}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Self-Attention at a High Level}

\begin{columns}
    \begin{column}[T]{0.5\linewidth}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{transformer_self-attention_visualization_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}		

		\end{column}
    \begin{column}[T]{0.5\linewidth}

\begin{itemize}
\item ``The animal didn't cross the street because it was too tired'', what ``it'' refers to?
\item  the street or to the animal?
\item For “it”, self-attention mechanism (some how, or should) allows it to associate “it” with “animal”.
\item ie for each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.
\item Lets see how that can be achieved.
\end{itemize}
    \end{column}
  \end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Self-Attention in Principle}


\begin{itemize}
\item Slight digression: lets say you have database of key($k$)-value($v$) pairs, then you have a query ($q$), which needs to go and search in the database and get corresponding or closest value.
\item So, $q$ is matched (similarity) with all $k$s, whichever matches most, its $v$ is returned.
\item Same mechanism is used to create self-attention score.
\item For each word-vector in the sentence ($q$), we find similarity with $k$ vectors or all the words, which ever matches most, its $v$ vector is returned.
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Self-Attention in Detail}

\begin{columns}
    \begin{column}[T]{0.5\linewidth}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{transformer_self_attention_vectors_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}		

		\end{column}
    \begin{column}[T]{0.5\linewidth}

\begin{itemize}
\item Create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). 
\item So for each word, we create a Query vector, a Key vector, and a Value vector. \item These vectors are created by multiplying the embedding by three matrices, which are trained during the training process.
\end{itemize}
    \end{column}
  \end{columns}
  
  Multiplying $x_1$ by the $W^Q$ weight matrix produces $q_1$, the ``query`` vector associated with that word. We end up creating a ``query'', a ``key'', and a ``value'' projection of each word in the input sentence.
  
  Note: new vectors are smaller, 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don’t HAVE to be smaller, just a choice.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Self-Attention Scores}

\begin{columns}
    \begin{column}[T]{0.5\linewidth}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{transformer_self_attention_score_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}		

		\end{column}
    \begin{column}[T]{0.5\linewidth}

\begin{itemize}
\item Calculating the self-attention for the first word in this example, “Thinking”. 
\item Need to score each word of the input sentence against this word, thats its importance wrt that word
\item score = dot product of words query vector with key vector of the running words. Each is a single scalar value. So, score for first word $w_1 = q_1 k_1 + q_1 k_2 + \ldots$
\end{itemize}
    \end{column}
  \end{columns}

* Please note that in the diagram the Embedding vectors also include positional encoding incorporated inside.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Self-Attention Scores}

\begin{columns}
    \begin{column}[T]{0.5\linewidth}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{self-attention_softmax_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}		

		\end{column}
    \begin{column}[T]{0.5\linewidth}

\begin{itemize}
\item Divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. 
\item This leads to having more stable gradients. There could be other possible values here, but this is the default), 
\item then pass the result through a softmax that normalizes from 0 to 1

\end{itemize}
    \end{column}
  \end{columns}
  
   Note: Clearly word with itself will have the highest softmax score, thats ok
 but then next matching-high scores have importance

* Please note that in the diagram the Embedding vectors also include positional encoding incorporated inside.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Self-Attention Scores with Value Vectors}

\begin{columns}
    \begin{column}[T]{0.5\linewidth}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{self-attention-output_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}		

		\end{column}
    \begin{column}[T]{0.5\linewidth}

\begin{itemize}
\item Multiply each value vector by the softmax scores respectively
\item The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words, basically weighted per position.
\item The last step is to sum up the weighted value vectors.
\item This produces the output of the self-attention layer at this position (for the first word).
\end{itemize}
    \end{column}
  \end{columns}

* Please note that in the diagram the Embedding vectors also include positional encoding incorporated inside.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Self-Attention Scores in a Matrix Way}

\begin{columns}
    \begin{column}[T]{0.5\linewidth}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{self-attention-matrix-calculation_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}		

		\end{column}
    \begin{column}[T]{0.5\linewidth}

\begin{itemize}
\item Assuming two words in the sentence, so number of rows are two, num columns are 512 (but shown 4 just for brevity) and the q/k/v vectors (64, or 3 boxes in the figure)
\item  Calculate the Query, Key, and Value matrices. 
\item Input sentence vector is matrix $X$, and multiplying it by the weight matrices to be trained ($W^Q, W^K, W^V$).
\end{itemize}
    \end{column}
  \end{columns}

* Please note that in the diagram the Embedding vectors $X$ also include positional encoding incorporated inside.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Self-Attention Scores in a single formula}


\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{self-attention-matrix-calculation-2_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}		

\end{frame}




% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Dot-Product Attention}


      % \begin{itemize}
			% \item Inputs: a query q and a set of key-value (k-v) pairs to an output
			% \item Query, keys, values, and output are all vectors
			% \item Output is weighted sum of values, where
			% \item Weight of each value is computed by an inner product of query and corresponding key
			% \item Queries and keys have same dimensionality $d_k$ , value have $d_v$
			% \end{itemize}
			
			% \begin{center}
			% \includegraphics[width=0.4\linewidth,keepaspectratio]{bert73}
			% \end{center}		
			
			% % {\tiny (Ref: CS224n: Natural Language Processing with Deep Learning - Christopher Manning)}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Matrix notation}


      % \begin{itemize}
			% \item Inputs: a query q and a set of key-value (k-v) pairs to an output
			% \item Query, keys, values, and output are all vectors
			% \item Output is weighted sum of values, where
			% \item Weight of each value is computed by an inner product of query and corresponding key
			% \item Queries and keys have same dimensionality $d_k$ , value have $d_v$
			% \end{itemize}
			
			% \begin{center}
			% \includegraphics[width=0.8\linewidth,keepaspectratio]{bert74}
			% \end{center}		
			
			% % {\tiny (Ref: CS224n: Natural Language Processing with Deep Learning - Christopher Manning)}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Key-Query-Value Attention}

			
			% \begin{center}
			% \includegraphics[width=\linewidth,keepaspectratio]{bert75}
			% \end{center}		
			
% % {\tiny (Ref: Language \& Machine Learning - John Hewitt)}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Key-Query-Value Attention}

			
			% \begin{center}
			% \includegraphics[width=\linewidth,keepaspectratio]{bert76}
			% \end{center}		
			
% % {\tiny (Ref: Language \& Machine Learning - John Hewitt)}

% \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Multi-head Attention}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{The Beast With Many Heads}

Having multiple self attention helps:

\begin{columns}
    \begin{column}[T]{0.5\linewidth}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{transformer_attention_heads_qkv_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}		

		\end{column}
    \begin{column}[T]{0.5\linewidth}

\begin{itemize}
\item Expands the model’s ability to focus on different positions. Because each weight matrices of Q K V are initialized randomly, they can get settled differently, and also differently for different sentences.
\item Paper uses 8 heads in a single multi-head self attention cell.
\item ie Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.
\end{itemize}
    \end{column}
  \end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{The Beast With Many Heads}

Having multiple self attention helps:

\begin{columns}
    \begin{column}[T]{0.5\linewidth}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{transformer_attention_heads_z_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}		

		\end{column}
    \begin{column}[T]{0.5\linewidth}

\begin{itemize}
\item One problem though. 
\item The next feed-forward layer is not expecting eight matrices – it’s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix.
\item How do we do that? 
\end{itemize}
    \end{column}
  \end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{The Beast With Many Heads}

Having multiple self attention helps:

\begin{columns}
    \begin{column}[T]{0.5\linewidth}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{transformer_attention_heads_weight_matrix_o_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}		

		\end{column}
    \begin{column}[T]{0.5\linewidth}

\begin{itemize}
\item  We concat the matrices then multiply them by an additional weights matrix $W^O$.
\end{itemize}
    \end{column}
  \end{columns}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Self Attention in Summary}


\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{transformer_multi-headed_self-attention-recap_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}	


* Please note that in the diagram the Embedding vectors $X$ also include positional encoding incorporated inside.	

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Self Attention in Summary Example}


\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{transformer_self-attention_visualization_2_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}	


As we encode the word "it", one attention head is focusing most on "the animal", while another is focusing on "tired" -- in a sense, the model's representation of the word "it" bakes in some of the representation of both "animal" and "tired".

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Self Attention in Summary Example}



\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{transformer_self-attention_visualization_3_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}	

If we add all the attention heads to the picture, however, things can be harder to interpret, but thats what happens.

This is how Self-Attention block generates, ``attended'' vectors.

\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Multi-headed attention}

			
			% \begin{center}
			% \includegraphics[width=\linewidth,keepaspectratio]{bert77}
			% \end{center}		
			
% % {\tiny (Ref: Language \& Machine Learning - John Hewitt)}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{The Transformer Encoder: Multi-headed attention}

			
			% \begin{center}
			% \includegraphics[width=\linewidth,keepaspectratio]{bert78}
			% \end{center}		
			
% % {\tiny (Ref: Language \& Machine Learning - John Hewitt)}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Attention visualization in layer 5}

			
			% \begin{center}
			% \includegraphics[width=0.8\linewidth,keepaspectratio]{bert79}
			% \end{center}		
			
			% % {\tiny (Ref: CS224n: Natural Language Processing with Deep Learning - Christopher Manning)}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Attention visualization: Implicit anaphora resolution}

			
			% \begin{center}
			% \includegraphics[width=0.6\linewidth,keepaspectratio]{bert80}
			% \end{center}		
			
			% In 5th layer. Isolated attentions from just the word ‘its’ for attention heads 5 and 6.  Note that the attentions are very sharp for this word.

			
			% % {\tiny (Ref: CS224n: Natural Language Processing with Deep Learning - Christopher Manning)}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Parallel attention heads}

			
			% \begin{center}
			% \includegraphics[width=0.6\linewidth,keepaspectratio]{bert81}
			% \end{center}		
			
		
			% % {\tiny (Ref: Ashish Vaswani)}

% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Add \& Norm}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{The Residuals}


\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{transformer_resideual_layer_norm_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}		

Each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a layer-normalization step.


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Visualize The Residuals}

Visualize the vectors and the layer-norm operation associated with self attention, it would look like this:

\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{transformer_resideual_layer_norm_2_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}		

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Also for the Decoder}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{transformer_resideual_layer_norm_3_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}		

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Feed Forward}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Feed Forward}

TBD
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Residual connections}

			% The Transformer Encoder: Residual connections [He et al., 2016]
			
			% \begin{center}
			% \includegraphics[width=\linewidth,keepaspectratio]{bert82}
			% \end{center}		
			
		
			% % {\tiny (Ref: John Hewitt)}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Layer normalization}
% The Transformer Encoder: Layer normalization [Ba et al., 2016]
			
			% \begin{center}
			% \includegraphics[width=\linewidth,keepaspectratio]{bert83}
			% \end{center}		
			
		
			% % {\tiny (Ref: John Hewitt)}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{ Softmax}

			% The Transformer Encoder: Scaled Dot Product [Vaswani et al., 2017]
			% \begin{center}
			% \includegraphics[width=\linewidth,keepaspectratio]{bert84}
			% \end{center}		
			
		
			% % {\tiny (Ref: John Hewitt)}

% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Encoder side is ready with latent space, parallel word embedding of the input sentence ie BERT}
\end{center}
\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Decoder Side}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{The Decoder Side - Overview}


\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{transformer_decoding_1_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}	


The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{The Decoder Side - Overview}


\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{transformer_decoding_2_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}	


The following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{The Decoder Side - Overview}



\begin{itemize}
  \item In the decoder, self-attention layers differ from those in the encoder.
    \begin{itemize}
      \item The self-attention layer in the decoder can only attend to earlier positions in the output sequence.
      \item This is achieved by masking future positions (setting them to $-\infty$) before the softmax step in the self-attention calculation.
    \end{itemize}
  \item The "Encoder-Decoder Attention" layer functions similarly to multiheaded self-attention.
    \begin{itemize}
      \item It generates its Queries matrix from the layer below it.
      \item The Keys and Values matrix are taken from the output of the encoder stack.
    \end{itemize}
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Output Embedding plus Positional Encoding}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Masked Attention}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Masked Multi-head Attention}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Linear plus Softmax}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{The Final Linear and Softmax Layer}

\begin{itemize}
  \item The decoder stack produces a vector of floats as its output.
  \item How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.
  \item The Linear layer is a fully connected neural network projecting the decoder stack's output vector into a much larger vector known as a logits vector.
    \begin{itemize}
      \item Assuming a model vocabulary of 10,000 unique English words, the logits vector has 10,000 cells, each corresponding to the score of a unique word.
    \end{itemize}
  \item The softmax layer transforms these scores into probabilities, ensuring they are all positive and sum up to 1.0.
    \begin{itemize}
      \item The cell with the highest probability determines the chosen word, which serves as the output for the current time step.
    \end{itemize}
  \item Given a final vector from the neural network,
    \begin{itemize}
      \item This vector is expanded to a vocabulary-long vecto via $ m \times n$ neural network.
      \item Output is through a softmax function, causing the values to be compressed to the range of 0 to 1.
      \item Each value can be interpreted as a probability.
    \end{itemize}
  \item The index in the vocabulary vector with the highest probability determines the output word.
	
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{The Final Linear and Softmax Layer}


\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{transformer_decoder_output_softmax_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}	

\end{frame}



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Attending Encoder side}

% The Transformer Encoder: Scaled Dot Product [Vaswani et al., 2017]
			
			% \begin{center}
			% \includegraphics[width=\linewidth,keepaspectratio]{bert85}
			% \end{center}		
			
		
			% % {\tiny (Ref: John Hewitt)}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Attending Encoder side}

			% The Transformer Encoder: Scaled Dot Product [Vaswani et al., 2017]
			
			% \begin{center}
			% \includegraphics[width=\linewidth,keepaspectratio]{bert86}
			% \end{center}		
			
		
			% % {\tiny (Ref: John Hewitt)}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Attending Encoder side}

						% The Transformer Encoder: Scaled Dot Product [Vaswani et al., 2017]

			% \begin{center}
			% \includegraphics[width=\linewidth,keepaspectratio]{bert87}
			% \end{center}		
			
		
			% % {\tiny (Ref: John Hewitt)}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Attending Encoder side}

						% The Transformer Encoder: Scaled Dot Product [Vaswani et al., 2017]

			
			% \begin{center}
			% \includegraphics[width=\linewidth,keepaspectratio]{bert88}
			% \end{center}		
			
		
			% % {\tiny (Ref: John Hewitt)}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Cross-attention}

			% The Transformer Decoder: Cross-attention (details)
			
			% \begin{center}
			% \includegraphics[width=\linewidth,keepaspectratio]{bert89}
			% \end{center}		
			
		
			% % {\tiny (Ref: John Hewitt)}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Cross-attention}

			% The Transformer Decoder: Cross-attention (details)
			
			% \begin{center}
			% \includegraphics[width=\linewidth,keepaspectratio]{bert90}
			% \end{center}		
			
			% % {\tiny (Ref: John Hewitt)}

% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Backpropogation during training}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Pre-training workflow}

% \begin{columns}
    % \begin{column}[T]{0.5\linewidth}
\begin{center}
\includegraphics[width=0.7\linewidth,keepaspectratio]{one-hot-vocabulary-example_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}		

		% \end{column}
    % \begin{column}[T]{0.5\linewidth}

\begin{itemize}
\item During training, an untrained model would go through the exact same forward pass. 
\item  But we have labeled dataset, ie inputs as well as output sentences. 
\item Say, vocab is of 6 words and using one hot encoding.
\item Assume our training sentences have only one pair, 'merci' and 'thanks'
\end{itemize}
    % \end{column}
  % \end{columns}	

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{The Loss Function}

% \begin{columns}
    % \begin{column}[T]{0.5\linewidth}
\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{transformer_logits_output_and_label_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}		

		% \end{column}
    % \begin{column}[T]{0.5\linewidth}

\begin{itemize}
\item Goal: For "merci", we want the output to be a probability distribution indicating the word “thanks”. 
\item But since this model is not yet trained (meaning all weight matrices have random values), that’s unlikely to happen just yet.
\end{itemize}
    % \end{column}
  % \end{columns}	

\begin{itemize}
\item How do you compare two probability distributions? 
\item Say, we simply subtract one from the other. (For more details, look at cross-entropy and Kullback–Leibler divergence.)
\end{itemize}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{The Loss Function - More complex example}

For example – input: ``je suis etudiant'' and expected output: ``i am a student''. We want our model to successively output probability distributions where:

\begin{columns}
    \begin{column}[T]{0.5\linewidth}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{output_target_probability_distributions_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}		

		\end{column}
    \begin{column}[T]{0.5\linewidth}

\begin{itemize}
\item Each probability distribution is represented by a vector of width vocab\_size (6 in our toy example, but more realistically a number like 30,000 or 50,000)
\item The first probability distribution has the highest probability at the cell associated with the word “i”
\item The second probability distribution has the highest probability at the cell associated with the word “am”
\item And so on, until the fifth output distribution indicates ‘<EOS>’ symbol, which also has a cell associated with it from the 10,000 element vocabulary.
\end{itemize}
    \end{column}
  \end{columns}	

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{The Loss Function - More complex example}

\begin{columns}
    \begin{column}[T]{0.5\linewidth}
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{output_trained_model_probability_distributions_jay}


{\tiny (Ref: ``The Illustrated Transformer'' - Jay Alammar)}
\end{center}		

 We repeat this for positions \#2 and \#3…etc. This method is called “beam search”
		\end{column}
    \begin{column}[T]{0.5\linewidth}

\begin{itemize}
\item Now, because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest (greedy decoding)
\item Another way, take top 2, (say, ‘I’ and ‘a’ for example), then in the next step, run the model twice: once assuming the first output position was the word ‘I’, and another time assuming the first output position was the word ‘a’, and whichever version produced less error considering both positions \#1 and \#2 is kept.
\end{itemize}
    \end{column}
  \end{columns}	

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Decoder only workflow}
\end{center}
\end{frame}
