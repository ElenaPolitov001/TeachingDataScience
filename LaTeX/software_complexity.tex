%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Computational Complexity}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{What does it do?, How fast does it do it?}

		
			\begin{itemize}
			\item What does the code compute?
			\item Sum of squares: \texttt{foo(n)} computes $\sum\limits_{i=0}^{n-1} i^2$
			\item How fast is it?
			\item Harder to answer:
			\item 1 second for $n=1000$.
			\item But what if $n$ changes?
			\item And what if we use another computer?
	\end{itemize}
	
		\lstinputlisting{src/for-loop.py}
	
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Why do we ask this?}

	\begin{columns}
		\column{0.455\textwidth}
			\begin{itemize}
			\item \lstinputlisting{src/for-loop.py}
			
			\item \lstinputlisting{src/nested-for-loop.py}
	\end{itemize}
			
		\column{0.455\textwidth}
		
			\begin{itemize}
			\item Comparing implementations: 	How can we compare \texttt{foo} and \texttt{bar}?
			\item By counting operations!
		\end{itemize}
	\end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Counting operations}
	\begin{columns}
		\column{0.455\textwidth}
			\lstinputlisting{src/for-loop.py}
		\column{0.455\textwidth}
		
Counting operations:			How many operations happen when we call \texttt{foo(n)}?
			\begin{itemize}
				\item $2 + n$
				\item $2 + n + n$
				\item $3 + 2n + n-1$
				\item $4 + n + n + n + n-1$
			\end{itemize}
	\end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Getting rid of those nasty constants}

	\begin{itemize}
		\item Observation: We do not care if it's is $2+n$ or even $3+2n$.
		\item The important part is that it \textit{scales with the input}.
			
		\item We call this the ``asymptotic run time complexity''.
	
		\item \lstinputlisting{src/for-loop.py}

		\item No more numbers:		We say this code has $c_0 + c_1n$ operations, where:
		\begin{itemize}
			\item $c_0$ is initialisation of $s$ on line 2 and the return statement on line 5.
			\item $c_1$ is the \texttt{range} function on line 3 and the multiplication and addition on line 4.
		\end{itemize}
	\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Getting rid of those nasty constants}

	\begin{itemize}
		\item lstinputlisting{src/nested-for-loop.py}
		\item So what about here?:	How can we describe the number of operations here?	
		\item Quadratic time:
				We say this code has $c_0 + c_1n + c_2 n^2$ operations, where:
				\begin{itemize}
					\item $c_0$ is initialisation of $s$ on line 2 and the return statement on line 6.
					\item $c_1$ is the \texttt{range} function on line 3.
					\item $c_2$ is the \texttt{range} function on line 4 and the multiplication and addition on line 5.
				\end{itemize}
			\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Some numbers!}
	
Differences in run time for	Different code snippets, all executed 1000 times.

		\hfill\\
		\begin{tabular}{c | c c c c c c c}
			\scriptsize
			Input size & constant & linear & quadratic & cubic & exponential & factorial\\
			\midrule
			
			1 & $<$10 ms & $<$10 ms & $<$10 ms & $<$10 ms & $<$10 ms & $<$10 ms\\
			2 & $<$10 ms & $<$10 ms & $<$10 ms & $<$10 ms & $<$10 ms & $<$10 ms\\
			
			5 & $<$10 ms & $<$10 ms & $<$10 ms & 36 ms & 40 ms & 210 ms\\
			
			7 & $<$10 ms & $<$10 ms & $<$10 ms & 49 ms & 50 ms & \alert{$>$3000 ms} \\
			
			10 & $<$10 ms & $<$10 ms & 23 ms & 78 ms & 84 ms & \alert{$>$3000 ms}\\
			
			100 & $<$10 ms & $<$10 ms & 284 ms & \alert{$>$3000 ms} & \alert{$>$3000 ms} & \alert{$>$3000 ms} \\
			
			1000 & $<$10 ms & 54 ms & \alert{$>$3000 ms} &\alert{$>$3000 ms} & \alert{$>$3000 ms} & \alert{$>$3000 ms} \\
			
			10000 & $<$10 ms &  \alert{$>$3000 ms} &\alert{$>$3000 ms} &\alert{$>$3000 ms} & \alert{$>$3000 ms} & \alert{$>$3000 ms} \\
		\end{tabular}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Big-Oh notation}

	\begin{itemize}
		\item We care about what we call: ``asymptotic run time complexity''.
		\item We denote this using big-Oh, e.g.\ $f(n) = 3n + 2$ is $O(n)$.

		\item You (may?) have used big-Oh to a certain point before. E.g.\ as $n$ approaches $5$.
			
		\item In computer science we only think about when $n$ approaches $\infty$.
	\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Formally}
	\begin{definition}[Big-Oh]
		A function $f(n)$ is $O(g(n))$ iff there is a positive real constant $c$ and a positive integer $n_0$ such that for
		all $n \geq n_0$ it holds that $f(n) \leq c g(n)$. In other words:\\
		$\exists c \in \mathbb{R}, \exists n_0 \in \mathbb{N} (c > 0 \wedge n_0 \geq 1 \wedge (\forall n \in \mathbb{N} (n
		\geq n_0 \to f(n) \leq cg(n))))$.
	\end{definition}
	
		Which of the following is/are true?
		\begin{itemize}
			\item $n^2$ is $O(n^3)$
			\item $8n^2$ is $O(n^2)$
			\item $16n^2 + 5n + 2$ is $O(n^2)$
			\item $16n^2 + 5n \log n$ is $O(n^2)$
			\item $16n^2\log n$ is $O(n^2)$
		\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Let's prove that}

		Prove that $f(n) = 16n^2 + 5n + 2$ is $O(n^2)$.
	
	\begin{proof}
		To prove: $\exists c > 0, n_0 \geq 1$ such that $\forall n \geq n_0$ $16n^2 + 5n + 2 \leq cn^2$.\\
		
		Take $n_0 = 1$, now for all $n \in \mathbb{N}$ with $n \geq n_0$:
		\begin{align*}
			16n^2 + 5n + 2 &\leq 16n^2 + 5n^2 + 2n^2 \\
										 &= 23n^2
		\end{align*}
		So take $c=23$.
	\end{proof}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Polynomial run time}
	
		\begin{itemize}
			\item A function has a polynomial run time $T(n)$ if $T(n)$ is $O(n^c)$ for some constant $c$.	
			\item Most of the algorithms treated in this course have a polynomial run time.
			\item We will revisit the notion of polynomial run times in the very last lecture, where we study some problems that
			\item we believe to have no polynomial time solution!
	\end{itemize}	
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Some numbers! Revisited}
	
Differences in run time:		We can now formalize our previous table a little bit:
		

		\begin{tabular}{c | c c c c c c c}
			\scriptsize
			Input size & \alert{$O(1)$} & \alert{$O(n)$} & \alert{$O(n^2)$} & \alert{$O(n^3)$} & \alert{$O(2^n)$} & \alert{$O(n!)$} \\
			\midrule
			1 & $<$10 ms & $<$10 ms & $<$10 ms & $<$10 ms & $<$10 ms & $<$10 ms\\
			2 & $<$10 ms & $<$10 ms & $<$10 ms & $<$10 ms & $<$10 ms & $<$10 ms\\
			5 & $<$10 ms & $<$10 ms & $<$10 ms & 36 ms & 40 ms & 210 ms\\
			7 & $<$10 ms & $<$10 ms & $<$10 ms & 49 ms & 50 ms & $>$3000 ms \\
			10 & $<$10 ms & $<$10 ms & 23 ms & 78 ms & 84 ms & $>$3000 ms\\
			100 & $<$10 ms & $<$10 ms & 284 ms & $>$3000 ms & $>$3000 ms & $>$3000 ms \\
			1000 & $<$10 ms & 54 ms & $>$3000 ms &$>$3000 ms & $>$3000 ms & $>$3000 ms \\
			10000 & $<$10 ms &  $>$3000 ms &$>$3000 ms &$>$3000 ms & $>$3000 ms & $>$3000 ms \\
		\end{tabular}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Revisiting our code snippets}
	
	\lstinputlisting{src/for-loop.py}
	\begin{columns}
		\column{0.455\textwidth}
		Which of these describes the run time $T(n)$?
	\begin{itemize}
		\item $T(n)$ is $O(1)$.
		\item $T(n)$ is $O(n)$. 
		\item $T(n)$ is $O(n^2)$. 
		\item $T(n)$ is $O(n^3)$. 
		\item I don't know.
	\end{itemize}	
		\column{0.455\textwidth}
	\begin{itemize}
		\item B through D are correct.
		\item We often request the tightest bound. Which in this case is $O(n)$.
	\end{itemize}	
		
	\end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Which case?}
	
	\begin{columns}
		\column{0.755\textwidth}

		Which of these forms a tight bound on the run time $T(n)$?
		
	\begin{itemize}
		\item $O(1)$. 
		\item $O(n)$. 
		\item $O(n^2)$. 
		\item I don't know.
	\end{itemize}	
		\column{0.255\textwidth}
		
			Only B
		
				We talk about the \textit{worst-case}.
	\end{columns}
	
	\lstinputlisting{src/for-loop-wc.py}
	
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{So \ldots}

	\begin{columns}
		\column{0.455\textwidth}
	\begin{itemize}
		\item \lstinputlisting{src/for-loop.py}
		
		\item \lstinputlisting{src/nested-for-loop.py}
	\end{itemize}	
		
		\column{0.455\textwidth}
	\begin{itemize}
		\item How can we compare \texttt{foo} and \texttt{bar}?
		\item By comparing their asymptotic run time complexity!
		\item What are the limitations?
	\end{itemize}	
		
	\end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Space complexity}
	\begin{itemize}
		\item So far we have only looked at the required \alert{time} of a function.
		\item What about the required \alert{space}?
			
		\item Just like time, space is a \alert{finite} resource.	
		\item So it is important to be able to set bounds on the usage.
	
		\item Difference?:Is there any difference in how time and space are used by functions?
		\item Recycling:	Yes! Space can be reused, whereas time cannot.
	\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{A first example}
	
			\begin{itemize}
				\item What is the \alert{space} complexity of this list comprehension?
			
			\begin{itemize}
				\item $\Theta(1)$
				\item $\Theta(n)$ 
				\item $\Theta(n^2)$
				\item I don't know.
			\end{itemize}
				\item We have $n$ integers, each requiring some constant amount of space $c$. Thus $S(n) = cn$, so $\Theta(n)$ space is
		required.
			\end{itemize}
		
		\lstinputlisting{src/comprehension-complexity.py}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{frame}
	% \frametitle{Function calls}
	% \framesubtitle{Based on a slide by Robbert Krebbers}
	% \begin{columns}
		% \column{0.455\textwidth}
		% \small
		% \textbf{When a function is called:}
		% \begin{itemize}
			% \item A \emph{stack frame} is \emph{added} to memory.
			% \item The stack frame contains:
				% \begin{itemize}
					% \item The function arguments
					% \item The local variables
					% \item The \emph{return address} to track the statement that called the function
				% \end{itemize}
		% \end{itemize}

		% \medskip
		% \textbf{When a function returns:}
		% \begin{itemize}
			% \item The \emph{stack frame} is \emph{removed}.
			% \item Control returns to the \emph{return address}.
		% \end{itemize}
		% \column{0.455\textwidth}

		% \begin{tikzpicture}[
			% node distance=0.2em,
			% stackframe/.style={draw=black,
			% text width=4.5em,minimum height=3em,text centered,font=\small},
			% ]
			% \node[stackframe,fill=green!10] (main) {
				% stack frame for \lstinline|main|
			% };
			% \node[stackframe,above=of main,fill=green!20] (f) {
				% stack frame for \lstinline|f|
			% };
			% \node[stackframe,above=of f,fill=green!30] (g) {
				% stack frame for \lstinline|g|
			% };

			% \node[stackframe,left=0.5em of g,yshift=5em,fill=green!40] (push) {stack frame for \lstinline|h|};
			% \node[stackframe,right=0.5em of g,yshift=5em,fill=green!30] (pop) {stack frame for \lstinline|g|};

			% \draw[->,thick] (push) edge[out=0,in=90] node[left,yshift=-1.5em] {add} ($(g.north)+(-1em,0)$);
			% \draw[<-,thick] (pop) edge[out=180,in=90] node[right,yshift=-1.5em] {remove} ($(g.north)+(1em,0)$);

		% \end{tikzpicture}
	% \end{columns}

	% 
	% \begin{columns}[t]
		% \column{0.755\textwidth}

		% \begin{block}{The same function?}
			% Can there be multiple frames for the same function on the stack?
		% \end{block}
		% 
		% \column{0.255\textwidth}
		% \begin{block}{Yes!}
			% Recursion!
		% \end{block}
	% \end{columns}
% \end{frame}

% \begin{frame}[fragile]{The call stack in action}
	% \framesubtitle{Based on a slide by Robbert Krebbers}
% \begin{minipage}[t]{0.5\textwidth}
% \textbf{Let us call \lstinline|foo(3)|}:

% \medskip
% \begin{lstlisting}
% % [linebackgroundcolor={%
  % % \btLstHL<1>{}%
  % % \btLstHL<2>{5-8}%
  % % \btLstHL<3>{6-8}%
  % % \btLstHL<5>{7-8}%
  % % \btLstHL<7>{8-8}%
  % % \btLstHL<4,6>{2}%
% % }]
% def bar(n: int) -> int:
  % return n;

% def foo(n: int) -> int:
	% res = 8
	% res += bar(n-1) 
	% res += bar(n-2) 
	% return res
% \end{lstlisting}

% \medskip
% \onslide<2->{
% \textbf{When a function is called:}
% \begin{itemize}
% \item A \emph{stack frame} is \emph{added} to the stack
% \item Containing the function arguments, local variables, and the \emph{return address}
% \end{itemize}}
% \end{minipage}
% \hfill
% \begin{minipage}[t]{0.48\textwidth}
% \textbf{Stack:}

% \smallskip
% % \begin{tikzpicture}[
  % % node distance=0.2em,
  % % stackframe/.style={font=\small,draw=structure,thick,fill=structure!0.1,text width=8em},
	% % every label/.style={right,font=\scriptsize\tt},
% % ]
% % \onslide<2->{\node[stackframe,label=right:foo(3),onslide=<2-3>{draw=alert},onslide=<5>{draw=alert},onslide=<7>{draw=alert}] (foo) {
  % % \texttt{n} = 3 \\
  % % \texttt{res} = \only<2-4>{8}\only<5-6>{10}\only<7->{11} \\
  % % \texttt{return}=\emph{main}
% % };}

% % \onslide<4>{\node[stackframe,label=right:bar(2),above=of foo,onslide=<4>{draw=alert}] (fac2) {
  % % \texttt{n} = 2 \\
  % % \texttt{return}=line~6
% % };}

% % \onslide<6>{\node[stackframe,label=right:bar(1),above=of foo,onslide=<6>{draw=alert}] (fac1b) {
  % % \texttt{n} = 1 \\
  % % \texttt{return}=line~7
% % };}

% % \invisible{\node[stackframe,label=right:factorial(2),above=of fac1b,onslide=<6>{draw=alert}] (fac2b) {
  % % \texttt{n} = 1 \\
  % % \texttt{n} = 1 \\
  % % \texttt{n} = 1 \\
  % % \texttt{return}=line~7
% % };}
% % \end{tikzpicture}

% \medskip
% \onslide<4->{
% \textbf{When a function returns:}
% \begin{itemize}
% \item The \emph{stack frame} is \emph{removed}
% \item Control returns to the \emph{return address}
% \end{itemize}}
% \end{minipage}
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{A long story short}
	
		\begin{itemize}
		\item Observations:
	\begin{itemize}
		\item Calling a function takes space!
		\item This is important when dealing with recursive functions (which we will discuss after the break).
		\item All of the parameters are stored in this bit of space as well.
	\end{itemize}
	
		\item What about lists?:
		Does this mean that the ``stack frame'' for \texttt{baz(my\_list)} requires $O(n)$ space?
	
		\item Nope:
		No! We pass a \textit{reference} instead of a copy. We tell \texttt{baz} where the list is so that it can
		access (or change!) it. Thus this call still requires only $O(1)$ space.\\
	\end{itemize}

		{\scriptsize
		See this excellent StackOverflow post explaining this in more detail if you are interested:
	\url{https://stackoverflow.com/a/986145}.}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{A second example}
	\framesubtitle{Using a list}
	
			\begin{itemize}
				\item What is the space complexity of the function \texttt{maya}?
			\begin{itemize}
				\item $\Theta(1)$
				\item $\Theta(n)$ 
				\item $\Theta(n^2)$
				\item I don't know.
			\end{itemize}
	
				\item Quadratic space:
		We create a list of $n^2$ items, so we need $\Theta(n^2)$ space. We could say $S(n) = c_0 + c_1n^2$, where $c_0$ is
		for the stack frame, \texttt{s}, \texttt{i} and \texttt{j}. $c_1$ is for \texttt{x}.
	\end{itemize}
	
\lstinputlisting{src/big-oh-example.py}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{A second example - modified}
	\framesubtitle{Doing without the list}
	

		\lstinputlisting{src/big-oh-example-v2.py}
		
			\begin{itemize}
				\item What is the space complexity of the function \texttt{mia}?
			\begin{itemize}
				\item $\Theta(1)$
				\item $\Theta(n)$ 
				\item $\Theta(n^2)$
				\item I don't know.
			\end{itemize}
	
				\item Constant space:
		We now only require to store the variable \texttt{s} and call the function \texttt{range}. Both of these require
		constant space, so $S(n) = c_0$ is $\Theta(1)$.
	\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{A final example - modified}
	\framesubtitle{Using a list from a parameter}
						\begin{itemize}
				\item What is the space complexity of the function \texttt{sum}?
			\begin{itemize}
				\item $\Theta(1)$
				\item $\Theta(n)$ 
				\item $\Theta(n^2)$
				\item I don't know.
			\end{itemize}
				\item Constant space:
		Remember that a list that is passed as input, is a \textit{reference} and does not take space!
				\item Observation:
			Had input contributed to the space complexity, there would be no sub-linear space complexities!	
		\end{itemize}	
		
				\lstinputlisting{src/big-oh-example-v3.py}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Relations between time and space?}

						\begin{itemize}
				\item It's all (a) relative (dimension):
		Given that a function \texttt{foo} uses $\Theta(n)$ space, what, if anything, can we conclude about the amount of
		time $T(n)$ \texttt{foo} requires?
		\begin{itemize}
			\item $T(n)$ is $\Omega(n)$
			\item $T(n)$ is $\Theta(n)$
			\item $T(n)$ is $O(n)$
			\item We cannot conclude anything.
			\item I don't know
		\end{itemize}
	\end{block}
		
			\item A nice lower bound:
			Claiming all of this memory (space) requires time! So we need $\Omega(n)$ time to execute \texttt{foo}!
		\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Do your remember?}
	\begin{definition}[Big-Oh]
		A function $f(n)$ is $O(g(n))$ iff there is a positive real constant $c$ and a positive integer $n_0$ such that for
		all $n \geq n_0$ it holds that $f(n) \leq c g(n)$. In other words:\\
		$\exists c \in \mathbb{R}, \exists n_0 \in \mathbb{N} (c > 0 \wedge n_0 \geq 1 \wedge (\forall n \in \mathbb{N} (n
		\geq n_0 \to f(n) \leq cg(n))))$.
	\end{definition}
	
	\begin{columns}
		\column{0.455\textwidth}
		\lstinputlisting{src/big-oh-example.py}
			
		\column{0.455\textwidth}
		\alt<4>{
			\begin{block}{Run time}
				The run time is described as $T(n) = c_0 + c_1n + c_2n^2$, where
				\begin{itemize}
					\item $c_0$ is for lines 2, 5, and 9.
					\item $c_1n$ is for the range in line 3.
					\item $c_2n^2$ is for lines 4, 5, and 8.
				\end{itemize}
				Thus $T(n)$ is $O(n^2)$.
			\end{block}
			}{
			\begin{block}{So\dots}
				What is a tight upper bound on the run time of \texttt{maya}?
				\begin{enumerate}[A.]
					\item $O(1)$
					\item $O(n)$
					\item $O(n^2)$
					\item $O(n^3)$
					\item I don't know.
				\end{enumerate}
			\end{block}
		}
	\end{columns}
\end{frame}

\begin{frame}
	\frametitle{Which case?}
	\lstinputlisting{src/for-loop-wc.py}
	\begin{columns}
		\column{0.755\textwidth}
	\begin{block}{So which is it?}
		Which of these forms a tight bound on the run time $T(n)$?
	\begin{enumerate}[A.]
		\small
		\item $O(1)$. 
		\item $O(n)$. 
		\item $O(n^2)$. 
		\item I don't know.
	\end{enumerate}	
	\end{block}
		\column{0.255\textwidth}
		
		\begin{block}{}
			Only B
		\end{block}
		
			\begin{block}{Worst Kaas}
				We talk about the \textit{worst-case}.
			\end{block}	
	\end{columns}
\end{frame}

\begin{frame}
	\frametitle{More practice?}
	\begin{itemize}
		\item We will practice this more in tomorrow's tutorial!
		\item As well as big-Oh proofs (i.e. finding $c$ and $n_0$, such that\dots).
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Some python built-in functions}
	\begin{itemize}
		\item You already know about a number of built-in python functions.
			\begin{itemize}
				\item \texttt{range}
			
				\item \texttt{in} (like: \texttt{if $8$ in $x$:})
			
				\item list-comprehensions
			\end{itemize}
			
		\item What is their time complexity?
	\end{itemize}	
\end{frame}

\begin{frame}
	\frametitle{On the topic of ranges}
	\framesubtitle{Get your cowboy boots ready!}

	\begin{columns}
		\column{0.455\textwidth}
			\lstinputlisting{src/range-complexity.py}
		\column{0.455\textwidth}
		
		\begin{block}{GROUP BY}
			Group the different lines by their run time complexity (are they $O(1)$, $O(n)$, $O(n^2)$, etc?)
		\end{block}
	\end{columns}
	
	\begin{block}{So what are they?}
		\begin{enumerate}
			\item $O(n)$, we go through $n$ items.
			\item $O(1)$, there are a constant number of items (100).
			\item $O(n)$, although we go through only $n/2$ items, this still grows linearly as $n$ grows.
			\item $O(1)$, this is again a constant number of items (100).
		\end{enumerate}
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{What about in?}
	\framesubtitle{Everyone get in here!}

	\begin{columns}
		\column{0.455\textwidth}
			\lstinputlisting{src/in-complexity.py}
		\column{0.455\textwidth}
		
		\begin{block}{Searching}
			What is the time complexity of this operation?
			\begin{enumerate}[A.]
				\item $O(1)$
				\item $O(n)$ where $n = $\texttt{len(my\_list)}.
				\item $O(n^2)$ where $n = $\texttt{len(my\_list)}.
				\item I don't know.
			\end{enumerate}
		\end{block}
	\end{columns}
	
	\begin{block}{Linear time}
		Worst case we need to check every element, so $O(n)$ time.
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{List comprehensions}
	\framesubtitle{Comprehende?}

	\begin{columns}
		\column{0.455\textwidth}
			\lstinputlisting{src/comprehension-complexity.py}
		\column{0.455\textwidth}
		
		\begin{block}{Comprehension}
			What is the time complexity of this list comprehension?
			\begin{enumerate}[A.]
				\item $O(1)$
				\item $O(n)$ 
				\item $O(n^2)$
				\item I don't know.
			\end{enumerate}
		\end{block}
	\end{columns}
	
	\begin{block}{Linear time}
		The answer is in the for loop. This is $O(n)$ and so the creation of the list is also $O(n)$. 
	\end{block}
	
		\begin{block}{Why though?}
			We will see why exactly when we discuss lists next week.
		\end{block}	
\end{frame}

\begin{frame}
	\frametitle{More bounds}
	\begin{center}
		\includegraphics[width=0.6\textwidth]{images/bounds.jpg}\\
		\hspace*{15pt}\hbox{\scriptsize Image By:\thinspace{\itshape David Mulder}}
		%https://www.flickr.com/photos/113026679@N03/43459273171
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{A lower bound}
	\framesubtitle{Omega}
	\begin{definition}[Big-$\Omega$ (Omega)]
		A function $f(n)$ is $\Omega(g(n))$ iff there is a positive real constant $c$ and a positive integer $n_0$ such that for
		all $n \geq n_0$ it holds that $f(n) \geq c g(n)$. In other words:\\
		$\exists c \in \mathbb{R}, \exists n_0 \in \mathbb{N} (c > 0 \wedge n_0 \geq 1 \wedge (\forall n \in \mathbb{N} (n
		\geq n_0 \to f(n) \geq cg(n))))$.
	\end{definition}
	
	\begin{block}{What of it?}
		Assume that $f(n)$ is $O(g(n))$ what, if anything, can we now conclude?
		\begin{enumerate}[A.]
			\item $f(n)$ is $\Omega(g(n))$
			\item $g(n)$ is $O(f(n))$
			\item $g(n)$ is $\Omega(f(n))$
			\item None of the above.
			\item I don't know.
		\end{enumerate}
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{So what?}
	\begin{block}{Why do we care?}
		What can we use big-$\Omega$ for?
	\end{block}
	
	\begin{block}{Not much!}
		Very very little :) \\
		
		Though occassionally we can prove things require e.g.\ $\Omega(n)$ steps, even if we do not know how to solve it
		exactly.\\
		
		But if something is both $O(f(n))$ and $\Omega(f(n))$\dots
	\end{block}
	
	\begin{definition}[Big-$\Theta$ (Theta)]
		A function $f(n)$ is $\Theta(g(n))$ iff there are positive real constants $c_0, c_1$ and a positive integer $n_0$ such that for
		all $n \geq n_0$ it holds that $c_0 g(n) \leq f(n) \leq c_1 g(n)$. In other words:\\
		{\small
		$\exists c_0,c_1 \in \mathbb{R}, \exists n_0 \in \mathbb{N} (c_0> 0 \wedge c_1 > 0\wedge n_0 \geq 1 \wedge (\forall
		n \in \mathbb{N} (n \geq n_0 \to c_1 g(n) \leq f(n) \leq c_2 g(n))))$.
	}
	\end{definition}
\end{frame}

\begin{frame}
	\vspace{-10pt}
	\begin{overlayarea}{\textwidth}{\textheight}
			\begin{block}{Why do we care about this?}
				So is big-$\Theta$ any use?
			\end{block}
			
	\vspace{-5pt}
			\begin{block}{Yes!}
				It is basically the ``tight upper bound'' we discussed yesterday.
			\end{block}
			
	\vspace{-5pt}
		\only<-5>{
			\begin{columns}
				\column{0.455\textwidth}
				\lstinputlisting{src/big-oh-example.py}
					
				\column{0.455\textwidth}
				\alt<5>{
					\begin{block}{Run time}
						The run time is described as $T(n) = c_0 + c_1n + c_2n^2$, where
						\begin{itemize}
							\item $c_0$ is for lines 2, 5, and 9.
							\item $c_1n$ is for the range in line 3.
							\item $c_2n^2$ is for lines 4, 5, and 8.
						\end{itemize}
						Thus $T(n)$ is $\Theta(n^2)$.
					\end{block}
					}{
					\begin{block}{So\dots}
						What is a tight bound on the run time of \texttt{maya}?
						\begin{enumerate}[A.]
						\small
							\item $\Theta(1)$
							\item $\Theta(n)$
							\item $\Theta(n^2)$
							\item $\Theta(n^3)$
							\item I don't know.
						\end{enumerate}
					\end{block}
				}
			\end{columns}
		}
		\only<6>{
				\begin{alertblock}{Despite all that...}
					We still often \textit{just} ask for ``a tight upper bound'' and will accept a big-Oh.
				\end{alertblock}	
		}
	\end{overlayarea}
\end{frame}
