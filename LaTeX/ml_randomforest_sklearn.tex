%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Random Forest with Scikit-Learn}

{\tiny (Ref: Ensemble Machine Learning Algorithms in Python with scikit-learn - Jason Brownlee)}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Random Forest Template Code}
\begin{lstlisting}
from sklearn.ensemble import RandomForestClassifier

model= RandomForestClassifier() 

model.fit(X, y) 

predicted= model.predict(x_test) 
\end{lstlisting}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{}
% \begin{center}
% {\Large Test case: Diabetes in Pima Indians}
% \end{center}
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Random Forest Classification}

	\begin{itemize}
	\item Random forest is an extension of bagged decision trees.
	\item Samples of the training dataset are taken with replacement, but the trees are constructed in a way that reduces the correlation between individual classifiers. 
	\item Specifically, rather than greedily choosing the best split point in the construction of the tree, only a random subset of features are considered for each split.
	\end{itemize}
	
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Random Forest Classification}
Running the example provides a mean estimate of classification accuracy.

\begin{lstlisting}
from sklearn import model_selection
from sklearn.ensemble import RandomForestClassifier
import pandas
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/ \
pima-indians-diabetes/ pima-indians-diabetes.data"
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
dataframe = pandas.read_csv(url, names=names)
array = dataframe.values
X = array[:,0:8]
Y = array[:,8]
seed = 7
num_trees = 100
max_features = 3
kfold = model_selection.KFold(n_splits=10, random_state=seed)
model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)
results = model_selection.cross_val_score(model, X, Y, cv=kfold)
print(results.mean())

0.770727956254
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Test case: Iris Flowers}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Random Forest with Iris Dataset}

\begin{center}
\includegraphics[width=0.8\linewidth]{iris1}
\end{center}

\begin{lstlisting}
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
train = pd.read_csv("iris_train.csv")
test = pd.read_csv("iris_test.csv")
print(train.head())
\end{lstlisting}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Random Forest with Iris Dataset}
The data have to be in a numpy array in order for the random forest algorithm to accept it!
\begin{lstlisting}
cols = ['petal_length', 'petal_width', 'sepal_length', 'sepal_width'] 
colsRes = ['class']

trainArr = train.as_matrix(cols) #training array
trainRes = train.as_matrix(colsRes) # training results

## Training!
rf = RandomForestClassifier(n_estimators=100) # initialize
rf.fit(trainArr, trainRes) # fit the data to the algorithm
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Random Forest with Iris Dataset}
Put the test data in the same format!
\begin{lstlisting}
testArr = test.as_matrix(cols)
results = rf.predict(testArr)

# something I like to do is to add it back to the data frame, so I can compare side-by-side
test['predictions'] = results
print(test.head())
\end{lstlisting}
%\begin{center}
%\includegraphics[width=0.5\linewidth]{iris2}
%\end{center}
%With this dataset, the random forest algorithm predicted class perfectly. That is unlikely to happen on a more challenging problem, but I hope now you know how to get started!
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Random Forests (Recap)}
\begin{itemize}
\item Designed for decision tree classifiers
\item Bagging and Boosting can be applied to other learning algorithms
\item Example of ``manipulating the input features'' 
\item Idea: combines predictions made by multiple decision trees
\item Each tree is induced based on an independent set of random vectors
\item Random forests are generated from fixed probability distribution (unlike Boosting)

\end{itemize}
\end{frame}