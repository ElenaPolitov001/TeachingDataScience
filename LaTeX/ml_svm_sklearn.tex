%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Support Vector Machine with Scikit-Learn}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{SVM Template Code}
\begin{lstlisting}
from sklearn import svm 

#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset 

model = svm.svc() 

model.fit(X, y) 
model.score(X, y) 

#Predict Output 
predicted= model.predict(x_test)
\end{lstlisting}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Support Vector Machine}
{\tiny (Ref: Machine Learning Algorithm Recipes in scikit-learn - Jason Brownlee)}


\begin{lstlisting}
# Support Vector Machine
from sklearn import datasets
from sklearn import metrics
from sklearn.svm import SVC
# load the iris datasets
dataset = datasets.load_iris()
# fit a SVM model to the data
model = SVC()
model.fit(dataset.data, dataset.target)
print(model)
# make predictions
expected = dataset.target
predicted = model.predict(dataset.data)
# summarize the fit of the model
print(metrics.classification_report(expected, predicted))
print(metrics.confusion_matrix(expected, predicted))
\end{lstlisting}


\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Test case: Synthetic Data}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Support Vector Machine Classifier}
\begin{lstlisting}
from sklearn.datasets.samples_generator import make_blobs

X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)

xfit = np.linspace(-1, 3.5)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')

# Draw three lines that couple separate the data
for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:
    yfit = m * xfit + b
    plt.plot(xfit, yfit, '-k')
    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none', color='#AAAAAA', alpha=0.4)

plt.xlim(-1, 3.5);
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Support Vector Machine Classifier}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{svm1}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
Fit the model
\begin{lstlisting}
from sklearn.svm import SVC

clf = SVC(kernel='linear')
clf.fit(X, y)

# Plot the boundary:

plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')
plot_svc_decision_function(clf)
plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=200, facecolors='none');
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}

\begin{center}
\includegraphics[width=0.3\linewidth,keepaspectratio]{svm2}
\end{center}

Plotting function code:
\begin{lstlisting}
def plot_svc_decision_function(clf, ax=None):
    """Plot the decision function for a 2D SVC"""
    if ax is None:
        ax = plt.gca()
    x = np.linspace(plt.xlim()[0], plt.xlim()[1], 30)
    y = np.linspace(plt.ylim()[0], plt.ylim()[1], 30)
    Y, X = np.meshgrid(y, x)
    P = np.zeros_like(X)
    for i, xi in enumerate(x):
        for j, yj in enumerate(y):
            P[i, j] = clf.decision_function([xi, yj])
    # plot the margins
    ax.contour(X, Y, P, colors='k',
               levels=[-1, 0, 1], alpha=0.5,
               linestyles=['--', '-', '--'])
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Support Vector Machine with Kernels Classifier}



\begin{itemize}
\item Kernels are useful when the decision boundary is not linear. 
\item A Kernel is some functional transformation of the input data. 
\item SVMs ensure kernel calculations are efficient. 
\item In the example below, a linear boundary is not useful in separating the groups of points.
\end{itemize}

\begin{center}
\includegraphics[width=0.3\linewidth,keepaspectratio]{svm3}
\end{center}


\begin{lstlisting}
from sklearn.datasets.samples_generator import make_circles
X, y = make_circles(100, factor=.1, noise=.1)

clf = SVC(kernel='linear').fit(X, y)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')
plot_svc_decision_function(clf);
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Support Vector Machine with Kernels Classifier}
A simple model that could be useful is a radial basis function:

\begin{center}
\includegraphics[width=0.35\linewidth,keepaspectratio]{svm4}
\end{center}

In three dimensions, there is a clear separation between the data.

\begin{lstlisting}
r = np.exp(-(X[:, 0] ** 2 + X[:, 1] ** 2))

from mpl_toolkits import mplot3d

def plot_3D(elev=30, azim=30):
    ax = plt.subplot(projection='3d')
    ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='spring')
    ax.view_init(elev=elev, azim=azim)
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Support Vector Machine with Kernels Classifier}

\begin{center}
\includegraphics[width=0.5\linewidth,keepaspectratio]{svm5}
\end{center}

 Run the SVM with the rbf kernel:
\begin{lstlisting}
clf = SVC(kernel='rbf')
clf.fit(X, y)

plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')
plot_svc_decision_function(clf)
plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],
            s=200, facecolors='none');
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile]\frametitle{Support Vector Machine Additional Notes}
%\begin{itemize}
%\item  When using an SVM you need to choose the right values for parameters such as c and gamma. 
%\item Model validation can help to determine these optimal values by trial and error.
%\item SVMs run in $O(n^3)$ performance.
%%\item LinearSVC is scalable, SVC does not seem to be scalable.
%\item For large data sets try transforming the data to a smaller space and use LinearSVC with rbf.
%\end{itemize}
%\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Exercise: Ex6Data1}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Exercise : Support Vector Machine}
\begin{itemize}
\item 2D example dataset which can be separated by a linear boundary
\item Binary classification to be done using SVM
\item Data file: ex6data1.mat
\end{itemize}
\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{svmex1}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Exercise : Support Vector Machine}
\begin{itemize}
\item Try using different values of C (C controls penalty of mis-classified training examples)
\item Large C tells SVM to classify all the examples correctly and vice versa.
\end{itemize}
$C=1$
\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{svmex2}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Exercise : Support Vector Machine}
$C=100$
\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{svmex3}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Exercise : Support Vector Machine}
Load libraries and data
\begin{lstlisting}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
from scipy.io import loadmat

raw_data = loadmat('data/ex6data1.mat')

data = pd.DataFrame(raw_data['X'], columns=['X1', 'X2'])
data['y'] = raw_data['y']

positive = data[data['y'].isin([1])]
negative = data[data['y'].isin([0])]
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Exercise : Support Vector Machine}
Plot

\begin{center}
\includegraphics[width=0.4\linewidth,keepaspectratio]{svmex6}
\end{center}


\begin{lstlisting}
fig, ax = plt.subplots(figsize=(12,8))
ax.scatter(positive['X1'], positive['X2'], s=50, marker='x', label='Positive')
ax.scatter(negative['X1'], negative['X2'], s=50, marker='o', label='Negative')
ax.legend()
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Exercise : Support Vector Machine}
\begin{itemize}
\item  Notice that one outlier positive example sits apart from the others
\item The classes are still linearly separable but it's a very tight fit. 
\item Going to train a linear support vector machine to learn the class boundary
\item Use $svm.LinearSVC$ 
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Exercise : Support Vector Machine}
Sklearn  svm.LinearSVC and $C=1$
\begin{lstlisting}
from sklearn import svm
svc = svm.LinearSVC(C=1, loss='hinge', max_iter=1000)

svc.fit(data[['X1', 'X2']], data['y'])

svc.score(data[['X1', 'X2']], data['y'])

0.98039215686274506
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Exercise : Support Vector Machine}
With $C=100$
\begin{lstlisting}
svc2 = svm.LinearSVC(C=100, loss='hinge', max_iter=1000)
svc2.fit(data[['X1', 'X2']], data['y'])
svc2.score(data[['X1', 'X2']], data['y'])

1.0
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Exercise : Support Vector Machine}
\begin{itemize}
\item  Got a perfect classification of the training data, 
\item However by increasing the value of C, the decision boundary is no longer a natural fit for the data. 
\item Plot confidence level for each class prediction
\item Its a function of the points distance from the hyper-plane.
\item Visualization could be subtle but lets try.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Exercise : Support Vector Machine}
Confidence $C=1$
\begin{center}
\includegraphics[width=0.4\linewidth,keepaspectratio]{svmex7}
\end{center}
Color strength is confidence value. Look at the outlier classification as well.

\begin{lstlisting}
data['SVM 1 Confidence'] = svc.decision_function(data[['X1', 'X2']])

fig, ax = plt.subplots(figsize=(12,8))
ax.scatter(data['X1'], data['X2'], s=50, c=data['SVM 1 Confidence'], cmap='seismic')
ax.set_title('SVM (C=1) Decision Confidence')
\end{lstlisting}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Exercise : Support Vector Machine}
Confidence $C=100$
\begin{center}
\includegraphics[width=0.4\linewidth,keepaspectratio]{svmex8}
\end{center}
Color strength is confidence value. Look at the outlier classification as well.

\begin{lstlisting}
data['SVM 2 Confidence'] = svc2.decision_function(data[['X1', 'X2']])

fig, ax = plt.subplots(figsize=(12,8))
ax.scatter(data['X1'], data['X2'], s=50, c=data['SVM 2 Confidence'], cmap='seismic')
ax.set_title('SVM (C=100) Decision Confidence')
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Exercise : Support Vector Machine as Spam Classifier}
Non Linear Kernel for example data-set 2
\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{svmex4}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Exercise : Support Vector Machine as Spam Classifier}
Result for example data-set 2 could be:
\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{svmex5}
\end{center}
\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}[fragile]\frametitle{Exercise : Support Vector Machine}
%Non Linear SVM using Gaussian Kernel
%\begin{itemize}
%\item Sklearn has ready Gaussian function
%\item Lets write on our own
%\end{itemize}
%\begin{lstlisting}
%def gaussian_kernel(x1, x2, sigma):
%    return np.exp(-(np.sum((x1 - x2) ** 2) / (2 * (sigma ** 2))))
%    
%x1 = np.array([1.0, 2.0, 1.0])
%x2 = np.array([0.0, 4.0, -1.0])
%sigma = 2
%
%gaussian_kernel(x1, x2, sigma)
%
%0.32465246735834974
%\end{lstlisting}
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Exercise : Support Vector Machine}
\begin{itemize}
\item Its a similarity/distance value between pair of examples, parametrized by sigma.
\item Lower value, points are apart.
\item Lets use this on another non linearly separable dataset
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Exercise: Ex6Data2}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Exercise : Support Vector Machine}
Load data
\begin{lstlisting}
raw_data = loadmat('data/ex6data2.mat')

data = pd.DataFrame(raw_data['X'], columns=['X1', 'X2'])
data['y'] = raw_data['y']

positive = data[data['y'].isin([1])]
negative = data[data['y'].isin([0])]
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Exercise : Support Vector Machine}
Plot

\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{svmex9}
\end{center}


\begin{lstlisting}
fig, ax = plt.subplots(figsize=(12,8))
ax.scatter(positive['X1'], positive['X2'], s=30, marker='x', label='Positive')
ax.scatter(negative['X1'], negative['X2'], s=30, marker='o', label='Negative')
ax.legend()
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Exercise : Support Vector Machine}
Build a support vector machine classifier using the built-in RBF kernel (default is ``kernel='rbf'''), Radial Basis Function.
\begin{lstlisting}
svc = svm.SVC(C=100, gamma=10, probability=True)

svc.fit(data[['X1', 'X2']], data['y'])
svc.score(data[['X1', 'X2']], data['y'])

0.9698725376593279

\end{lstlisting}
\end{frame}


